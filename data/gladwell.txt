
DAVID AND GOLIATH
 
WHAT THE DOG SAW
 
OUTLIERS
 
BLINK
 
THE TIPPING POINT
 
ARTICLES
 
ETC.
The Naked Face

Posted August 5, 2002 by MALCOLM GLADWELL & filed under ANNALS OF PSYCHOLOGY, THE NEW YORKER - ARCHIVE.

Can you read people’s thoughts just by looking at them?

1.

Some years ago, John Yarbrough was working patrol for the Los Angeles County Sheriff’s Department. It was about two in the morning. He and his partner were in the Willowbrook section of South Central Los Angeles, and they pulled over a sports car. “Dark, nighttime, average stop,” Yarbrough recalls. “Patrol for me was like going hunting. At that time of night in the area I was working, there was a lot of criminal activity, and hardly anyone had a driver’s license. Almost everyone had something intoxicating in the car. We stopped drunk drivers all the time. You’re hunting for guns or lots of dope, or suspects wanted for major things. You look at someone and you get an instinctive reaction. And the longer you’ve been working the stronger that instinctive reaction is.”

Yarbrough was driving, and in a two-man patrol car the procedure is for the driver to make the approach and the officer on the passenger side to provide backup. He opened the door and stepped out onto the street, walking toward the vehicle with his weapon drawn. Suddenly, a man jumped out of the passenger side and pointed a gun directly at him. The two of them froze, separated by no more than a few yards. “There was a tree behind him, to his right,” Yarbrough recalls. “He was about seventeen. He had the gun in his right hand. He was on the curb side. I was on the other side, facing him. It was just a matter of who was going to shoot first. I remember it clear as day. But for some reason I didn’t shoot him.” Yarbrough is an ex-marine with close-cropped graying hair and a small mustache, and he speaks in measured tones. “Is he a danger? Sure. He’s standing there with a gun, and what person in his right mind does that facing a uniformed armed policeman? If you looked at it logically, I should have shot him. But logic had nothing to do with it. Something just didn’t feel right. It was a gut reaction not to shoot– a hunch that at that exact moment he was not an imminent threat to me.” So Yarbrough stopped, and, sure enough, so did the kid. He pointed a gun at an armed policeman on a dark street in South Central L.A., and then backed down.

Yarbrough retired last year from the sheriff’s department after almost thirty years, sixteen of which were in homicide. He now lives in western Arizona, in a small, immaculate house overlooking the Colorado River, with pictures of John Wayne, Charles Bronson, Clint Eastwood, and Dale Earnhardt on the wall. He has a policeman’s watchfulness: while he listens to you, his eyes alight on your face, and then they follow your hands, if you move them, and the areas to your immediate left and right– and then back again, in a steady cycle. He grew up in an affluent household in the San Fernando Valley, the son of two doctors, and he is intensely analytical: he is the sort to take a problem and break it down, working it over slowly and patiently in his mind, and the incident in Willowbrook is one of those problems. Policemen shoot people who point guns directly at them at two in the morning. But something he saw held him back, something that ninety-nine people out of a hundred wouldn’t have seen.

Many years later, Yarbrough met with a team of psychologists who were conducting training sessions for law enforcement. They sat beside him in a darkened room and showed him a series of videotapes of people who were either lying or telling the truth. He had to say who was doing what. One tape showed people talking about their views on the death penalty and on smoking in public. Another featured a series of nurses who were all talking about a nature film they were supposedly watching, even though some of them were actually watching grisly documentary footage about burn victims and amputees. It may sound as if the tests should have been easy, because we all think we can tell whether someone is lying. But these were not the obvious fibs of a child, or the prevarications of people whose habits and tendencies we know well. These were strangers who were motivated to deceive, and the task of spotting the liars turns out to be fantastically difficult. There is just too much information–words, intonation, gestures, eyes, mouth–and it is impossible to know how the various cues should be weighted, or how to put them all together, and in any case it’s all happening so quickly that you can’t even follow what you think you ought to follow. The tests have been given to policemen, customs officers, judges, trial lawyers, and psychotherapists, as well as to officers from the F.B.I., the C.I.A., the D.E.A., and the Bureau of Alcohol, Tobacco, and Firearms– people one would have thought would be good at spotting lies. On average, they score fifty per cent, which is to say that they would have done just as well if they hadn’t watched the tapes at all and just guessed. But every now and again– roughly one time in a thousand–someone scores off the charts. A Texas Ranger named David Maxwell did extremely well, for example, as did an ex-A.T.F. agent named J.J. Newberry, a few therapists, an arbitrator, a vice cop– and John Yarbrough, which suggests that what happened in Willowbrook may have been more than a fluke or a lucky guess. Something in our faces signals whether we’re going to shoot, say, or whether we’re lying about the film we just saw. Most of us aren’t very good at spotting it. But a handful of people are virtuosos. What do they see that we miss?

2.

All of us, a thousand times a day, read faces. When someone says “I love you,” we look into that person’s eyes to judge his or her sincerity. When we meet someone new, we often pick up on subtle signals, so that, even though he or she may have talked in a normal and friendly manner, afterward we say, “I don’t think he liked me,” or “I don’t think she’s very happy.” We easily parse complex distinctions in facial expression. If you saw me grinning, for example, with my eyes twinkling, you’d say I was amused. But that’s not the only way we interpret a smile. If you saw me nod and smile exaggeratedly, with the corners of my lips tightened, you would take it that I had been teased and was responding sarcastically. If I made eye contact with someone, gave a small smile and then looked down and averted my gaze, you would think I was flirting. If I followed a remark with an abrupt smile and then nodded, or tilted my head sideways, you might conclude that I had just said something a little harsh, and wanted to take the edge off it. You wouldn’t need to hear anything I was saying in order to reach these conclusions. The face is such an extraordinarily efficient instrument of communication that there must be rules that govern the way we interpret facial expressions. But what are those rules? And are they the same for everyone?

In the nineteen-sixties, a young San Francisco psychologist named Paul Ekman began to study facial expression, and he discovered that no one knew the answers to those questions. Ekman went to see Margaret Mead, climbing the stairs to her tower office at the American Museum of Natural History. He had an idea. What if he travelled around the world to find out whether people from different cultures agreed on the meaning of different facial expressions? Mead, he recalls, “looked at me as if I were crazy.” Like most social scientists of her day, she believed that expression was culturally determined– that we simply used our faces according to a set of learned social conventions. Charles Darwin had discussed the face in his later writings; in his 1872 book, “The Expression of the Emotions in Man and Animals,” he argued that all mammals show emotion reliably in their faces. But in the nineteen-sixties academic psychologists were more interested in motivation and cognition than in emotion or its expression. Ekman was undaunted; he began travelling to places like Japan, Brazil, and Argentina, carrying photographs of men and women making a variety of distinctive faces. Everywhere he went, people agreed on what those expressions meant. But what if people in the developed world had all picked up the same cultural rules from watching the same movies and television shows? So Ekman set out again, this time making his way through the jungles of Papua New Guinea, to the most remote villages, and he found that the tribesmen there had no problem interpreting the expressions, either. This may not sound like much of a breakthrough. But in the scientific climate of the time it was a revelation. Ekman had established that expressions were the universal products of evolution. There were fundamental lessons to be learned from the face, if you knew where to look.

Paul Ekman is now in his sixties. He is clean-shaven, with closely set eyes and thick, prominent eyebrows, and although he is of medium build, he seems much larger than he is: there is something stubborn and substantial in his demeanor. He grew up in Newark, the son of a pediatrician, and entered the University of Chicago at fifteen. He speaks deliberately: before he laughs, he pauses slightly, as if waiting for permission. He is the sort to make lists, and number his arguments. His academic writing has an orderly logic to it; by the end of an Ekman essay, each stray objection and problem has been gathered up and catalogued. In the mid-sixties, Ekman set up a lab in a ramshackle Victorian house at the University of California at San Francisco, where he holds a professorship. If the face was part of a physiological system, he reasoned, the system could be learned. He set out to teach himself. He treated the face as an adventurer would a foreign land, exploring its every crevice and contour. He assembled a videotape library of people’s facial expressions, which soon filled three rooms in his lab, and studied them to the point where he could look at a face and pick up a flicker of emotion that might last no more than a fraction of a second. Ekman created the lying tests. He filmed the nurses talking about the movie they were watching and the movie they weren’t watching. Working with Maureen O’Sullivan, a psychologist from the University of San Francisco, and other colleagues, he located people who had a reputation for being uncannily perceptive, and put them to the test, and that’s how Yarbrough and the other high-scorers were identified. O’Sullivan and Ekman call this study of gifted face readers the Diogenes Project, after the Greek philosopher of antiquity who used to wander around Athens with a lantern, peering into people’s faces as he searched for an honest man. Ekman has taken the most vaporous of sensations– the hunch you have about someone else– and sought to give them definition. Most of us don’t trust our hunches, because we don’t know where they came from. We think they can’t be explained. But what if they can?

3.

Paul Ekman got his start in the face-reading business because of a man named Silvan Tomkins, and Silvan Tomkins may have been the best face reader there ever was. Tomkins was from Philadelphia, the son of a dentist from Russia. He was short, and slightly thick around the middle, with a wild mane of white hair and huge black plastic-rimmed glasses. He taught psychology at Princeton and Rutgers, and was the author of “Affect, Imagery, Consciousness,” a four-volume work so dense that its readers were evenly divided between those who understood it and thought it was brilliant and those who did not understand it and thought it was brilliant. He was a legendary talker. At the end of a cocktail party, fifteen people would sit, rapt, at Tomkins’s feet, and someone would say, “One more question!” and they would all sit there for another hour and a half, as Tomkins held forth on, say, comic books, a television sitcom, the biology of emotion, his problem with Kant, and his enthusiasm for the latest fad diets, all enfolded into one extended riff. During the Depression, in the midst of his doctoral studies at Harvard, he worked as a handicapper for a horse-racing syndicate, and was so successful that he lived lavishly on Manhattan’s Upper East Side. At the track, where he sat in the stands for hours, staring at the horses through binoculars, he was known as the Professor. “He had a system for predicting how a horse would do based on what horse was on either side of him, based on their emotional relationship,” Ekman said. If a male horse, for instance, had lost to a mare in his first or second year, he would be ruined if he went to the gate with a mare next to him in the lineup. (Or something like that– no one really knew for certain.) Tomkins felt that emotion was the code to life, and that with enough attention to particulars the code could be cracked. He thought this about the horses, and, more important, he thought this about the human face.

Tomkins, it was said, could walk into a post office, go over to the “Wanted” posters, and, just by looking at mug shots, tell you what crimes the various fugitives had committed. “He would watch the show “To Tell the Truth,’ and without fault he could always pick the person who was lying and who his confederates were,” his son, Mark, recalls. “He actually wrote the producer at one point to say it was too easy, and the man invited him to come to New York, go backstage, and show his stuff.” Virginia Demos, who teaches psychology at Harvard, recalls having long conversations with Tomkins. “We would sit and talk on the phone, and he would turn the sound down as Jesse Jackson was talking to Michael Dukakis, at the Democratic National Convention. And he would read the faces and give his predictions on what would happen. It was profound.”

Ekman’s most memorable encounter with Tomkins took place in the late sixties. Ekman had just tracked down a hundred thousand feet of film that had been shot by the virologist Carleton Gajdusek in the remote jungles of Papua New Guinea. Some of the footage was of a tribe called the South Fore, who were a peaceful and friendly people. The rest was of the Kukukuku, who were hostile and murderous and who had a homosexual ritual where pre-adolescent boys were required to serve as courtesans for the male elders of the tribe. Ekman was still working on the problem of whether human facial expressions were universal, and the Gajdusek film was invaluable. For six months, Ekman and his collaborator, Wallace Friesen, sorted through the footage. They cut extraneous scenes, focussing just on closeups of the faces of the tribesmen, and when the editing was finished Ekman called in Tomkins.

The two men, protégé and mentor, sat at the back of the room, as faces flickered across the screen. Ekman had told Tomkins nothing about the tribes involved; all identifying context had been edited out. Tomkins looked on intently, peering through his glasses. At the end, he went up to the screen and pointed to the faces of the South Fore. “These are a sweet, gentle people, very indulgent, very peaceful,” he said. Then he pointed to the faces of the Kukukuku. “This other group is violent, and there is lots of evidence to suggest homosexuality.” Even today, a third of a century later, Ekman cannot get over what Tomkins did. “My God! I vividly remember saying, “Silvan, how on earth are you doing that?’ ” Ekman recalls. “And he went up to the screen and, while we played the film backward, in slow motion, he pointed out the particular bulges and wrinkles in the face that he was using to make his judgment. That’s when I realized, “I’ve got to unpack the face.’ It was a gold mine of information that everyone had ignored. This guy could see it, and if he could see it, maybe everyone else could, too.”

Ekman and Friesen decided that they needed to create a taxonomy of facial expressions, so day after day they sat across from each other and began to make every conceivable face they could. Soon, though, they realized that their efforts weren’t enough. “I met an anthropologist, Wade Seaford, told him what I was doing, and he said, ‘Do you have this movement?’” –and here Ekman contracted what’s called the triangularis, which is the muscle that depresses the corners of the lips, forming an arc of distaste– “and it wasn’t in my system, because I had never seen it before. I had built a system not on what the face can do but on what I had seen. I was devastated. So I came back and said, ‘I’ve got to learn the anatomy.’ ” Friesen and Ekman then combed through medical textbooks that outlined each of the facial muscles, and identified every distinct muscular movement that the face could make. There were forty-three such movements. Ekman and Friesen called them “action units.” Then they sat across from each other again, and began manipulating each action unit in turn, first locating the muscle in their mind and then concentrating on isolating it, watching each other closely as they did, checking their movements in a mirror, making notes of how the wrinkle patterns on their faces would change with each muscle movement, and videotaping the movement for their records. On the few occasions when they couldn’t make a particular movement, they went next door to the U.C.S.F. anatomy department, where a surgeon they knew would stick them with a needle and electrically stimulate the recalcitrant muscle. “That wasn’t pleasant at all,” Ekman recalls. When each of those action units had been mastered, Ekman and Friesen began working action units in combination, layering one movement on top of another. The entire process took seven years. “There are three hundred combinations of two muscles,” Ekman says. “If you add in a third, you get over four thousand. We took it up to five muscles, which is over ten thousand visible facial configurations.” Most of those ten thousand facial expressions don’t mean anything, of course. They are the kind of nonsense faces that children make. But, by working through each action-unit combination, Ekman and Friesen identified about three thousand that did seem to mean something, until they had catalogued the essential repertoire of human emotion.

4.

On a recent afternoon, Ekman sat in his office at U.C.S.F., in what is known as the Human Interaction Laboratory, a standard academic’s lair of books and files, with photographs of his two heroes, Tomkins and Darwin, on the wall. He leaned forward slightly, placing his hands on his knees, and began running through the action-unit configurations he had learned so long ago. “Everybody can do action unit four,” he began. He lowered his brow, using his depressor glabellae, depressor supercilli, and corrugator. “Almost everyone can do A.U. nine.” He wrinkled his nose, using his levator labii superioris, alaeque nasi. “Everybody can do five.” He contracted his levator palpebrae superioris, raising his upper eyelid.

I was trying to follow along with him, and he looked up at me. “You’ve got a very good five,” he said generously. “The more deeply set your eyes are, the harder it is to see the five. Then there’s seven.” He squinted. “Twelve.” He flashed a smile, activating the zygomatic major. The inner parts of his eyebrows shot up. “That’s A.U. —- distress, anguish.” Then he used his frontalis, pars lateralis, to raise the outer half of his eyebrows. “That’s A.U. two. It’s also very hard, but it’s worthless. It’s not part of anything except Kabuki theatre. Twenty-three is one of my favorites. It’s the narrowing of the red margin of the lips. Very reliable anger sign. It’s very hard to do voluntarily.” He narrowed his lips. “Moving one ear at a time is still the hardest thing to do. I have to really concentrate. It takes everything I’ve got.” He laughed. “This is something my daughter always wanted me to do for her friends. Here we go.” He wiggled his left ear, then his right ear. Ekman does not appear to have a particularly expressive face. He has the demeanor of a psychoanalyst, watchful and impassive, and his ability to transform his face so easily and quickly was astonishing. “There is one I can’t do,” he went on. “It’s A.U. thirty-nine. Fortunately, one of my postdocs can do it. A.U. thirty-eight is dilating the nostrils. Thirty-nine is the opposite. It’s the muscle that pulls them down.” He shook his head and looked at me again. “Oooh! You’ve got a fantastic thirty-nine. That’s one of the best I’ve ever seen. It’s genetic. There should be other members of your family who have this heretofore unknown talent. You’ve got it, you’ve got it.” He laughed again. “You’re in a position to flash it at people. See, you should try that in a singles bar!”

Ekman then began to layer one action unit on top of another, in order to compose the more complicated facial expressions that we generally recognize as emotions. Happiness, for instance, is essentially A.U. six and twelve– contracting the muscles that raise the cheek (orbicularis oculi, pars orbitalis) in combination with the zygomatic major, which pulls up the corners of the lips. Fear is A.U. one, two and four, or, more fully, one, two, four, five, and twenty, with or without action units twenty-five, twenty-six, or twenty-seven. That is: the inner brow raiser (frontalis, pars medialis) plus the outer brow raiser (frontalis, pars lateralis) plus the brow-lowering depressor supercilli plus the levator palpebrae superioris (which raises the upper lid), plus the risorius (which stretches the lips), the parting of the lips (depressor labii), and the masseter (which drops the jaw). Disgust? That’s mostly A.U. nine, the wrinkling of the nose (levator labii superioris, alaeque nasi), but it can sometimes be ten, and in either case may be combined with A.U. fifteen or sixteen or seventeen.

Ekman and Friesen ultimately assembled all these combinations–and the rules for reading and interpreting them– into the Facial Action Coding System, or FACS, and wrote them up in a five-hundred-page binder. It is a strangely riveting document, full of details like the possible movements of the lips (elongate, de-elongate, narrow, widen, flatten, protrude, tighten and stretch); the four different changes of the skin between the eyes and the cheeks (bulges, bags, pouches, and lines); or the critical distinctions between infraorbital furrows and the nasolabial furrow. Researchers have employed the system to study everything from schizophrenia to heart disease; it has even been put to use by computer animators at Pixar (“Toy Story”), andat DreamWorks (“Shrek”). FACS takes weeks to master in its entirety, and only five hundred people around the world have been certified to use it in research. But for those who have, the experience of looking at others is forever changed. They learn to read the face the way that people like John Yarbrough did intuitively. Ekman compares it to the way you start to hear a symphony once you’ve been trained to read music: an experience that used to wash over you becomes particularized and nuanced.

Ekman recalls the first time he saw Bill Clinton, during the 1992 Democratic primaries. “I was watching his facial expressions, and I said to my wife, ‘This is Peck’s Bad Boy,’ ” Ekman says. “This is a guy who wants to be caught with his hand in the cookie jar, and have us love him for it anyway. There was this expression that’s one of his favorites. It’s that hand-in-the-cookie-jar, love-me-Mommy-because-I’m-a-rascal look. It’s A.U. twelve, fifteen, seventeen, and twenty-four, with an eye roll.” Ekman paused, then reconstructed that particular sequence of expressions on his face. He contracted his zygomatic major, A.U. twelve, in a classic smile, then tugged the corners of his lips down with his triangularis, A.U. fifteen. He flexed the mentalis, A.U. seventeen, which raises the chin, slightly pressed his lips together in A.U. twenty-four, and finally rolled his eyes–and it was as if Slick Willie himself were suddenly in the room. “I knew someone who was on his communications staff. So I contacted him. I said, ‘Look, Clinton’s got this way of rolling his eyes along with a certain expression, and what it conveys is “I’m a bad boy.” I don’t think it’s a good thing. I could teach him how not to do that in two to three hours.’ And he said, ‘Well, we can’t take the risk that he’s known to be seeing an expert on lying.’ I think it’s a great tragedy, because . . .” Ekman’s voice trailed off. It was clear that he rather liked Clinton, and that he wanted Clinton’s trademark expression to have been no more than a meaningless facial tic. Ekman shrugged. “Unfortunately, I guess, he needed to get caught–and he got caught.”

5.

Early in his career, Paul Ekman filmed forty psychiatric patients, including a woman named Mary, a forty-two-year-old housewife. She had attempted suicide three times, and survived the last attempt–an overdose of pills–only because someone found her in time and rushed her to the hospital. Her children had left home and her husband was inattentive, and she was depressed. When she first went to the hospital, she simply sat and cried, but she seemed to respond well to therapy. After three weeks, she told her doctor that she was feeling much better and wanted a weekend pass to see her family. The doctor agreed, but just before Mary was to leave the hospital she confessed that the real reason she wanted to go on weekend leave was so that she could make another suicide attempt. Several years later, a group of young psychiatrists asked Ekman how they could tell when suicidal patients were lying. He didn’t know, but, remembering Mary, he decided to try to find out. If the face really was a reliable guide to emotion, shouldn’t he be able to look back on the film and tell that she was lying? Ekman and Friesen began to analyze the film for clues. They played it over and over for dozens of hours, examining in slow motion every gesture and expression. Finally, they saw it. As Mary’s doctor asked her about her plans for the future, a look of utter despair flashed across her face so quickly that it was almost imperceptible.

Ekman calls that kind of fleeting look a “microexpression,” and one cannot understand why John Yarbrough did what he did on that night in South Central without also understanding the particular role and significance of microexpressions. Many facial expressions can be made voluntarily. If I’ m trying to look stern as I give you a tongue-lashing, I’ll have no difficulty doing so, and you’ ll have no difficulty interpreting my glare. But our faces are also governed by a separate, involuntary system. We know this because stroke victims who suffer damage to what is known as the pyramidal neural system will laugh at a joke, but they cannot smile if you ask them to. At the same time, patients with damage to another part of the brain have the opposite problem. They can smile on demand, but if you tell them a joke they can’t laugh. Similarly, few of us can voluntarily do A.U. one, the sadness sign. (A notable exception, Ekman points out, is Woody Allen, who uses his frontalis, pars medialis, to create his trademark look of comic distress.) Yet we raise our inner eyebrows all the time, without thinking, when we are unhappy. Watch a baby just as he or she starts to cry, and you’ll often see the frontalis, pars medialis, shoot up, as if it were on a string.

Perhaps the most famous involuntary expression is what Ekman has dubbed the Duchenne smile, in honor of the nineteenth-century French neurologist Guillaume Duchenne, who first attempted to document the workings of the muscles of the face with the camera. If I ask you to smile, you’ ll flex your zygomatic major. By contrast, if you smile spontaneously, in the presence of genuine emotion, you’ ll not only flex your zygomatic but also tighten the orbicularis oculi, pars orbitalis, which is the muscle that encircles the eye. It is almost impossible to tighten the orbicularis oculi, pars lateralis, on demand, and it is equally difficult to stop it from tightening when we smile at something genuinely pleasurable. This kind of smile “does not obey the will,” Duchenne wrote. “Its absence unmasks the false friend.” When we experience a basic emotion, a corresponding message is automatically sent to the muscles of the face. That message may linger on the face for just a fraction of a second, or be detectable only if you attached electrical sensors to the face, but It’s always there. Silvan Tomkins once began a lecture by bellowing, “The face is like the penis!” and this is what he meant–that the face has, to a large extent, a mind of its own. This doesn’t mean we have no control over our faces. We can use our voluntary muscular system to try to suppress those involuntary responses. But, often, some little part of that suppressed emotion–the sense that I’ m really unhappy, even though I deny it–leaks out. Our voluntary expressive system is the way we intentionally signal our emotions. But our involuntary expressive system is in many ways even more important: it is the way we have been equipped by evolution to signal our authentic feelings.

“You must have had the experience where somebody comments on your expression and you didn’t know you were making it,”Ekman says. “Somebody tells you, “What are you getting upset about?’ “Why are you smirking?’ You can hear your voice, but you can’t see your face. If we knew what was on our face, we would be better at concealing it. But that wouldn’t necessarily be a good thing. Imagine if there were a switch that all of us had, to turn off the expressions on our face at will. If babies had that switch, we wouldn’t know what they were feeling. They’ d be in trouble. You could make an argument, if you wanted to, that the system evolved so that parents would be able to take care of kids. Or imagine if you were married to someone with a switch? It would be impossible. I don’t think mating and infatuation and friendships and closeness would occur if our faces didn’t work that way.”

Ekman slipped a tape taken from the O.J. Simpson trial into the VCR. It was of Kato Kaelin, Simpson’s shaggy-haired house guest, being examined by Marcia Clark, one of the prosecutors in the case. Kaelin sits in the witness box, with his trademark vacant look. Clark asks a hostile question. Kaelin leans forward and answers softly. “Did you see that?” Ekman asked me. I saw nothing, just Kato being Kato– harmless and passive. Ekman stopped the tape, rewound it, and played it back in slow motion. On the screen, Kaelin moved forward to answer the question, and in that fraction of a second his face was utterly transformed. His nose wrinkled, as he flexed his levator labii superioris, alaeque nasi. His teeth were bared, his brows lowered. “It was almost totally A.U. nine,” Ekman said. “It’s disgust, with anger there as well, and the clue to that is that when your eyebrows go down, typically your eyes are not as open as they are here. The raised upper eyelid is a component of anger, not disgust. It’s very quick.” Ekman stopped the tape and played it again, peering at the screen. “You know, he looks like a snarling dog.”

Ekman said that there was nothing magical about his ability to pick up an emotion that fleeting. It was simply a matter of practice. “I could show you forty examples, and you could pick it up. I have a training tape, and people love it. They start it, and they can’t see any of these expressions. Thirty-five minutes later, they can see them all. What that says is that this is an accessible skill.”

Ekman showed another clip, this one from a press conference given by Kim Philby in 1955. Philby had not yet been revealed as a Soviet spy, but two of his colleagues, Donald Maclean and Guy Burgess, had just defected to the Soviet Union. Philby is wearing a dark suit and a white shirt. His hair is straight and parted to the left. His face has the hauteur of privilege.

“Mr. Philby,” he is asked. “Mr. Macmillan, the foreign secretary, said there was no evidence that you were the so-called third man who allegedly tipped off Burgess and Maclean. Are you satisfied with that clearance that he gave you?”

Philby answers confidently, in the plummy tones of the English upper class. “Yes, I am.”

“Well, if there was a third man, were you in fact the third man?”

“No,” Philby says, just as forcefully. “I was not.”

Ekman rewound the tape, and replayed it in slow motion. “Look at this,” he said, pointing to the screen. “Twice, after being asked serious questions about whether he’s committed treason, he’s going to smirk. He looks like the cat who ate the canary.” The expression was too brief to see normally. But at quarter speed it was painted on his face–the lips pressed together in a look of pure smugness. “He’s enjoying himself, isn’t he?” Ekman went on. “I call this–duping delight– the thrill you get from fooling other people.” Ekman started the VCR up again. “There’s another thing he does.” On the screen, Philby was answering another question. “In the second place, the Burgess-Maclean affair has raised issues of great”– he pauses– “delicacy.” Ekman went back to the pause, and froze the tape. “Here it is,”he said. “A very subtle microexpression of distress or unhappiness. It’s only in the eyebrows– in fact, just in one eyebrow.” Sure enough, Philby’s right inner eyebrow was raised in an unmistakable A.U. one. “It’s very brief,” Ekman said. “He’s not doing it voluntarily. And it totally contradicts all his confidence and assertiveness. It comes when he’s talking about Burgess and Maclean, whom he had tipped off. It’s a hot spot that suggests, ‘You shouldn’t trust what you hear.’ ”

A decade ago, Ekman joined forces with J. J. Newberry–the ex-A.T.F. agent who is one of the high-scorers in the Diogenes Project– to put together a program for educating law-enforcement officials around the world in the techniques of interviewing and lie detection. In recent months, they have flown to Washington, D.C., to assist the C.I.A. and the F.B.I. in counter-terrorism training. At the same time, the Defense Advanced Research Projects Agency (DARPA) has asked Ekman and his former student Mark Frank, now at Rutgers, to develop experimental scenarios for studying deception that would be relevant to counter-terrorism. The objective is to teach people to look for discrepancies between what is said and what is signalled–to pick up on the difference between Philby’s crisp denials and his fleeting anguish. It’s a completely different approach from the shouting cop we see on TV and in the movies, who threatens the suspect and sweeps all of the papers and coffee cups off the battered desk. The Hollywood interrogation is an exercise in intimidation, and its point is to force the suspect to tell you what you need to know. It does not take much to see the limitations of this strategy. It depends for its success on the coöperation of the suspect–when, of course, the suspect’s involuntary communication may be just as critical. And it privileges the voice over the face, when the voice and the face are equally significant channels in the same system.

Ekman received his most memorable lesson in this truth when he and Friesen first began working on expressions of anger and distress. “It was weeks before one of us finally admitted feeling terrible after a session where we’ d been making one of those faces all day,” Friesen says. “Then the other realized that he’d been feeling poorly, too, so we began to keep track.” They then went back and began monitoring their body during particular facial movements. “Say you do A.U. one, raising the inner eyebrows, and six, raising the cheeks, and fifteen, the lowering of the corner of the lips,” Ekman said, and then did all three. “What we discovered is that that expression alone is sufficient to create marked changes in the autonomic nervous system. When this first occurred, we were stunned. We weren’t expecting this at all. And it happened to both of us. We felt terrible . What we were generating was sadness, anguish. And when I lower my brows, which is four, and raise the upper eyelid, which is five, and narrow the eyelids, which is seven, and press the lips together, which is twenty-four, I’ m generating anger. My heartbeat will go up ten to twelve beats. My hands will get hot. As I do it, I can’t disconnect from the system. It’s very unpleasant, very unpleasant.”

Ekman, Friesen, and another colleague, Robert Levenson, who teaches at Berkeley, published a study of this effect in Science. They monitored the bodily indices of anger, sadness, and fear–heart rate and body temperature–in two groups. The first group was instructed to remember and relive a particularly stressful experience. The other was told to simply produce a series of facial movements, as instructed by Ekman– to “assume the position,” as they say in acting class. The second group, the people who were pretending, showed the same physiological responses as the first. A few years later, a German team of psychologists published a similar study. They had a group of subjects look at cartoons, either while holding a pen between their lips–an action that made it impossible to contract either of the two major smiling muscles, the risorius and the zygomatic major– or while holding a pen clenched between their teeth, which had the opposite effect and forced them to smile. The people with the pen between their teeth found the cartoons much funnier. Emotion doesn’t just go from the inside out. It goes from the outside in. What’s more, neither the subjects “assuming the position” nor the people with pens in their teeth knew they were making expressions of emotion. In the facial-feedback system, an expression you do not even know that you have can create an emotion you did not choose to feel.

It is hard to talk to anyone who knows FACS without this point coming up again and again. Face-reading depends not just on seeing facial expressions but also on taking them seriously. One reason most of us–like the TV cop– do not closely attend to the face is that we view its evidence as secondary, as an adjunct to what we believe to be real emotion. But there’s nothing secondary about the face, and surely this realization is what set John Yarbrough apart on the night that the boy in the sports car came at him with a gun. It’s not just that he saw a microexpression that the rest of us would have missed. It’s that he took what he saw so seriously that he was able to overcome every self-protective instinct in his body, and hold his fire.

6.

Yarbrough has a friend in the L.A. County Sheriff’s Department, Sergeant Bob Harms, who works in narcotics in Palmdale. Harms is a member of the Diogenes Project as well, but the two men come across very differently. Harms is bigger than Yarbrough, taller and broader in the chest, with soft brown eyes and dark, thick hair. Yarbrough is restoring a Corvette and wears Rush Limbaugh ties, and he says that if he hadn’t been a cop he would have liked to stay in the Marines. Harms came out of college wanting to be a commercial artist; now he plans to open a bed-and-breakfast in Vermont with his wife when he retires. On the day we met, Harms was wearing a pair of jean shorts and a short-sleeved patterned shirt. His badge was hidden inside his shirt. He takes notes not on a yellow legal pad, which he considers unnecessarily intimidating to witnesses, but on a powder-blue one. “I always get teased because I’m the touchy-feely one,” Harms said. “John Yarbrough is very analytical. He thinks before he speaks. There is a lot going on inside his head. He’s constantly thinking four or five steps ahead, then formulating whatever his answers are going to be. That’s not how I do my interviews. I have a conversation. It’s not “Where were you on Friday night?’ Because that’s the way we normally communicate. I never say, “I’m Sergeant Harms.’ I always start by saying, “I’m Bob Harms, and I’m here to talk to you about your case,’ and the first thing I do is smile.”

The sensation of talking to the two men, however, is surprisingly similar. Normal conversation is like a game of tennis: you talk and I listen, you listen and I talk, and we feel scrutinized by our conversational partner only when the ball is in our court. But Yarbrough and Harms never stop watching, even when they’re doing the talking. Yarbrough would comment on my conversational style, noting where I held my hands as I talked, or how long I would wait out a lull in the conversation. At one point, he stood up and soundlessly moved to the door– which he could have seen only in his peripheral vision–opening it just before a visitor rang the doorbell. Harms gave the impression that he was deeply interested in me. It wasn’t empathy. It was a kind of powerful curiosity. “I remember once, when I was in prison custody, I used to shake prisoners’ hands,” Harms said. “The deputies thought I was crazy. But I wanted to see what happened, because that’s what these men are starving for, some dignity and respect.”

Some of what sets Yarbrough and Harms and the other face readers apart is no doubt innate. But the fact that people can be taught so easily to recognize microexpressions, and can learn FACS, suggests that we all have at least the potential capacity for this kind of perception. Among those who do very well at face-reading, tellingly, are some aphasics, such as stroke victims who have lost the ability to understand language. Collaborating with Ekman on a paper that was recently published in Nature, the psychologist Nancy Etcoff, of Massachusetts General Hospital, described how a group of aphasics trounced a group of undergraduates at M.I.T. on the nurses tape. Robbed of the power to understand speech, the stroke victims had apparently been forced to become far more sensitive to the information written on people’s faces. “They are compensating for the loss in one channel through these other channels,” Etcoff says. “We could hypothesize that there is some kind of rewiring in the brain, but I don’t think we need that explanation. They simply exercise these skills much more than we do.” Ekman has also done work showing that some abused children are particularly good at reading faces as well: like the aphasics in the study, they developed “interpretive strategies”–in their case, so they could predict the behavior of their volatile parents.

What appears to be a kind of magical, effortless intuition about faces, then, may not really be effortless and magical at all. This kind of intuition is a product of desire and effort. Silvan Tomkins took a sabbatical from Princeton when his son Mark was born, and stayed in his house on the Jersey Shore, staring into his son’s face, long and hard, picking up the patterns of emotion–the cycles of interest, joy, sadness, and anger–that flash across an infant’s face in the first few months of life. He taught himself the logic of the furrows and the wrinkles and the creases, the subtle differences between the pre-smile and the pre-cry face. Later, he put together a library of thousands of photographs of human faces, in every conceivable expression. He developed something called the Picture Arrangement Test, which was his version of the Rorschach blot: a patient would look at a series of pictures and be asked to arrange them in a sequence and then tell a story based on what he saw. The psychologist was supposed to interpret the meaning of the story, but Tomkins would watch a videotape of the patient with the sound off, and by studying the expressions on the patient’s face teach himself to predict what the story was. Face-reading, for those who have mastered it, becomes a kind of compulsion; it becomes hard to be satisfied with the level and quality of information that most of us glean from normal social encounters. “Whenever we get together,” Harms says of spending time with other face readers, “we debrief each other. We’re constantly talking about cases, or some of these videotapes of Ekman’s, and we say, “I missed that, did you get that?’ Maybe there’s an emotion attached there. We’re always trying to place things, and replaying interviews in our head.”

This is surely why the majority of us don’t do well at reading faces: we feel no need to make that extra effort. People fail at the nurses tape, Ekman says, because they end up just listening to the words. That’s why, when Tomkins was starting out in his quest to understand the face, he always watched television with the sound turned off. “We are such creatures of language that what we hear takes precedence over what is supposed to be our primary channel of communication, the visual channel,” he once said. “Even though the visual channel provides such enormous information, the fact is that the voice preëmpts the individual’s attention, so that he cannot really see the face while he listens.” We prefer that way of dealing with the world because it does not challenge the ordinary boundaries of human relationships. Ekman, in one of his essays, writes of what he learned from the legendary sociologist Erving Goffman. Goffman said that part of what it means to be civilized is not to “steal” information that is not freely given to us. When someone picks his nose or cleans his ears, out of unthinking habit, we look away. Ekman writes that for Goffman the spoken word is “the acknowledged information, the information for which the person who states it is willing to take responsibility,” and he goes on:

When the secretary who is miserable about a fight with her husband the previous night answers, “Just fine,” when her boss asks, “How are you this morning?”–that false message may be the one relevant to the boss’s interactions with her. It tells him that she is going to do her job. The true message–that she is miserable–he may not care to know about at all as long as she does not intend to let it impair her job performance.

What would the boss gain by reading the subtle and contradictory microexpressions on his secretary’s face? It would be an invasion of her privacy and an act of disrespect. More than that, it would entail an obligation. He would be obliged to do something, or say something, or feel something that might otherwise be avoided entirely. To see what is intended to be hidden, or, at least, what is usually missed, opens up a world of uncomfortable possibilities. This is the hard part of being a face reader. People like that have more faith in their hunches than the rest of us do. But faith is not certainty. Sometimes, on a routine traffic stop late at night, you end up finding out that your hunch was right. But at other times you’ll never know. And you can’t even explain it properly, because what can you say? You did something the rest of us would never have done, based on something the rest of us would never have seen.

“I was working in West Hollywood once, in the nineteen-eighties,” Harms said. “I was with a partner, Scott. I was driving. I had just recently come off the prostitution team, and we spotted a man in drag. He was on Sunset, and I didn’t recognize him. At that time, Sunset was normally for females. So it was kind of odd. It was a cold night in January. There was an all-night restaurant on Sunset called Ben Franks, so I asked my partner to roll down the window and ask the guy if he was going to Ben Franks– just to get a reaction. And the guy immediately keys on Scott, and he’s got an overcoat on, and he’s all bundled up, and he starts walking over to the car. It had been raining so much that the sewers in West Hollywood had backed up, and one of the manhole covers had been cordoned off because it was pumping out water. The guy comes over to the squad car, and he’s walking right through that. He’s fixated on Scott. So we asked him what he was doing. He says, “I was out for a walk.’ And then he says, “I have something to show you.’”

Later, after the incident was over, Harms and his partner learned that the man had been going around Hollywood making serious threats, that he was unstable and had just attempted suicide, that he was in all likelihood about to erupt. A departmental inquiry into the incident would affirm that Harms and his partner had been in danger: the man was armed with a makeshift flamethrower, and what he had in mind, evidently, was to turn the inside of the squad car into an inferno. But at the time all Harms had was a hunch, a sense from the situation and the man’s behavior and what he glimpsed inside the man’s coat and on the man’s face– something that was the opposite of whatever John Yarbrough saw in the face of the boy in Willowbrook. Harms pulled out his gun and shot the man through the open window. “Scott looked at me and was, like, “What did you do?’ because he didn’t perceive any danger,” Harms said. “But I did.”

© 2013 Malcolm Gladwell.

DAVID AND GOLIATH
 
WHAT THE DOG SAW
 
OUTLIERS
 
BLINK
 
THE TIPPING POINT
 
ARTICLES
 
ETC.
The New-Boy Network

Posted May 29, 2000 by MALCOLM GLADWELL & filed under DEPT. OF HUMAN RESOURCES, THE NEW YORKER - ARCHIVE.

What do job interviews really tell us?

1.

Nolan Myers grew up in Houston, the elder of two boys in a middle- class family. He went to Houston’s High School for the Performing and Visual Arts and then Harvard, where he intended to major in History and Science. After discovering the joys of writing code, though, he switched to computer science. “Programming is one of those things you get involved in, and you just can’t stop until you finish,” Myers says. “You get involved in it, and all of a sudden you look at your watch and it’s four in the morning! I love the elegance of it.” Myers is short and slightly stocky and has pale-blue eyes. He smiles easily, and when he speaks he moves his hands and torso for emphasis. He plays in a klezmer band called the Charvard Chai Notes. He talks to his parents a lot. He gets B’s and B-pluses.

This spring, in the last stretch of his senior year, Myers spent a lot of time interviewing for jobs with technology companies. He talked to a company named Trilogy, down in Texas, but he didn’t think he would fit in. “One of Trilogy’s subsidiaries put ads out in the paper saying that they were looking for the top tech students, and that they’d give them two hundred thousand dollars and a BMW,” Myers said, shaking his head in disbelief. In another of his interviews, a recruiter asked him to solve a programming problem, and he made a stupid mistake and the recruiter pushed the answer back across the table to him, saying that his “solution” accomplished nothing. As he remembers the moment, Myers blushes. “I was so nervous. I thought, Hmm, that sucks!” The way he says that, though, makes it hard to believe that he really was nervous, or maybe what Nolan Myers calls nervous the rest of us call a tiny flutter in the stomach. Myers doesn’t seem like the sort to get flustered. He’s the kind of person you would call the night before the big test in seventh grade, when nothing made sense and you had begun to panic.

I like Nolan Myers. He will, I am convinced, be very good at whatever career he chooses. I say those two things even though I have spent no more than ninety minutes in his presence. We met only once, on a sunny afternoon in April at the Au Bon Pain in Harvard Square. He was wearing sneakers and khakis and a polo shirt, in a dark-green pattern. He had a big backpack, which he plopped on the floor beneath the table. I bought him an orange juice. He fished around in his wallet and came up with a dollar to try and repay me, which I refused. We sat by the window. Previously, we had talked for perhaps three minutes on the phone, setting up the interview. Then I E-mailed him, asking him how I would recognize him at Au Bon Pain. He sent me the following message, with what I’m convinced—again, on the basis of almost no evidence—to be typical Myers panache: “22ish, five foot seven, straight brown hair, very good-looking. :) .” I have never talked to his father, his mother, or his little brother, or any of his professors. I have never seen him ecstatic or angry or depressed. I know nothing of his personal habits, his tastes, or his quirks. I cannot even tell you why I feel the way I do about him. He’s good-looking and smart and articulate and funny, but not so good-looking and smart and articulate and funny that there is some obvious explanation for the conclusions I’ve drawn about him. I just like him, and I’m impressed by him, and if I were an employer looking for bright young college graduates, I’d hire him in a heartbeat.

I heard about Nolan Myers from Hadi Partovi, an executive with Tellme, a highly touted Silicon Valley startup offering Internet access through the telephone. If you were a computer-science major at M.I.T., Harvard, Stanford, Caltech, or the University of Waterloo this spring, looking for a job in software, Tellme was probably at the top of your list. Partovi and I talked in the conference room at Tellme’s offices, just off the soaring, open floor where all the firm’s programmers and marketers and executives sit, some of them with bunk beds built over their desks. (Tellme recently moved into an old printing plant—a low- slung office building with a huge warehouse attached—and, in accordance with new-economy logic, promptly turned the old offices into a warehouse and the old warehouse into offices.) Partovi is a handsome man of twenty-seven, with olive skin and short curly black hair, and throughout our entire interview he sat with his chair tilted precariously at a forty-five-degree angle. At the end of a long riff about how hard it is to find high-quality people, he blurted out one name: Nolan Myers. Then, from memory, he rattled off Myers’s telephone number. He very much wanted Myers to come to Tellme.

Partovi had met Myers in January, during a recruiting trip to Harvard. “It was a heinous day,” Partovi remembers. “I started at seven and went until nine. I’d walk one person out and walk the other in.” The first fifteen minutes of every interview he spent talking about Tellme—its strategy, its goals, and its business. Then he gave everyone a short programming puzzle. For the rest of the hour-long meeting, Partovi asked questions. He remembers that Myers did well on the programming test, and after talking to him for thirty to forty minutes he became convinced that Myers had, as he puts it, “the right stuff.” Partovi spent even less time with Myers than I did. He didn’t talk to Myers’s family, or see him ecstatic or angry or depressed, either. He knew that Myers had spent last summer as an intern at Microsoft and was about to graduate from an Ivy League school. But virtually everyone recruited by a place like Tellme has graduated from an élite university, and the Microsoft summer-internship program has more than six hundred people in it. Partovi didn’t even know why he liked Myers so much. He just did. “It was very much a gut call,” he says.

This wasn’t so very different from the experience Nolan Myers had with Steve Ballmer, the C.E.O. of Microsoft. Earlier this year, Myers attended a party for former Microsoft interns called Gradbash. Ballmer gave a speech there, and at the end of his remarks Myers raised his hand. “He was talking a lot about aligning the company in certain directions,” Myers told me, “and I asked him about how that influences his ability to make bets on other directions. Are they still going to make small bets?” Afterward, a Microsoft recruiter came up to Myers and said, “Steve wants your E-mail address.” Myers gave it to him, and soon he and Ballmer were E-mailing. Ballmer, it seems, badly wanted Myers to come to Microsoft. “He did research on me,” Myers says. “He knew which group I was interviewing with, and knew a lot about me personally. He sent me an E-mail saying that he’d love to have me come to Microsoft, and if I had any questions I should contact him. So I sent him a response, saying thank you. After I visited Tellme, I sent him an E-mail saying I was interested in Tellme, here were the reasons, that I wasn’t sure yet, and if he had anything to say I said I’d love to talk to him. I gave him my number. So he called, and after playing phone tag we talked—about career trajectory, how Microsoft would influence my career, what he thought of Tellme. I was extremely impressed with him, and he seemed very genuinely interested in me.”

What convinced Ballmer he wanted Myers? A glimpse! He caught a little slice of Nolan Myers in action and—just like that—the C.E.O. of a four-hundred-billion-dollar company was calling a college senior in his dorm room. Ballmer somehow knew he liked Myers, the same way Hadi Partovi knew, and the same way I knew after our little chat at Au Bon Pain. But what did we know? What could we know? By any reasonable measure, surely none of us knew Nolan Myers at all.

It is a truism of the new economy that the ultimate success of any enterprise lies with the quality of the people it hires. At many technology companies, employees are asked to all but live at the office, in conditions of intimacy that would have been unthinkable a generation ago. The artifacts of the prototypical Silicon Valley office—the videogames, the espresso bar, the bunk beds, the basketball hoops—are the elements of the rec room, not the workplace. And in the rec room you want to play only with your friends. But how do you find out who your friends are?Today, recruiters canvas the country for résumés. They analyze employment histories and their competitors’ staff listings. They call references, and then do what I did with Nolan Myers: sit down with a perfect stranger for an hour and a half and attempt to draw conclusions about that stranger’s intelligence and personality. The job interview has become one of the central conventions of the modern economy. But what, exactly, can you know about a stranger after sitting down and talking with him for an hour?

2.

Some years ago, an experimental psychologist at Harvard University, Nalini Ambady, together with Robert Rosenthal, set out to examine the nonverbal aspects of good teaching. As the basis of her research, she used videotapes of teaching fellows which had been made during a training program at Harvard. Her plan was to have outside observers look at the tapes with the sound off and rate the effectiveness of the teachers by their expressions and physical cues. Ambady wanted to have at least a minute of film to work with. When she looked at the tapes, though, there was really only about ten seconds when the teachers were shown apart from the students. “I didn’t want students in the frame, because obviously it would bias the ratings,” Ambady says. “So I went to my adviser, and I said, ‘This isn’t going to work.’”

But it did. The observers, presented with a ten-second silent video clip, had no difficulty rating the teachers on a fifteen- item checklist of personality traits. In fact, when Ambady cut the clips back to five seconds, the ratings were the same. They were even the same when she showed her raters just two seconds of videotape. That sounds unbelievable unless you actually watch Ambady’s teacher clips, as I did, and realize that the eight seconds that distinguish the longest clips from the shortest are superfluous: anything beyond the first flash of insight is unnecessary. When we make a snap judgment, it is made in a snap. It’s also, very clearly, a judgment:we get a feeling that we have no difficulty articulating.

Ambady’s next step led to an even more remarkable conclusion. She compared those snap judgments of teacher effectiveness with evaluations made, after a full semester of classes, by students of the same teachers. The correlation between the two, she found, was astoundingly high. A person watching a two-second silent video clip of a teacher he has never met will reach conclusions about how good that teacher is that are very similar to those of a student who sits in the teacher’s class for an entire semester.

Recently, a comparable experiment was conducted by Frank Bernieri, a psychologist at the University of Toledo. Bernieri, working with one of his graduate students, Neha Gada-Jain, selected two people to act as interviewers, and trained them for six weeks in the proper procedures and techniques of giving an effective job interview. The two then interviewed ninety-eight volunteers, of various ages and backgrounds. The interviews lasted between fifteen and twenty minutes, and afterward each interviewer filled out a six-page, five-part evaluation of the person he’d just talked to. Originally, the intention of the study was to find out whether applicants who had been coached in certain nonverbal behaviors designed to ingratiate themselves with their interviewers—like mimicking the interviewers’ physical gestures or posture—would get better ratings than applicants who behaved normally. As it turns out, they didn’t. But then another of Bernieri’s students, an undergraduate named Tricia Prickett, decided that she wanted to use the interview videotapes and the evaluations that had been collected to test out the adage that “the handshake is everything.”

“She took fifteen seconds of videotape showing the applicant as he or she knocks on the door, comes in, shakes the hand of the interviewer, sits down, and the interviewer welcomes the person,” Bernieri explained. Then, like Ambady, Prickett got a series of strangers to rate the applicants based on the handshake clip, using the same criteria that the interviewers had used. Once more, against all expectations, the ratings were very similar to those of the interviewers. “On nine out of the eleven traits the applicants were being judged on, the observers significantly predicted the outcome of the interview,” Bernieri says. “The strength of the correlations was extraordinary.”

This research takes Ambady’s conclusions one step further. In the Toledo experiment, the interviewers were trained in the art of interviewing. They weren’t dashing off a teacher evaluation on their way out the door. They were filling out a formal, detailed questionnaire, of the sort designed to give the most thorough and unbiased account of an interview. And still their ratings weren’t all that different from those of people off the street who saw just the greeting.

This is why Hadi Partovi, Steve Ballmer, and I all agreed on Nolan Myers. Apparently, human beings don’t need to know someone in order to believe that they know someone. Nor does it make that much difference, apparently, that Partovi reached his conclusion after putting Myers through the wringer for an hour, I reached mine after ninety minutes of amiable conversation at Au Bon Pain, and Ballmer reached his after watching and listening as Myers asked a question.

Bernieri and Ambady believe that the power of first impressions suggests that human beings have a particular kind of prerational ability for making searching judgments about others. In Ambady’s teacher experiments, when she asked her observers to perform a potentially distracting cognitive task—like memorizing a set of numbers—while watching the tapes, their judgments of teacher effectiveness were unchanged. But when she instructed her observers to think hard about their ratings before they made them, their accuracy suffered substantially. Thinking only gets in the way. “The brain structures that are involved here are very primitive,” Ambady speculates. “All of these affective reactions are probably governed by the lower brain structures.” What we are picking up in that first instant would seem to be something quite basic about a person’s character, because what we conclude after two seconds is pretty much the same as what we conclude after twenty minutes or, indeed, an entire semester. “Maybe you can tell immediately whether someone is extroverted, or gauge the person’s ability to communicate,”Bernieri says. “Maybe these clues or cues are immediately accessible and apparent.” Bernieri and Ambady are talking about the existence of a powerful form of human intuition. In a way, that’s comforting, because it suggests that we can meet a perfect stranger and immediately pick up on something important about him. It means that I shouldn’t be concerned that I can’t explain why I like Nolan Myers, because, if such judgments are made without thinking, then surely they defy explanation.

But there’s a troubling suggestion here as well. I believe that Nolan Myers is an accomplished and likable person. But I have no idea from our brief encounter how honest he is, or whether he is self-centered, or whether he works best by himself or in a group, or any number of other fundamental traits. That people who simply see the handshake arrive at the same conclusions as people who conduct a full interview also implies, perhaps, that those initial impressions matter too much—that they color all the other impressions that we gather over time.

For example, I asked Myers if he felt nervous about the prospect of leaving school for the workplace, which seemed like a reasonable question, since I remember how anxious I was before my first job. Would the hours scare him? Oh no, he replied, he was already working between eighty and a hundred hours a week at school. “Are there things that you think you aren’t good at, which make you worry?” I continued.

His reply was sharp: “Are there things that I’m not good at, or things that I can’t learn? I think that’s the real question. There are a lot of things I don’t know anything about, but I feel comfortable that given the right environment and the right encouragement I can do well at.” In my notes, next to that reply, I wrote “Great answer!” and I can remember at the time feeling the little thrill you experience as an interviewer when someone’s behavior conforms with your expectations. Because I had decided, right off, that I liked him, what I heard in his answer was toughness and confidence. Had I decided early on that I didn’t like Nolan Myers, I would have heard in that reply arrogance and bluster. The first impression becomes a self-fulfilling prophecy: we hear what we expect to hear. The interview is hopelessly biased in favor of the nice.

3.

When Ballmer and Partovi and I met Nolan Myers, we made a prediction. We looked at the way he behaved in our presence—at the way he talked and acted and seemed to think—and drew conclusions about how he would behave in other situations. I had decided, remember, that Myers was the kind of person you called the night before the big test in seventh grade. Was I right to make that kind of generalization?

This is a question that social psychologists have looked at closely. In the late nineteen-twenties, in a famous study, the psychologist Theodore Newcomb analyzed extroversion among adolescent boys at a summer camp. He found that how talkative a boy was in one setting—say, lunch—was highly predictive of how talkative that boy would be in the same setting in the future. A boy who was curious at lunch on Monday was likely to be curious at lunch on Tuesday. But his behavior in one setting told you almost nothing about how he would behave in a different setting: from how someone behaved at lunch, you couldn’t predict how he would behave during, say, afternoon playtime. In a more recent study, of conscientiousness among students at Carleton College, the researchers Walter Mischel, Neil Lutsky, and Philip K. Peake showed that how neat a student’s assignments were or how punctual he was told you almost nothing about how often he attended class or how neat his room or his personal appearance was. How we behave at any one time, evidently, has less to do with some immutable inner compass than with the particulars of our situation.

This conclusion, obviously, is at odds with our intuition. Most of the time, we assume that people display the same character traits in different situations. We habitually underestimate the large role that context plays in people’s behavior. In the Newcomb summer-camp experiment, for example, the results showing how little consistency there was from one setting to another in talkativeness, curiosity, and gregariousness were tabulated from observations made and recorded by camp counsellors on the spot. But when, at the end of the summer, those same counsellors were asked to give their final impressions of the kids, they remembered the children’s behavior as being highly consistent.

“The basis of the illusion is that we are somehow confident that we are getting what is there, that we are able to read off a person’s disposition,” Richard Nisbett, a psychologist at the University of Michigan, says. “When you have an interview with someone and have an hour with them, you don’t conceptualize that as taking a sample of a person’s behavior, let alone a possibly biased sample, which is what it is. What you think is that you are seeing a hologram, a small and fuzzy image but still the whole person.”

Then Nisbett mentioned his frequent collaborator, Lee Ross, who teaches psychology at Stanford. “There was one term when he was teaching statistics and one term he was teaching a course with a lot of humanistic psychology. He gets his teacher evaluations. The first referred to him as cold, rigid, remote, finicky, and uptight. And the second described this wonderful warmhearted guy who was so deeply concerned with questions of community and getting students to grow. It was Jekyll and Hyde. In both cases, the students thought they were seeing the real Lee Ross.”

Psychologists call this tendency—to fixate on supposedly stable character traits and overlook the influence of context—the Fundamental Attri-bution Error, and if you combine this error with what we know about snap judgments the interview becomes an even more problematic encounter. Not only had I let my first impressions color the informationI gathered about Myers, but I had also assumed that the way he behaved with me in an interview setting was indicative of the way he would always behave. It isn’t that the interview is useless; what I learned about Myers—that he and I get along well—is something I could never have got from a résumé or by talking to his references. It’s just that our conversation turns out to have been less useful, and potentially more misleading, than I had supposed. That most basic of human rituals—the conversation with a stranger—turns out to be a minefield.

4.

Not long after I met with Nolan Myers, I talked with a human- resources consultant from Pasadena named Justin Menkes. Menkes’s job is to figure out how to extract meaning from face-to-face encounters, and with that in mind he agreed to spend an hour interviewing me the way he thinks interviewing ought to be done. It felt, going in, not unlike a visit to a shrink, except that instead of having months, if not years, to work things out, Menkes was set upon stripping away my secrets in one session. Consider, he told me, a commonly asked question like “Describe a few situations in which your work was criticized. How did you handle the criticism?” The problem, Menkes said, is that it’s much too obvious what the interviewee is supposed to say. “There was a situation where I was working on a project, and I didn’t do as well as I could have,” he said, adopting a mock-sincere singsong. “My boss gave me some constructive criticism. And I redid the project. It hurt. Yet we worked it out.” The same is true of the question “What would your friends say about you?”—to which the correct answer (preferably preceded by a pause, as if to suggest that it had never dawned on you that someone would ask such a question) is “My guess is that they would call me a people person—either that or a hard worker.”

Myers and I had talked about obvious questions, too. “What is your greatest weakness?” I asked him. He answered, “I tried to work on a project my freshman year, a children’s festival. I was trying to start a festival as a benefit here in Boston. And I had a number of guys working with me. I started getting concerned with the scope of the project we were working on—how much responsibility we had, getting things done. I really put the brakes on, but in retrospect I really think we could have done it and done a great job.”

Then Myers grinned and said, as an aside, “Do I truly think that is a fault? Honestly, no.” And, of course, he’s right. All I’d really asked him was whether he could describe a personal strength as if it were a weakness, and, in answering as he did, he had merely demonstrated his knowledge of the unwritten rules of the interview.

But, Menkes said, what if those questions were rephrased so that the answers weren’t obvious? For example: “At your weekly team meetings, your boss unexpectedly begins aggressively critiquing your performance on a current project. What do you do?”

I felt a twinge of anxiety. What would I do? I remembered a terrible boss I’d had years ago. “I’d probably be upset,” I said. “But I doubt I’d say anything. I’d probably just walk away.” Menkes gave no indication whether he was concerned or pleased by that answer. He simply pointed out that another person might well have said something like “I’d go and see my boss later in private, and confront him about why he embarrassed me in front of my team.” I was saying that I would probably handle criticism—even inappropriate criticism—from a superior with stoicism; in the second case, the applicant was saying he or she would adopt a more confrontational style. Or, at least, we were telling the interviewer that the workplace demands either stoicism or confrontation—and to Menkes these are revealing and pertinent pieces of information.

Menkes moved on to another area—handling stress. A typical question in this area is something like “Tell me about a time when you had to do several things at once. How did you handle the situation? How did you decide what to do first?” Menkes says this is also too easy. “I just had to be very organized,” he began again in his mock-sincere singsong. “I had to multitask. I had to prioritize and delegate appropriately. I checked in frequently with my boss.” Here’s how Menkes rephrased it: “You’re in a situation where you have two very important responsibilities that both have a deadline that is impossible to meet. You cannot accomplish both. How do you handle that situation?”

“Well,” I said, “I would look at the two and decide what I was best at, and then go to my boss and say, ‘It’s better that I do one well than both poorly,’ and we’d figure out who else could do the other task.”

Menkes immediately seized on a telling detail in my answer. I was in-terested in what job I would do best. But isn’t the key issue what job the company most needed to have done? With that comment, I had revealed some-thing valuable: that in a time of work-related crisis I start from a self-centered consideration. “Perhaps you are a bit of a solo practitioner,” Menkes said diplomatically. “That’s an essential bit of information.”

Menkes deliberately wasn’t drawing any broad conclusions. If we are not people who are shy or talkative or outspoken but people who are shy in some contexts, talkative in other situations, and outspoken in still other areas, then what it means to know someone is to catalogue and appreciate all those variations. Menkes was trying to begin that process of cataloguing. This interviewing technique is known as “structured interviewing,” and in studies by industrial psychologists it has been shown to be the only kind of interviewing that has any success at all in predicting performance in the workplace. In the structured interviews, the format is fairly rigid. Each applicant is treated in precisely the same manner. The questions are scripted. The interviewers are carefully trained, and each applicant is rated on a series of predetermined scales.

What is interesting about the structured interview is how narrow its objectives are. When I interviewed Nolan Myers I was groping for some kind of global sense of who he was; Menkes seemed entirely uninterested in arriving at that same general sense of me—he seemed to realize how foolish that expectation was for an hour-long interview. The structured interview works precisely because it isn’t really an interview; it isn’t about getting to know someone, in a traditional sense. It’s as much concerned with rejecting information as it is with collecting it.

Not surprisingly, interview specialists have found it extraordinarily difficult to persuade most employers to adopt the structured interview. It just doesn’t feel right. For most of us, hiring someone is essentially a romantic process, in which the job interview functions as a desexualized version of a date. We are looking for someone with whom we have a certain chemistry, even if the coupling that results ends in tears and the pursuer and the pursued turn out to have nothing in common. We want the unlimited promise of a love affair. The structured interview, by contrast, seems to offer only the dry logic and practicality of an arranged marriage.

5.

Nolan Myers agonized over which job to take. He spent half an hour on the phone with Steve Ballmer, and Ballmer was very persuasive. “He gave me very, very good advice,” Myers says of his conversations with the Microsoft C.E.O. “He felt that I should go to the place that excited me the most and that I thought would be best for my career. He offered to be my mentor.” Myers says he talked to his parents every day about what to do. In February, he flew out to California and spent a Saturday going from one Tellme executive to another, asking and answering questions. “Basically, I had three things I was looking for. One was long-term goals for the company. Where did they see themselves in five years? Second, what position would I be playing in the company?” He stopped and burst out laughing. “And I forget what the third one is.” In March, Myers committed to Tellme.

Will Nolan Myers succeed at Tellme? I think so, although I honestly have no idea. It’s a harder question to answer now than it would have been thirty or forty years ago. If this were 1965, Nolan Myers would have gone to work at I.B.M. and worn a blue suit and sat in a small office and kept his head down, and the particulars of his personality would not have mattered so much. It was not so important that I.B.M. understood who you were before it hired you, because you understood what I.B.M. was. If you walked through the door at Armonk or at a branch office in Illinois, you knew what you had to be and how you were supposed to act. But to walk through the soaring, open offices of Tellme, with the bunk beds over the desks, is to be struck by how much more demanding the culture of Silicon Valley is. Nolan Myers will not be provided with a social script, that blue suit and organization chart. Tellme, like any technology startup these days, wants its employees to be part of a fluid team, to be flexible and innovative, to work with shifting groups in the absence of hierarchy and bureaucracy, and in that environment, where the workplace doubles as the rec room, the particulars of your personality matter a great deal.

This is part of the new economy’s appeal, because Tellme’s soaring warehouse is a more productive and enjoyable place to work than the little office boxes of the old I.B.M. But the danger here is that we will be led astray in judging these newly important particulars of character. If we let personability—some indefinable, prerational intuition, magnified by the Fundamental Attribution Error—bias the hiring process today, then all we will have done is replace the old-boy network, where you hired your nephew, with the new-boy network, where you hire whoever impressed you most when you shook his hand. Social progress, unless we’re careful, can merely be the means by which we replace the obviously arbitrary with the not so obviously arbitrary.

Myers has spent much of the past year helping to teach Introduction to Computer Science. He realized, he says, that one of the reasons that students were taking the course was that they wanted to get jobs in the software industry. “I decided that, having gone through all this interviewing, I had developed some expertise, and I would like to share that. There is a real skill and art in presenting yourself to potential employers. And so what we did in this class was talk about the kinds of things that employers are looking for—what are they looking for in terms of personality. One of the most important things is that you have to come across as being confident in what you are doing and in who you are. How do you do that? Speak clearly and smile.” As he said that, Nolan Myers smiled. “For a lot of people, that’s a very hard skill to learn. But for some reason I seem to understand it intuitively.”

© 2013 Malcolm Gladwell.

DAVID AND GOLIATH
 
WHAT THE DOG SAW
 
OUTLIERS
 
BLINK
 
THE TIPPING POINT
 
ARTICLES
 
ETC.
The Art of Failure

Posted August 21, 2000 by MALCOLM GLADWELL & filed under PERFORMANCE STUDIES, THE NEW YORKER - ARCHIVE.

Why some people choke and others panic.

There was a moment, in the third and deciding set of the 1993 Wimbledon final, when Jana Novotna seemed invincible. She was leading 4-1 and serving at 40-30, meaning that she was one point from winning the game, and just five points from the most coveted championship in tennis. She had just hit a backhand to her opponent, Steffi Graf, that skimmed the net and landed so abruptly on the far side of the court that Graf could only watch, in flat- footed frustration. The stands at Center Court were packed. The Duke and Duchess of Kent were in their customary place in the royal box. Novotna was in white, poised and confident, her blond hair held back with a headband–and then something happened. She served the ball straight into the net. She stopped and steadied herself for the second serve–the toss, the arch of the back–but this time it was worse. Her swing seemed halfhearted, all arm and no legs and torso. Double fault. On the next point, she was slow to react to a high shot by Graf, and badly missed on a forehand volley. At game point, she hit an overhead straight into the net. Instead of 5-1, it was now 4-2. Graf to serve: an easy victory, 4-3. Novotna to serve. She wasn’t tossing the ball high enough. Her head was down. Her movements had slowed markedly. She double-faulted once, twice, three times. Pulled wide by a Graf forehand, Novotna inexplicably hit a low, flat shot directly at Graf, instead of a high crosscourt forehand that would have given her time to get back into position: 4-4. Did she suddenly realize how terrifyingly close she was to victory? Did she remember that she had never won a major tournament before? Did she look across the net and see Steffi Graf–Steffi Graf!–the greatest player of her generation?

On the baseline, awaiting Graf’s serve, Novotna was now visibly agitated, rocking back and forth, jumping up and down. She talked to herself under her breath. Her eyes darted around the court. Graf took the game at love; Novotna, moving as if in slow motion, did not win a single point: 5-4, Graf. On the sidelines, Novotna wiped her racquet and her face with a towel, and then each finger individually. It was her turn to serve. She missed a routine volley wide, shook her head, talked to herself. She missed her first serve, made the second, then, in the resulting rally, mis-hit a backhand so badly that it sailed off her racquet as if launched into flight. Novotna was unrecognizable, not an élite tennis player but a beginner again. She was crumbling under pressure, but exactly why was as baffling to her as it was to all those looking on. Isn’t pressure supposed to bring out the best in us? We try harder. We concentrate harder. We get a boost of adrenaline. We care more about how well we perform. So what was happening to her?

At championship point, Novotna hit a low, cautious, and shallow lob to Graf. Graf answered with an unreturnable overhead smash, and, mercifully, it was over. Stunned, Novotna moved to the net. Graf kissed her twice. At the awards ceremony, the Duchess of Kent handed Novotna the runner-up’s trophy, a small silver plate, and whispered something in her ear, and what Novotna had done finally caught up with her. There she was, sweaty and exhausted, looming over the delicate white-haired Duchess in her pearl necklace. The Duchess reached up and pulled her head down onto her shoulder, and Novotna started to sob.

Human beings sometimes falter under pressure. Pilots crash and divers drown. Under the glare of competition, basketball players cannot find the basket and golfers cannot find the pin. When that happens, we say variously that people have “panicked” or, to use the sports colloquialism, “choked.” But what do those words mean? Both are pejoratives. To choke or panic is considered to be as bad as to quit. But are all forms of failure equal? And what do the forms in which we fail say about who we are and how we think?We live in an age obsessed with success, with documenting the myriad ways by which talented people overcome challenges and obstacles. There is as much to be learned, though, from documenting the myriad ways in which talented people sometimes fail.

“Choking” sounds like a vague and all-encompassing term, yet it describes a very specific kind of failure. For example, psychologists often use a primitive video game to test motor skills. They’ll sit you in front of a computer with a screen that shows four boxes in a row, and a keyboard that has four corresponding buttons in a row. One at a time, x’s start to appear in the boxes on the screen, and you are told that every time this happens you are to push the key corresponding to the box. According to Daniel Willingham, a psychologist at the University of Virginia, if you’re told ahead of time about the pattern in which those x’s will appear, your reaction time in hitting the right key will improve dramatically. You’ll play the game very carefully for a few rounds, until you’ve learned the sequence, and then you’ll get faster and faster. Willingham calls this “explicit learning.” But suppose you’re not told that the x’s appear in a regular sequence, and even after playing the game for a while you’re not aware that there is a pattern. You’ll still get faster: you’ll learn the sequence unconsciously. Willingham calls that “implicit learning”–learning that takes place outside of awareness. These two learning systems are quite separate, based in different parts of the brain. Willingham says that when you are first taught something–say, how to hit a backhand or an overhead forehand–you think it through in a very deliberate, mechanical manner. But as you get better the implicit system takes over: you start to hit a backhand fluidly, without thinking. The basal ganglia, where implicit learning partially resides, are concerned with force and timing, and when that system kicks in you begin to develop touch and accuracy, the ability to hit a drop shot or place a serve at a hundred miles per hour. “This is something that is going to happen gradually,” Willingham says. “You hit several thousand forehands, after a while you may still be attending to it. But not very much. In the end, you don’t really notice what your hand is doing at all.”

Under conditions of stress, however, the explicit system sometimes takes over. That’s what it means to choke. When Jana Novotna faltered at Wimbledon, it was because she began thinking about her shots again. She lost her fluidity, her touch. She double-faulted on her serves and mis-hit her overheads, the shots that demand the greatest sensitivity in force and timing. She seemed like a different person–playing with the slow, cautious deliberation of a beginner–because, in a sense, she was a beginner again: she was relying on a learning system that she hadn’t used to hit serves and overhead forehands and volleys since she was first taught tennis, as a child. The same thing has happened to Chuck Knoblauch, the New York Yankees’ second baseman, who inexplicably has had trouble throwing the ball to first base. Under the stress of playing in front of forty thousand fans at Yankee Stadium, Knoblauch finds himself reverting to explicit mode, throwing like a Little Leaguer again.

Panic is something else altogether. Consider the following account of a scuba-diving accident, recounted to me by Ephimia Morphew, a human-factors specialist at nasa: “It was an open-water certification dive, Monterey Bay, California, about ten years ago. I was nineteen. I’d been diving for two weeks. This was my first time in the open ocean without the instructor. Just my buddy and I. We had to go about forty feet down, to the bottom of the ocean, and do an exercise where we took our regulators out of our mouth, picked up a spare one that we had on our vest, and practiced breathing out of the spare. My buddy did hers. Then it was my turn. I removed my regulator. I lifted up my secondary regulator. I put it in my mouth, exhaled, to clear the lines, and then I inhaled, and, to my surprise, it was water. I inhaled water. Then the hose that connected that mouthpiece to my tank, my air source, came unlatched and air from the hose came exploding into my face.

“Right away, my hand reached out for my partner’s air supply, as if I was going to rip it out. It was without thought. It was a physiological response. My eyes are seeing my hand do something irresponsible. I’m fighting with myself. Don’t do it. Then I searched my mind for what I could do. And nothing came to mind. All I could remember was one thing: If you can’t take care of yourself, let your buddy take care of you. I let my hand fall back to my side, and I just stood there.”

This is a textbook example of panic. In that moment, Morphew stopped thinking. She forgot that she had another source of air, one that worked perfectly well and that, moments before, she had taken out of her mouth. She forgot that her partner had a working air supply as well, which could easily be shared, and she forgot that grabbing her partner’s regulator would imperil both of them. All she had was her most basic instinct: get air. Stress wipes out short-term memory. People with lots of experience tend not to panic, because when the stress suppresses their short- term memory they still have some residue of experience to draw on. But what did a novice like Morphew have? I searched my mind for what I could do. And nothing came to mind.

Panic also causes what psychologists call perceptual narrowing. In one study, from the early seventies, a group of subjects were asked to perform a visual acuity task while undergoing what they thought was a sixty-foot dive in a pressure chamber. At the same time, they were asked to push a button whenever they saw a small light flash on and off in their peripheral vision. The subjects in the pressure chamber had much higher heart rates than the control group, indicating that they were under stress. That stress didn’t affect their accuracy at the visual-acuity task, but they were only half as good as the control group at picking up the peripheral light. “You tend to focus or obsess on one thing,” Morphew says. “There’s a famous airplane example, where the landing light went off, and the pilots had no way of knowing if the landing gear was down. The pilots were so focussed on that light that no one noticed the autopilot had been disengaged, and they crashed the plane.” Morphew reached for her buddy’s air supply because it was the only air supply she could see.

Panic, in this sense, is the opposite of choking. Choking is about thinking too much. Panic is about thinking too little. Choking is about loss of instinct. Panic is reversion to instinct. They may look the same, but they are worlds apart.

Why does this distinction matter? In some instances, it doesn’t much. If you lose a close tennis match, it’s of little moment whether you choked or panicked; either way, you lost. But there are clearly cases when how failure happens is central to understanding why failure happens.

Take the plane crash in which John F. Kennedy, Jr., was killed last summer. The details of the flight are well known. On a Friday evening last July, Kennedy took off with his wife and sister-in-law for Martha’s Vineyard. The night was hazy, and Kennedy flew along the Connecticut coastline, using the trail of lights below him as a guide. At Westerly, Rhode Island, he left the shoreline, heading straight out over Rhode Island Sound, and at that point, apparently disoriented by the darkness and haze, he began a series of curious maneuvers: He banked his plane to the right, farther out into the ocean, and then to the left. He climbed and descended. He sped up and slowed down. Just a few miles from his destination, Kennedy lost control of the plane, and it crashed into the ocean.

Kennedy’s mistake, in technical terms, was that he failed to keep his wings level. That was critical, because when a plane banks to one side it begins to turn and its wings lose some of their vertical lift. Left unchecked, this process accelerates. The angle of the bank increases, the turn gets sharper and sharper, and the plane starts to dive toward the ground in an ever-narrowing corkscrew. Pilots call this the graveyard spiral. And why didn’t Kennedy stop the dive? Because, in times of low visibility and high stress, keeping your wings level–indeed, even knowing whether you are in a graveyard spiral–turns out to be surprisingly difficult. Kennedy failed under pressure.

Had Kennedy been flying during the day or with a clear moon, he would have been fine. If you are the pilot, looking straight ahead from the cockpit, the angle of your wings will be obvious from the straight line of the horizon in front of you. But when it’s dark outside the horizon disappears. There is no external measure of the plane’s bank. On the ground, we know whether we are level even when it’s dark, because of the motion-sensing mechanisms in the inner ear. In a spiral dive, though, the effect of the plane’s G-force on the inner ear means that the pilot feels perfectly level even if his plane is not. Similarly, when you are in a jetliner that is banking at thirty degrees after takeoff, the book on your neighbor’s lap does not slide into your lap, nor will a pen on the floor roll toward the “down” side of the plane. The physics of flying is such that an airplane in the midst of a turn always feels perfectly level to someone inside the cabin.

This is a difficult notion, and to understand it I went flying with William Langewiesche, the author of a superb book on flying, “Inside the Sky.” We met at San Jose Airport, in the jet center where the Silicon Valley billionaires keep their private planes. Langewiesche is a rugged man in his forties, deeply tanned, and handsome in the way that pilots (at least since the movie “The Right Stuff”) are supposed to be. We took off at dusk, heading out toward Monterey Bay, until we had left the lights of the coast behind and night had erased the horizon. Langewiesche let the plane bank gently to the left. He took his hands off the stick. The sky told me nothing now, so I concentrated on the instruments. The nose of the plane was dropping. The gyroscope told me that we were banking, first fifteen, then thirty, then forty-five degrees. “We’re in a spiral dive,” Langewiesche said calmly. Our airspeed was steadily accelerating, from a hundred and eighty to a hundred and ninety to two hundred knots. The needle on the altimeter was moving down. The plane was dropping like a stone, at three thousand feet per minute. I could hear, faintly, a slight increase in the hum of the engine, and the wind noise as we picked up speed. But if Langewiesche and I had been talking I would have caught none of that. Had the cabin been unpressurized, my ears might have popped, particularly as we went into the steep part of the dive. But beyond that? Nothing at all. In a spiral dive, the G-load–the force of inertia–is normal. As Langewiesche puts it, the plane likes to spiral-dive. The total time elapsed since we started diving was no more than six or seven seconds. Suddenly, Langewiesche straightened the wings and pulled back on the stick to get the nose of the plane up, breaking out of the dive. Only now did I feel the full force of the G-load, pushing me back in my seat. “You feel no G-load in a bank,” Langewiesche said. “There’s nothing more confusing for the uninitiated.”

I asked Langewiesche how much longer we could have fallen. “Within five seconds, we would have exceeded the limits of the airplane,” he replied, by which he meant that the force of trying to pull out of the dive would have broken the plane into pieces. I looked away from the instruments and asked Langewiesche to spiral-dive again, this time without telling me. I sat and waited. I was about to tell Langewiesche that he could start diving anytime, when, suddenly, I was thrown back in my chair. “We just lost a thousand feet,” he said.

This inability to sense, experientially, what your plane is doing is what makes night flying so stressful. And this was the stress that Kennedy must have felt when he turned out across the water at Westerly, leaving the guiding lights of the Connecticut coastline behind him. A pilot who flew into Nantucket that night told the National Transportation Safety Board that when he descended over Martha’s Vineyard he looked down and there was “nothing to see. There was no horizon and no light…. I thought the island might [have] suffered a power failure.” Kennedy was now blind, in every sense, and he must have known the danger he was in. He had very little experience in flying strictly by instruments. Most of the time when he had flown up to the Vineyard the horizon or lights had still been visible. That strange, final sequence of maneuvers was Kennedy’s frantic search for a clearing in the haze. He was trying to pick up the lights of Martha’s Vineyard, to restore the lost horizon. Between the lines of the National Transportation Safety Board’s report on the crash, you can almost feel his desperation:

About 2138 the target began a right turn in a southerly direction. About 30 seconds later, the target stopped its descent at 2200 feet and began a climb that lasted another 30 seconds. During this period of time, the target stopped the turn, and the airspeed decreased to about 153 KIAS. About 2139, the target leveled off at 2500 feet and flew in a southeasterly direction. About 50 seconds later, the target entered a left turn and climbed to 2600 feet. As the target continued in the left turn, it began a descent that reached a rate of about 900 fpm.

But was he choking or panicking? Here the distinction between those two states is critical. Had he choked, he would have reverted to the mode of explicit learning. His movements in the cockpit would have become markedly slower and less fluid. He would have gone back to the mechanical, self-conscious application of the lessons he had first received as a pilot–and that might have been a good thing. Kennedy needed to think, to concentrate on his instruments, to break away from the instinctive flying that served him when he had a visible horizon.

But instead, from all appearances, he panicked. At the moment when he needed to remember the lessons he had been taught about instrument flying, his mind–like Morphew’s when she was underwater–must have gone blank. Instead of reviewing the instruments, he seems to have been focussed on one question: Where are the lights of Martha’s Vineyard? His gyroscope and his other instruments may well have become as invisible as the peripheral lights in the underwater-panic experiments. He had fallen back on his instincts–on the way the plane felt–and in the dark, of course, instinct can tell you nothing. The N.T.S.B. report says that the last time the Piper’s wings were level was seven seconds past 9:40, and the plane hit the water at about 9:41, so the critical period here was less than sixty seconds. At twenty-five seconds past the minute, the plane was tilted at an angle greater than forty-five degrees. Inside the cockpit it would have felt normal. At some point, Kennedy must have heard the rising wind outside, or the roar of the engine as it picked up speed. Again, relying on instinct, he might have pulled back on the stick, trying to raise the nose of the plane. But pulling back on the stick without first levelling the wings only makes the spiral tighter and the problem worse. It’s also possible that Kennedy did nothing at all, and that he was frozen at the controls, still frantically searching for the lights of the Vineyard, when his plane hit the water. Sometimes pilots don’t even try to make it out of a spiral dive. Langewiesche calls that “one G all the way down.”

What happened to Kennedy that night illustrates a second major difference between panicking and choking. Panicking is conventional failure, of the sort we tacitly understand. Kennedy panicked because he didn’t know enough about instrument flying. If he’d had another year in the air, he might not have panicked, and that fits with what we believe–that performance ought to improve with experience, and that pressure is an obstacle that the diligent can overcome. But choking makes little intuitive sense. Novotna’s problem wasn’t lack of diligence; she was as superbly conditioned and schooled as anyone on the tennis tour. And what did experience do for her? In 1995, in the third round of the French Open, Novotna choked even more spectacularly than she had against Graf, losing to Chanda Rubin after surrendering a 5-0 lead in the third set. There seems little doubt that part of the reason for her collapse against Rubin was her collapse against Graf–that the second failure built on the first, making it possible for her to be up 5-0 in the third set and yet entertain the thought I can still lose. If panicking is conventional failure, choking is paradoxical failure.

Claude Steele, a psychologist at Stanford University, and his colleagues have done a number of experiments in recent years looking at how certain groups perform under pressure, and their findings go to the heart of what is so strange about choking. Steele and Joshua Aronson found that when they gave a group of Stanford undergraduates a standardized test and told them that it was a measure of their intellectual ability, the white students did much better than their black counterparts. But when the same test was presented simply as an abstract laboratory tool, with no relevance to ability, the scores of blacks and whites were virtually identical. Steele and Aronson attribute this disparity to what they call “stereotype threat”: when black students are put into a situation where they are directly confronted with a stereotype about their group–in this case, one having to do with intelligence–the resulting pressure causes their performance to suffer.

Steele and others have found stereotype threat at work in any situation where groups are depicted in negative ways. Give a group of qualified women a math test and tell them it will measure their quantitative ability and they’ll do much worse than equally skilled men will; present the same test simply as a research tool and they’ll do just as well as the men. Or consider a handful of experiments conducted by one of Steele’s former graduate students, Julio Garcia, a professor at Tufts University. Garcia gathered together a group of white, athletic students and had a white instructor lead them through a series of physical tests: to jump as high as they could, to do a standing broad jump, and to see how many pushups they could do in twenty seconds. The instructor then asked them to do the tests a second time, and, as you’d expect, Garcia found that the students did a little better on each of the tasks the second time around. Then Garcia ran a second group of students through the tests, this time replacing the instructor between the first and second trials with an African-American. Now the white students ceased to improve on their vertical leaps. He did the experiment again, only this time he replaced the white instructor with a black instructor who was much taller and heavier than the previous black instructor. In this trial, the white students actually jumped less high than they had the first time around. Their performance on the pushups, though, was unchanged in each of the conditions. There is no stereotype, after all, that suggests that whites can’t do as many pushups as blacks. The task that was affected was the vertical leap, because of what our culture says: white men can’t jump.

It doesn’t come as news, of course, that black students aren’t as good at test-taking as white students, or that white students aren’t as good at jumping as black students. The problem is that we’ve always assumed that this kind of failure under pressure is panic. What is it we tell underperforming athletes and students? The same thing we tell novice pilots or scuba divers: to work harder, to buckle down, to take the tests of their ability more seriously. But Steele says that when you look at the way black or female students perform under stereotype threat you don’t see the wild guessing of a panicked test taker. “What you tend to see is carefulness and second-guessing,” he explains. “When you go and interview them, you have the sense that when they are in the stereotype-threat condition they say to themselves, ‘Look, I’m going to be careful here. I’m not going to mess things up.’ Then, after having decided to take that strategy, they calm down and go through the test. But that’s not the way to succeed on a standardized test. The more you do that, the more you will get away from the intuitions that help you, the quick processing. They think they did well, and they are trying to do well. But they are not.” This is choking, not panicking. Garcia’s athletes and Steele’s students are like Novotna, not Kennedy. They failed because they were good at what they did: only those who care about how well they perform ever feel the pressure of stereotype threat. The usual prescription for failure–to work harder and take the test more seriously–would only make their problems worse.

That is a hard lesson to grasp, but harder still is the fact that choking requires us to concern ourselves less with the performer and more with the situation in which the performance occurs. Novotna herself could do nothing to prevent her collapse against Graf. The only thing that could have saved her is if–at that critical moment in the third set–the television cameras had been turned off, the Duke and Duchess had gone home, and the spectators had been told to wait outside. In sports, of course, you can’t do that. Choking is a central part of the drama of athletic competition, because the spectators have to be there–and the ability to overcome the pressure of the spectators is part of what it means to be a champion. But the same ruthless inflexibility need not govern the rest of our lives. We have to learn that sometimes a poor performance reflects not the innate ability of the performer but the complexion of the audience; and that sometimes a poor test score is the sign not of a poor student but of a good one.

Through the first three rounds of the 1996 Masters golf tournament, Greg Norman held a seemingly insurmountable lead over his nearest rival, the Englishman Nick Faldo. He was the best player in the world. His nickname was the Shark. He didn’t saunter down the fairways; he stalked the course, blond and broad-shouldered, his caddy behind him, struggling to keep up. But then came the ninth hole on the tournament’s final day. Norman was paired with Faldo, and the two hit their first shots well. They were now facing the green. In front of the pin, there was a steep slope, so that any ball hit short would come rolling back down the hill into oblivion. Faldo shot first, and the ball landed safely long, well past the cup.

Norman was next. He stood over the ball. “The one thing you guard against here is short,” the announcer said, stating the obvious. Norman swung and then froze, his club in midair, following the ball in flight. It was short. Norman watched, stone-faced, as the ball rolled thirty yards back down the hill, and with that error something inside of him broke.

At the tenth hole, he hooked the ball to the left, hit his third shot well past the cup, and missed a makable putt. At eleven, Norman had a three-and-a-half-foot putt for par–the kind he had been making all week. He shook out his hands and legs before grasping the club, trying to relax. He missed: his third straight bogey. At twelve, Norman hit the ball straight into the water. At thirteen, he hit it into a patch of pine needles. At sixteen, his movements were so mechanical and out of synch that, when he swung, his hips spun out ahead of his body and the ball sailed into another pond. At that, he took his club and made a frustrated scythelike motion through the grass, because what had been obvious for twenty minutes was now official: he had fumbled away the chance of a lifetime.

Faldo had begun the day six strokes behind Norman. By the time the two started their slow walk to the eighteenth hole, through the throng of spectators, Faldo had a four- stroke lead. But he took those final steps quietly, giving only the smallest of nods, keeping his head low. He understood what had happened on the greens and fairways that day. And he was bound by the particular etiquette of choking, the understanding that what he had earned was something less than a victory and what Norman had suffered was something less than a defeat.

When it was all over, Faldo wrapped his arms around Norman. “I don’t know what to say–I just want to give you a hug,” he whispered, and then he said the only thing you can say to a choker: “I feel horrible about what happened. I’m so sorry.” With that, the two men began to cry.

© 2013 Malcolm Gladwell.

DAVID AND GOLIATH
 
WHAT THE DOG SAW
 
OUTLIERS
 
BLINK
 
THE TIPPING POINT
 
ARTICLES
 
ETC.
Do Parents Matter?

Posted August 17, 1998 by MALCOLM GLADWELL & filed under ANNALS OF BEHAVIOR, THE NEW YORKER - ARCHIVE.

Judith Rich Harris and child development

1.

The idea that will make Judith Rich Harris famous came to her, unbidden, on the afternoon of January 20, 1994. At the time, Harris was a textbook writer, with no doctorate or academic affiliation, working from her home in suburban New Jersey. Because of a lupus-like illness, she doesn’t have the strength to leave the house, and she’d spent that morning in bed. By early afternoon, though, she was at her desk, glancing through a paper by a prominent psychologist about juvenile delinquency, and for some reason a couple of unremarkable sentences struck her as odd: “Delinquency must be a social behavior that allows access to some desirable resource. I suggest that the resource is mature status, with its consequent power and privilege.” It is an observation consistent with our ideas about what it means to grow up. Teen-agers rebel against being teen-agers, against the restrictions imposed on them by adults. They smoke because only adults are supposed to smoke. They steal cars because they are too young to have cars. But Harris was suddenly convinced that the paper had it backward. “Adolescents aren’t trying to be like adults–they are trying to contrast themselves with adults,” she explains. “And it was as if a light had gone on in the sky. It was one of the most exciting things that have ever happened to me. In a minute or two, I had the germ of the theory, and in ten minutes I had enough of it to see that it was important.”

If adolescents didn’t want to be like adults, it was because they wanted to be like other adolescents. Children were identifying with and learning from other children, and Harris realized that once you granted that fact all the conventional wisdom about parents and family and child-rearing started to unravel. Why, for example, do the children of recent immigrants almost never retain the accents of their parents? How is it that the children of deaf parents manage to learn how to speak as well as children whose parents speak to them from the day they were born? The answer has always been that language is a skill acquired laterally–that what children pick up from other children is at least as important as what they pick up at home. Harris was asking whether this was true more generally: what if children also learn the things that make them who they are–that shape their characters and personalities–from their peer group? This would mean that, in some key sense, parents don’t much matter–that what’s important is not what children learn inside the home but what they learn outside the home.

“I was sitting and thinking,” Harris told me, looking bright-eyed as she clutched a tall glass of lemonade. She is tiny–a fragile, elfin grandmother with a mop of gray hair and a little-girl voice. We were in her kitchen, looking out on the green of her back yard. “I told my husband, Charlie, about it. I had signed a contract to write a developmental-psychology textbook, and I wasn’t quite ready to give it up. But the more I thought about it the more I realized I couldn’t go on writing developmental-psychology textbooks, because I could no longer say what my publishers wanted me to say.” Over the next six months, Harris immersed herself in the literature of social psychology and cultural anthropology. She read studies of group behavior in primates and unearthed studies from the nineteen-fifties of pre-adolescent boys. She couldn’t conduct any experiments of her own, because she didn’t belong to an academic institution. She couldn’t even use a proper academic library, because the closest university to her was Rutgers, which was forty-five minutes away, and she didn’t have the strength to leave her house for more than a few hours at a time. So she went to the local public library and ordered academic texts through interlibrary loan and sent for reprints of scientific articles through the mail, and the more she read the more she became convinced that her theory could tie together many of the recent puzzling findings in behavioral genetics and developmental psychology. In six weeks, in August and September of 1994, she wrote a draft and sent it off to the academic journal Psychological Review. It was an act of singular audacity, because Psychological Review is one of the most prestigious journals in psychology, and prestigious academic journals do not, as a rule, publish the musings of stay-at-home grandmothers without Ph.D.s. But her article was accepted, and in the space below her name, where authors typically put “Princeton University” or “Yale University” or “Oxford University,” Harris proudly put “Middletown, New Jersey.” Harris listed her CompuServe address in a footnote, and soon she was inundated with E-mail, because what she had to say was so compelling and so surprising and, in a wholly unexpected way, so sensible that everyone in the field wanted to know more. Who are you? scholars asked. Where did you come from? Why have I never heard of you before?

At this point, Harris’s health was not good. Her autoimmune disorder began to attack her heart and lungs, and she sometimes wondered how long she had to live. But, at the urging of some of her new friends in academe, she set out to write a book, and somehow in the writing of it she became stronger. That book, “The Nurture Assumption,” will be published this fall, and it is a graceful, lucid, and utterly persuasive assault on virtually every tenet of child development. It begins, “This book has two purposes: first, to dissuade you of the notion that a child’s personality–what used to be called ‘character’–is shaped or modified by the child’s parents; and second, to give you an alternative view of how the child’s personality is shaped.” On the back cover are enthusiastic blurbs from David Lykken, of the University of Minnesota; Robert Sapolsky, of Stanford; Dean Keith Simonton, of the University of California at Davis; John Bruer, of the James S. McDonnell Foundation; and Steven Pinker, of MIT–which, in the social-science business, is a bit like writing a book on basketball and having it endorsed by the starting five of the Chicago Bulls. This week, Harris will travel to San Francisco for the annual convention of the American Psychological Association, where she will receive a prize for her Psychological Review article.

“It’s as if the gods were making up to me all that they had done to me previously,” Harris told me. “It was the best gift I could have ever gotten: an idea. It wasn’t something that I could have known in advance. But, as it turned out, it was what I wanted most in the world–an idea that would give a direction and a purpose to my life.”

2.

Judith Harris’s big idea–that peers matter much more than parents–runs counter to nearly everything that a century of psychology and psychotherapy has told us about human development. Freud put parents at the center of the child’s universe, and there they have remained ever since. “They fuck you up, your mum and dad. They may not mean to, but they do,” the poet Philip Larkin memorably wrote, and that perspective is fundamental to the way we have been taught to understand ourselves. When we go to a therapist, we talk about our parents, in the hope that coming to grips with the events of childhood can help us decipher the mysteries of adulthood. When we say things like “That’s the way I was raised,” we mean that children instinctively and preferentially learn from their parents, that parents can be good or bad role models for children, that character and personality are passed down from one generation to the next. Child development has been, in many ways, concerned with understanding children through their parents.

In recent years, however, this idea has run into a problem. In a series of careful and comprehensive studies (among them the famous Minnesota studies of twins separated at birth) behavioral geneticists have concluded that about fifty per cent of the personality differences among people–traits such as friendliness, extroversion, nervousness, openness, and so on–are attributable to our genes, which means that the other half must be attributable to the environment. Yet when researchers have set out to look for this environmental influence they haven’t been able to find it. If the example of parents were important in a child’s development, you’d expect to see a consistent difference between the children of anxious and inexperienced parents and the children of authoritative and competent parents, even after taking into account the influence of heredity. Children who spend two hours a day with their parents should be different from children who spend eight hours a day with their parents. A home with lots of books should result in a different kind of child from a home with very few books. In other words, researchers should have been able to find some causal link between the specific social environment parents create for their children and the way those children turn out. They haven’t.

One of the largest and most rigorous studies of this kind is known as the Colorado Adoption Project. Between 1975 and 1982, a group of researchers at the University of Colorado, headed by Robert Plomin, one of the world’s leading behavioral geneticists, recruited two hundred and forty-five pregnant women from the Denver area who planned to give up their children for adoption. The researchers then followed the children into their new homes, giving them a battery of personality and intelligence tests at regular intervals throughout their childhood and giving similar tests to their adoptive parents. For the sake of comparison, the group also ran the same set of tests on a control group of two hundred and forty-five parents and their biological children. For the latter group, the results were pretty much as one might expect: in intellectual ability and certain aspects of personality, the kids proved to be fairly similar to their parents. The scores of the adopted kids, however, had nothing whatsoever in common with the scores of their adoptive parents: these children were no more similar in personality or intellectual skills to the people who reared them, fed them, clothed them, read to them, taught them, and loved them all their lives than they were to any two adults taken at random off the street.

Here is the puzzle. We think that children resemble their parents because of both genes and the home environment, both nature and nurture. But, if nurture matters even a little, why don’t the adopted kids have at least some greater-than-chance similarities to their adoptive parents? The Colorado study says that the only reason we are like our parents is that we share their genes, and that–by any measures of cognition and personality–when there is no genetic inheritance there is no resemblance.

This is the question that so preoccupied Harris on that winter morning four and a half years ago. She knew that most people in psychology had responded to findings like those of the Colorado project by turning an ever more powerful microscope on the family, assuming that if we couldn’t see the influence of parents through standard psychological measures it was because we weren’t looking hard enough. Not looking hard enough wasn’t the problem. The problem was that psychologists weren’t looking in the right place. They were looking inside the home when they should have been looking outside the home. The answer wasn’t parents; it was peers.

Harris argues that we have been in the grip of what she calls the “nurture assumption,” a parent-centered bias that has blinded us to what really matters in human development. Consider, she says, the seemingly common-sense statements “Children who are hugged are more likely to be nice” and “Children who are beaten are more likely to be unpleasant.” Sure enough, if you study nice, well-adjusted children, it turns out that they generally have well-adjusted and nice parents. But what does this really mean? Since genes account for about half of personality variations among people, it’s quite possible that nice children are nice simply because they received nice genes from their parents–and nice parents are going to be nice to their children. Hugging may have made the children happy, and it may have taught them a good way of expressing their affection, but it may not have been what made them nice. Or take the example of smoking. The children of smokers are more than twice as likely to smoke as the children of nonsmokers, so it’s natural to conclude that parents who smoke around their children set an example that their kids follow. In fact, a lot of parents who smoke feel guilty about it for that very reason. But if parents really cause smoking there ought to be elevated rates of smoking among the adopted children of smokers, and there aren’t. It turns out that nicotine addiction is heavily influenced by genes, and the reason that so many children of smokers smoke is that they have inherited a genetic susceptibility to tobacco from their parents. David C. Rowe, a professor of family studies at the University of Arizona (whose academic work on the limits of family influence Harris says was critical to her own thinking), has analyzed research into this genetic contribution, and he concludes that it accounts entirely for the elevated levels of cigarette use among the children of smokers. With smoking, as with niceness, what parents do seems to be nearly irrelevant.

Harris makes another, subtler point about parents. What if, she asks, the cause-and-effect assumption with niceness and hugging can also go the other way? What if, all other things being equal, nice children tend to be hugged because they are nice, and unpleasant children tend to be beaten because they are unpleasant? Children, after all, are born with individual temperaments. Some children are easy to rear from the start and others are difficult, and those innate characteristics, she says, can strongly influence how parents treat them. Harris tells a story about a mother with two young children–a five-year-old girl, named Audrey, and a seven-year-old boy, named Mark–who walked by Harris’s house one day when she was out in the front yard with her dog, Page. Page ran toward the children, barking menacingly. Audrey went up to the animal and asked her mother, “Can I pet him?” Her mother quickly told her not to. Mark, meanwhile, was cowering on the other side of the street, and he stayed there even after Harris rushed up and grabbed Page by the collar. “Come on, Mark, the dog won’t hurt you,” the mother said, and she waited for her son to come back across the street. What is the parenting “style” here that is supposedly so important in shaping personality? This mother is playing two very different roles–coaxing the frightened Mark and reining in the brash Audrey–and in each case her behavior is shaped by the actions and the temperament of her child, and not the other way around.

This phenomenon–what Harris calls child-to-parent effects–has been explored in detail by psychological researchers. David Reiss, of George Washington University, and Robert Plomin, the behavioral geneticist who headed the Colorado study, and a number of colleagues have just completed a ten-year, nine-million-dollar study of seven hundred and twenty American families. Thirty-two teams of testers were recruited, and they visited each family three times in the course of three years, giving parents and siblings personality tests, videotaping interactions between parents and children, questioning teachers, asking siblings about siblings, asking parents about children, asking children about parents–all to find out whether the differences in how parents relate to each of their children make any predictable difference in the way those children end up. “We thought that this was going to be a straight shot,” Reiss told me. “The sibling who got the better micro environment would do better, be less depressed, be less antisocial. It seemed like a no-brainer.” It wasn’t. Plomin told me, “If we just ask the simple question ‘Does differential parental treatment relate to differences in adolescent adjustment?’ the answer is yes–hugely. If you take negative parents–conflict, hostility–it’s the strongest predictor of negative adjustment of the siblings.” But the study was designed to look at genetic influences as well–to examine whether children had personality traits that were causing parental behavior–and when those genetic factors were taken into consideration the link between negative parenting and problems in adolescence almost entirely disappeared. “The parents’ negativity isn’t causing the negative adjustment of the kids,” Plomin said. “It’s reflecting it. This was a tremendous surprise to us.” What looks like nurture is sometimes just nature, and what looks like a cause is sometimes just an effect.

3.

Harris takes this argument one step further. Consider, she says, the story of Cinderella:

The folks who gave us this tale ask us to accept the following premises: that Cinderella was able to go to the ball and not be recognized by her stepsisters, that despite years of degradation she was able to charm and hold the attention of a sophisticated guy like the prince, that the prince didn’t recognize her when he saw her again in her own home dressed in her workaday clothing, and that he never doubted that Cinderella would be able to fulfill the duties of a princess and, ultimately, of a queen.

If you think of the influence of parents and the home environment as monolithic, this tale does seem impossibly far-fetched. So why does the Cinderella story work? Because, Harris says, all of us understand that it is possible to be one person to our parents and another person to our friends. “Cinderella learned whenshe was still quite small that it was best to act meek when her stepmother was around, and to look unattractive in order to avoid arousing her jealousy,” Harris writes. But outside the house Cinderella learned that she could win friends by being pretty and charming. Harris says that this lesson–that away from our parents we can reconstruct ourselves–is one that all children learn very quickly, and it is an important limitation on the power of parents: even when they do succeed in influencing their children, those influences very often don’t travel outside the home.

The Cinderella effect shows up all the time in psychological research. For example, Harris notes that in the August, 1997, issue of the Archives of Pediatrics and Adolescent Medicine there is a study showing that the more mothers spanked their kids, the more troublesome the kids became. “When parents use corporal punishment to reduce antisocial behavior,” the researchers report, “the long-term effect tends to be the opposite.” These findings made headlines across the country. In the same issue of that journal, however, another study of children and corporal punishment reached the opposite conclusion: “For most children claims that spanking teaches aggression seem unfounded.” The disparity is baffling until you remember the Cinderella effect. The first study asked mothers to evaluate their children’s behavior at home. Not surprisingly, it suggested that repeated spanking contributes to the kind of negative relationship that causes further misbehavior. The second study, however, asked kids how often they got into fights at school, and the world of school is a very different place from the world of home. Just the fact that a child wasn’t getting along with his mother didn’t necessarily mean that he wouldn’t get along with his peers.

In another instance, Harris cites a Swedish study of picky eating among primary-school children. Some kids were picky eaters at school, some were picky at home, but only a small number were picky at home and school. A child who pushes away broccoli at the kitchen table might gobble it down in the school cafeteria. In the same way, a child might be shy and retiring at home but a chatterbox in the classroom. Harris applies the same logic to birth-order effects–the popular idea that a good part of our personality is determined by where we stand in relation to our siblings. “At home there are birth order effects, no question about it, and I believe that is why it’s so hard to shake people’s faith in them,” Harris writes. “If you see people with their parents or their siblings, you do see the differences you expect to see. The oldest does seem more serious, responsible, and bossy. The youngest does behave in a more carefree fashion.” But that’s only at home. Studies that look at the way people act outside the home, and away from the parents and siblings, don’t see any consistent effects at all. The younger brother cowed by his older siblings all his years of growing up is perfectly capable of being a dominant, take-charge figure when he’s among his friends. “Socialization research has demonstrated one thing clearly and irrefutably: a parent’s behavior toward a child affects how the child behaves in the presence of the parent or in contexts that are associated with the parent,” Harris concludes. “I have no problem with that–I agree with it. The parent’s behavior also affects the way the child feels about the parent. When a parent favors one child over another, not only does it cause hard feelings between the children–it also causes the unfavored child to harbor hard feelings against the parent. These feelings can last a lifetime.” But they don’t necessarily cross over into the life the child leads outside the home–the place where adults spend the majority of their lives.

4.

Not long ago, Anne-Marie Ambert, a sociologist at York University, in Ontario, asked her students to write short autobiographies describing, among other things, the events in their lives which made them most unhappy. Nine per cent identified something that their parents had done, while more than a third pointed to the way they had been treated by peers. Ambert concluded:

There is far more negative treatment by peers than by parents…. In these autobiographies, one reads accounts of students who had been happy and well adjusted, but quite rapidly began deteriorating psychologically, sometimes to the point of becoming physically ill and incompetent in school, after experiences such as being rejected by peers, excluded, talked about, racially discriminated against, laughed at, bullied, sexually harassed, taunted, chased or beaten.

This is Harris’s argument in a nutshell: that whatever our parents do to us is overshadowed, in the long run, by what our peers do to us. In “The Nurture Assumption,”Harris pulls together an extraordinary range of studies and observations to support this idea. Here, for example, is Harris on delinquency. First, she cites a study of juvenile delinquency–vandalism, theft, assault, weapons possession, and so on–among five hundred elementary-school and middle-school boys in Pittsburgh. The study found that African-American boys, many of them from poor, single-parent, “high-risk” families, committed far more delinquent acts than the white kids. That much isn’t surprising. But when the researchers divided up the black boys by neighborhood the effect of coming from a putatively high-risk family disappeared. Black kids who didn’t live in the poorest, underclass neighborhoods–even if they were from poor, single-parent families–were no more delinquent than their white, mostly middle-class peers. At the same time, Harris cites another large study–one that compared the behavior of poor inner-city kids from intact families to the behavior of those living only with their mothers. You’d assume that a child is always better off in a two-parent home, but the research doesn’t bear that out. “Adolescent males in this sample who lived in single-mother households did not differ from youth living in other family constellations in their alcohol and substance use, delinquency, school dropout, or psychological distress,” the study concluded. A child is better off, in other words, living in a troubled family in a good neighborhood than living in a good family in a troubled neighborhood. Peers trump parents.

Other studies have shown that children living without their biological fathers are more likely to drop out of school and, if female, to get pregnant in their teens. But is this because of the absence of a parent, Harris asks, or is it because of some factor that is merely associated with the absence of a parent? Having a stepfather around, for example, doesn’t make a kid any less likely to be unemployed, to drop out, or to be a teen-age mother. Nor does having lots of contact with one’s biological father after he has left. Nor does having another biological relative–a grandparent, for instance–in the home. Nor does it seem to matter when the father leaves: kids whose parents split up when they were in their early teens are no better off and no worse off than kids whose fathers left when they were infants. And, curiously, children whose fathers die aren’t worse off at all. In short, there isn’t a lot of evidence that the loss of adult guidance and role models caused by fatherlessness has specific behavioral consequences. So what is it? One obvious factor is income: single mothers have less money than married mothers, and income has a big effect on the welfare of children. If your parents split up and you move from Riverdale to the South Bronx, you’re obviously going to be a lot worse off–although it’s not the loss of your father that makes the difference. This brings us to another factor: relocation. Single-parent families move more often than intact families, and, according to one major study, those extra changes of residence could account for more than half the increased risk of dropping out, of teen-age pregnancy, and of unemployment among the children of divorce. The problem with divorce, in short, is not so much that it disrupts kids’ relationships with their parents as that it disrupts kids’ relationships with other kids. “Moving is rough on kids,” Harris writes. “Kids who have been moved around a lot–whether or not they have a father–are more likely to be rejected by their peers; they have more behavioral problems and more academic problems than those who have stayed put.”

5.

All these findings become less perplexing when you accept one of Harris’s central observations; namely, that kids aren’t interested in becoming copies of their parents. Children want to be good at being children. How, for example, do you persuade a preschooler to eat something new? Not by eating it yourself and hoping that your child follows suit. A preschooler doesn’t care what you think. But give the food to a roomful of preschoolers who like it, and it’s quite probable that your child will happily follow suit. From the very moment that children first meet other children, they take their cues from them.

One of the researchers whom Harris draws on in her peer discussion is William A. Corsaro, a professor of sociology at Indiana University and a pioneer in the ethnography of early childhood. He was one of the first researchers to spend months crouching by swing sets and next to monkey bars closely observing the speech and play patterns of preschoolers. In one of his many playground stakeouts, Corsaro was sitting next to a sandbox and watching two four-year-old girls, Jenny and Betty, play house, and put sand in pots, cupcake pans, and teapots. Suddenly, a third girl, Debbie, approached. Here is Corsaro’s full description of the scene:

After watching for about five minutes [Debbie] circles the sandbox three times and stops again and stands next to me. After a few more minutes of watching, Debbie moves to the sandbox and reaches for a teapot. Jenny takes the pot away from Debbie and mumbles, “No.” Debbie backs away and again stands near me, observing the activity of Jenny and Betty. Then she walks over next to Betty, who is filling the cupcake pan with sand.

Debbie watches Betty for just a few seconds, then says,”We’re friends, right, Betty?”

Betty, not looking up at Debbie, continues to place sand in the pan and says, “Right.”

Debbie now moves alongside Betty, takes a pot and spoon, begins putting sand in the pot, and says, “I’m making coffee.”

“I’m making cupcakes,” Betty replies.

Betty now turns to Jenny and says, “We’re mothers, right, Jenny?”

“Right,” says Jenny.

The three “mothers” continue to play together for about twenty more minutes, until the teachers announce cleanup time.

To adults, this exchange looks somewhat troubling. If you saw Debbie circling the sandbox over and over, you’d think she was shy and timid. And if you came upon the three girls just as Jenny told Debbie no you’d think Jenny was selfish and needed to be taught to share. In both cases, the children seem profoundly antisocial. In fact, Corsaro says, the opposite is true. A preschool playground is rather like a cocktail party. There are lots of informal clusters of kids playing together, and the kids are in constant movement, from cluster to cluster. Unlike at a cocktail party, though, the play clusters are very fragile. “If the phone rang right now,” Corsaro said to me when I met him, in his office in Bloomington, “I could answer it, talk for five minutes, and then we could pick up where we left off. It’s easy for us. When you are a three- or four-year-old and you’ve generated something spontaneous and it’s going well, it’s not so easy.” The bell can ring. An adult can step in. An older child can disrupt things. As a result, they spend a lot of effort trying to protect their play from disruption. Betty and Jenny aren’t resistant to sharing when they initially say no to Debbie. They are already sharing, and the point of keeping Debbie at bay is to defend that shared play.

What has evolved in preschool culture, then, is what Corsaro calls access strategies–an elaborate set of rules and rituals that govern when and how the third parties circulating through the playground are allowed to join an existing game. Debbie’s approach to the sandbox is what Corsaro calls nonverbal entry–the first common opening move in the access dance. She’s waiting for an invitation to join. It’s the same at an adult cocktail party. You don’t come up to an existing conversation and say, “May I join in?” You join the group quietly, as if to demonstrate respect for the existing conversation. When Debbie goes around and around the sandbox, she’s trying to understand the basis of Jenny and Betty’s play. Corsaro calls this encirclement. Notice that when Debbie initially reaches for a teapot Jenny says no. Debbie hasn’t proved that she understands the game in question. So she retreats and observes further. Then she makes what Corsaro calls a verbal reference to affiliation–”We’re friends, right?” It’s as if she were offering her bona fides. She gets a positive response. Now she enters again, this time making it absolutely clear that she understands the game: “I’m making coffee.” She’s in. This is how children learn to get along. Kids teach each other how to be social. Indeed, to the extent that adults might get involved in an access situation–by, for example, instructing Jenny and Betty that they have to share with Debbie–they would frustrate the learning process.

Corsaro is a quiet, bearded man of fifty, with the patient, stubborn air of someone who has spent the better part of his life sitting and watching screaming three-year-olds. Harris E-mailed him when she was writing her Psycholo gical Review paper, and the two have struck up an on-line friendship. Most people, Corsaro says, want to figure out what his work says about individual development. Harris, though, recognized at once what Corsaro considers the real lesson, which is the children’s immediate and powerful attraction to their own peer group. Once, Corsaro spent close to a year in a preschool where the children had been forbidden to bring their toys into the classroom. Before long, he noticed that they had found a way around the rule: the children were selecting the smallest of their toys–the boys chose Matchbox toy cars, for example, and the girls little plastic animals–and hiding them in their pockets. These were only preschoolers, but already they were organizing against the adult world, defining themselves as a group in opposition to their elders. “What I found interesting was not that the kids wanted to bring their own toys but that when they smuggled them in they never played with them alone. They played with them collectively,” Corsaro told me. “They wanted others to know that they had them. They wanted to share the toys with others. They are not only sharing the toy but sharing the fact that they are getting around the rule. This is what is unique. I think there is a real, strong emotional satisfaction in sharing things, in doing things together.” Even for a child of three or four, the group is critical.

6.

Judith Harris and her husband, Charles, have two children. The first, Nomi, is their biological daughter, and the second, Elaine, is adopted. In that sense, Harris’s own family is a kind of micro-version of the adoption studies that raise the question of parental influence, and she says that without the example of her daughters she might not have reached the conclusion she did. Nomi, the elder, was quiet and self-sufficient as a child, a National Merit Scholar who went on to do graduate work at MIT. “She is very much like me and Charlie,” Harris says. “She gave us no trouble while she was growing up. She didn’t require much guidance, because she didn’t want to do anything that we didn’t want her to do. Even before she could walk, she would crawl off to another part of the house, and I’d find her taking things out of a drawer and looking at them carefully–and putting them down carefully.”

Elaine was different. “When she was little, all you had to do was look down and she was there, right on my heels,” Harris recalls. “She always wanted to be with people. We started getting bad reports from the school right away–that she wouldn’t sit in her chair, and she was bothering other kids. When Nomi would ask a question, it was because she was interested in the answer. When Elaine would ask a question, it was because she was interested in having the interaction. Nomi would ask a question once. Elaine would often ask a question several times. As the girls got older, Nomi became a brain and Elaine became a dropout. Nomi was a member of a very small clique of intellectual kids, and Elaine was a member of the delinquent subgroup. They went in opposite directions.”

Harris has an optimistic air about her, as if all her troubles had only served to strengthen her appreciation of life. But it’s clear that bringing up Elaine represented a real crisis in her life. When Elaine was six and Nomi was ten, Harris became ill for the first time. She was in such pain that she couldn’t sit up for more than half an hour. She tried taking a graduate course in psychology, hoping to finish a doctorate she had started, in the early sixties, at Harvard, and she had a fellow-student carry a cot to class so she could lie down during lectures. But even that was too hard, so she became a textbook writer, lying in her bed, with a spiral-bound notebook on her knee, and Nomi acting as her typist. She had pneumonia, a heart murmur, pulmonary hypertension, shingles, a year of chronic hives, and a minor stroke. “Sometimes,” she says, “I felt like Job,” and in the midst of all her troubles her younger daughter seemed out of control.

“We had very bad years with her in her teens,” she recalls. “We didn’t know how to handle her.” Harris says that she began motherhood as a classic environmentalist, meaning she believed that children would reflect the environment in which they were reared. Had she stopped with Nomi, she says, she might have attributed Nomi’s studiousness and self-sufficiency and success to her own enlightened parenting. It was Elaine who made the puzzle posed by the adoption studies seem real. “I assumed that an adopted child would represent her environment, and that if I could give Elaine the same kind of environment I gave to my first child she would turn out–of course, not the same…” She thought for moment. “But I certainly didn’t expect that she would be so vastly different. I couldn’t see that I was having any effect on her at all.” Harris seems a little reluctant to talk about those years, particularly since Elaine turned out, as she puts it, “amazingly well” and is now happy and married, with a toddler and a career as a licensed practical nurse. But it’s not hard to imagine the kind of guilt and frustration she must have felt–maternal helplessness magnified by her physical debility–as she and Charles did everything that good parents are supposed to do yet still came up short. Her epiphany was, in a way, her release, because she came to believe that the reason she and Charles couldn’t see that they were having any effect on Elaine was that parents really can’t have a big effect on their children.

There are a hundred ways of explaining Nomi and Elaine, and there is, of course, something very convenient about the explanation that Harris arrived at: it’s the kind of thing that the mother of a difficult child wants to believe. Harris has constructed a theory that lets herself off the hook for her daughter’s troubled childhood. It should be said, though, that the idea that parents can control the destiny of their children by doing all the right things–by providing children with every lesson and every experience, by buying them the right toys and saying the right words and never spanking or publicly scolding them–is just as self-serving. At least, Harris’s theory calls for neighborhoods, peers, and children themselves to share the blame–and the credit–for how children turn out. The nurture assumption, by contrast, places the blame and the credit squarely on the parent, and has made it possible to demonize all those who fail to measure up to the strictest standards of supposedly optimal parenting. “I want to tell parents that it’s all right,” Harris told me. “A lot of people who should be contributing children to our society, who could be contributing very useful and fine children, are reluctant to do it, or are waiting very long to have children, because they feel that it requires such a huge commitment. If they knew that it was O.K. to have a child and let it be reared by a nanny or put it in a day-care center, or even to send it to a boarding school, maybe they’d believe that it would be O.K. to have a kid. You can have a kid without having to devote your entire life–your entire emotional expenditure–to this child for the next twenty years.”

Harris does not see children as delicate vessels and does not believe they are easily damaged by the missteps of their mothers and fathers. We have been told, Harris writes, to tell children not that they’ve been bad but that what they did was bad, or, even more appropriately, that what they did made us feel bad. In her view, we have come to insist on these niceties only because we have forgotten what the world of children is really like. “Kids are not that fragile,” she writes. “They are tougher than you think. They have to be, because the world out there does not handle them with kid gloves. At home, they might hear ‘What you did made me feel bad,’ but out on the playground it’s ‘You shithead!’”

Is Harris right? She is the first to admit that what she has provided is only, at this stage, a theory. From her tiny study, off the main hallway of her home in New Jersey, she is scarcely in a position to do the kind of multimillion-dollar, multi-year study that is needed to test her hypothesis. “My guess is that some of the more threatened elders in the field of psychology are going to go out of their way to try and savage this,” Robert Sapolsky, a neurobiologist at Stanford, says. “But my gut feeling is that this is really important. Harris makes a lot of sense. Sometimes she is a little doctrinaire”–he paused–”but, boy.” Already, Harris has helped wrench psychology away from its single-minded obsession with chronicling and interpreting the tiniest perturbations of family life. The nurture assumption, she says, has turned childhood into parenthood: it has turned the development of children into a story almost entirely about their parents. “Have you ever thought of yourself as a mirror?” Dorothy Corkille Briggs asks in her pop-psychology handbook “Your Child’s Self-Esteem.” “You are one–a psychological mirror your child uses to build his identity. And his whole life is affected by the conclusions he draws.” And here are Barbara Chernofsky and Diane Gage, in “Change Your Child’s Behavior by Changing Yours,” on how children relate to their parents: “Like living video cameras, children record what they observe.” This is the modern-day cult of parenting. It takes as self-evident the idea that the child is oriented, overwhelmingly, toward the parents. But why should that be true? Don’t parents, in fact, spend much of their time instructing their children not to act like adults–that they cannot be independent, that they cannot make decisions entirely by themselves, that different rules apply to them because they are children?

“If developmental psychology were an enterprise conducted by children, there is no question that peer relationships would be at the top of the list,” Peter Gray, a psychologist at Boston College, told me. “But because it is conducted by adults we tend, egocentrically, to believe that it is the relationship between us and our children that is important. But just look at them. Whom do they want to please? Are they wearing the kind of clothing that other kids are wearing or the kind that their parents are wearing? If the other kids are speaking another way, whose language are they going to learn? And, from an evolutionary perspective, whom should they be paying attention to? Their parents–the members of the previous generation–or their peers, who will be their future mates and future collaborators? It would more adaptive for them to be better attuned to the nuances of their peers’ behavior. That just makes a lot of sense.”

7.

Harris’s health is more stable now, and when she was putting the finishing touches on her book this summer she was sometimes able to work at the computer twelve, or even fourteen, hours a day. But anything more strenuous is out of the question. The woman who says that what really matters is what happens outside the home rarely leaves the home–not for vacations, or even to see a movie. Indeed, none of the heavyweight psychologists who have befriended her since her Psychological Review article ran have ever met her. “Writing E-mail is my recreation,” she wrote me in an E-mail.

When Harris goes to San Francisco this week, for the A.P.A. convention, it will be a kind of coming-out party. In preparation, during the past few weeks she has had to go shopping. “I have to buy clothes,” she said. “I’ve hardly been out of the house in years.” On August 15th, she will take the stage and receive a prize named in honor of the eminent scholar George A. Miller. Almost four decades ago, Harris was kicked out of graduate school after only two years, and the dean who delivered the news was the same George A. Miller. The two have since corresponded, and Miller has termed the irony “delicious.” In her acceptance remarks, Harris told me, she intends to read from the letter that Miller wrote her long ago: “I hesitate to say that you lack originality and independence, because in many areas of life you obviously possess both of those traits in abundance. But for some reason you have not been able to bring them to bear on the kind of problems in psychology to which this department is dedicated….We are in considerable doubt that you will develop into our professional stereotype of what an experimental psychologist should be.”

© 2013 Malcolm Gladwell.

DAVID AND GOLIATH
 
WHAT THE DOG SAW
 
OUTLIERS
 
BLINK
 
THE TIPPING POINT
 
ARTICLES
 
ETC.
Getting In

Posted October 10, 2005 by MALCOLM GLADWELL & filed under A CRITIC AT LARGE, THE NEW YORKER - ARCHIVE.

The social logic of Ivy League admissions.

1.

I applied to college one evening, after dinner, in the fall of my senior year in high school. College applicants in Ontario, in those days, were given a single sheet of paper which listed all the universities in the province. It was my job to rank them in order of preference. Then I had to mail the sheet of paper to a central college-admissions office. The whole process probably took ten minutes. My school sent in my grades separately. I vaguely remember filling out a supplementary two-page form listing my interests and activities. There were no S.A.T. scores to worry about, because in Canada we didn’t have to take the S.A.T.s. I don’t know whether anyone wrote me a recommendation. I certainly never asked anyone to. Why would I? It wasn’t as if I were applying to a private club.

I put the University of Toronto first on my list, the University of Western Ontario second, and Queen’s University third. I was working off a set of brochures that I’d sent away for. My parents’ contribution consisted of my father’s agreeing to drive me one afternoon to the University of Toronto campus, where we visited the residential college I was most interested in. I walked around. My father poked his head into the admissions office, chatted with the admissions director, and—I imagine—either said a few short words about the talents of his son or (knowing my father) remarked on the loveliness of the delphiniums in the college flower beds. Then we had ice cream. I got in.

Am I a better or more successful person for having been accepted at the University of Toronto, as opposed to my second or third choice? It strikes me as a curious question. In Ontario, there wasn’t a strict hierarchy of colleges. There were several good ones and several better ones and a number of programs—like computer science at the University of Waterloo—that were world-class. But since all colleges were part of the same public system and tuition everywhere was the same (about a thousand dollars a year, in those days), and a B average in high school pretty much guaranteed you a spot in college, there wasn’t a sense that anything great was at stake in the choice of which college we attended. The issue was whether we attended college, and—most important—how seriously we took the experience once we got there. I thought everyone felt this way. You can imagine my confusion, then, when I first met someone who had gone to Harvard.

There was, first of all, that strange initial reluctance to talk about the matter of college at all—a glance downward, a shuffling of the feet, a mumbled mention of Cambridge. “Did you go to Harvard?” I would ask. I had just moved to the United States. I didn’t know the rules. An uncomfortable nod would follow. Don’t define me by my school, they seemed to be saying, which implied that their school actually could define them. And, of course, it did. Wherever there was one Harvard graduate, another lurked not far behind, ready to swap tales of late nights at the Hasty Pudding, or recount the intricacies of the college—application essay, or wonder out loud about the whereabouts of Prince So-and-So, who lived down the hall and whose family had a place in the South of France that you would not believe. In the novels they were writing, the precocious and sensitive protagonist always went to Harvard; if he was troubled, he dropped out of Harvard; in the end, he returned to Harvard to complete his senior thesis. Once, I attended a wedding of a Harvard alum in his fifties, at which the best man spoke of his college days with the groom as if neither could have accomplished anything of greater importance in the intervening thirty years. By the end, I half expected him to take off his shirt and proudly display the large crimson “H” tattooed on his chest. What is this “Harvard” of which you Americans speak so reverently?

2.

In 1905, Harvard College adopted the College Entrance Examination Board tests as the principal basis for admission, which meant that virtually any academically gifted high—school senior who could afford a private college had a straightforward shot at attending. By 1908, the freshman class was seven per cent Jewish, nine per cent Catholic, and forty-five per cent from public schools, an astonishing transformation for a school that historically had been the preserve of the New England boarding-school complex known in the admissions world as St. Grottlesex.

As the sociologist Jerome Karabel writes in “The Chosen” (Houghton Mifflin; $28), his remarkable history of the admissions process at Harvard, Yale, and Princeton, that meritocratic spirit soon led to a crisis. The enrollment of Jews began to rise dramatically. By 1922, they made up more than a fifth of Harvard’s freshman class. The administration and alumni were up in arms. Jews were thought to be sickly and grasping, grade-grubbing and insular. They displaced the sons of wealthy Wasp alumni, which did not bode well for fund-raising. A. Lawrence Lowell, Harvard’s president in the nineteen-twenties, stated flatly that too many Jews would destroy the school: “The summer hotel that is ruined by admitting Jews meets its fate . . . because they drive away the Gentiles, and then after the Gentiles have left, they leave also.”

The difficult part, however, was coming up with a way of keeping Jews out, because as a group they were academically superior to everyone else. Lowell’s first idea—a quota limiting Jews to fifteen per cent of the student body—was roundly criticized. Lowell tried restricting the number of scholarships given to Jewish students, and made an effort to bring in students from public schools in the West, where there were fewer Jews. Neither strategy worked. Finally, Lowell—and his counterparts at Yale and Princeton—realized that if a definition of merit based on academic prowess was leading to the wrong kind of student, the solution was to change the definition of merit. Karabel argues that it was at this moment that the history and nature of the Ivy League took a significant turn.

The admissions office at Harvard became much more interested in the details of an applicant’s personal life. Lowell told his admissions officers to elicit information about the “character” of candidates from “persons who know the applicants well,” and so the letter of reference became mandatory. Harvard started asking applicants to provide a photograph. Candidates had to write personal essays, demonstrating their aptitude for leadership, and list their extracurricular activities. “Starting in the fall of 1922,” Karabel writes, “applicants were required to answer questions on “Race and Color,’ “Religious Preference,’ “Maiden Name of Mother,’ “Birthplace of Father,’ and “What change, if any, has been made since birth in your own name or that of your father? (Explain fully).’ ”

At Princeton, emissaries were sent to the major boarding schools, with instructions to rate potential candidates on a scale of 1 to 4, where 1 was “very desirable and apparently exceptional material from every point of view” and 4 was “undesirable from the point of view of character, and, therefore, to be excluded no matter what the results of the entrance examinations might be.” The personal interview became a key component of admissions in order, Karabel writes, “to ensure that “undesirables’ were identified and to assess important but subtle indicators of background and breeding such as speech, dress, deportment and physical appearance.” By 1933, the end of Lowell’s term, the percentage of Jews at Harvard was back down to fifteen per cent.

If this new admissions system seems familiar, that’s because it is essentially the same system that the Ivy League uses to this day. According to Karabel, Harvard, Yale, and Princeton didn’t abandon the elevation of character once the Jewish crisis passed. They institutionalized it.

Starting in 1953, Arthur Howe, Jr., spent a decade as the chair of admissions at Yale, and Karabel describes what happened under his guidance:

The admissions committee viewed evidence of “manliness” with particular enthusiasm. One boy gained admission despite an academic prediction of 70 because “there was apparently something manly and distinctive about him that had won over both his alumni and staff interviewers.” Another candidate, admitted despite his schoolwork being “mediocre in comparison with many others,” was accepted over an applicant with a much better record and higher exam scores because, as Howe put it, “we just thought he was more of a guy.” So preoccupied was Yale with the appearance of its students that the form used by alumni interviewers actually had a physical characteristics checklist through 1965. Each year, Yale carefully measured the height of entering freshmen, noting with pride the proportion of the class at six feet or more.

At Harvard, the key figure in that same period was Wilbur Bender, who, as the dean of admissions, had a preference for “the boy with some athletic interests and abilities, the boy with physical vigor and coordination and grace.” Bender, Karabel tells us, believed that if Harvard continued to suffer on the football field it would contribute to the school’s reputation as a place with “no college spirit, few good fellows, and no vigorous, healthy social life,” not to mention a “surfeit of “pansies,’ “decadent esthetes’ and “precious sophisticates.’ ” Bender concentrated on improving Harvard’s techniques for evaluating “intangibles” and, in particular, its “ability to detect homosexual tendencies and serious psychiatric problems.”

By the nineteen-sixties, Harvard’s admissions system had evolved into a series of complex algorithms. The school began by lumping all applicants into one of twenty-two dockets, according to their geographical origin. (There was one docket for Exeter and Andover, another for the eight Rocky Mountain states.) Information from interviews, references, and student essays was then used to grade each applicant on a scale of 1 to 6, along four dimensions: personal, academic, extracurricular, and athletic. Competition, critically, was within each docket, not between dockets, so there was no way for, say, the graduates of Bronx Science and Stuyvesant to shut out the graduates of Andover and Exeter. More important, academic achievement was just one of four dimensions, further diluting the value of pure intellectual accomplishment. Athletic ability, rather than falling under “extracurriculars,” got a category all to itself, which explains why, even now, recruited athletes have an acceptance rate to the Ivies at well over twice the rate of other students, despite S.A.T. scores that are on average more than a hundred points lower. And the most important category? That mysterious index of “personal” qualities. According to Harvard’s own analysis, the personal rating was a better predictor of admission than the academic rating. Those with a rank of 4 or worse on the personal scale had, in the nineteen-sixties, a rejection rate of ninety-eight per cent. Those with a personal rating of 1 had a rejection rate of 2.5 per cent. When the Office of Civil Rights at the federal education department investigated Harvard in the nineteen-eighties, they found handwritten notes scribbled in the margins of various candidates’ files. “This young woman could be one of the brightest applicants in the pool but there are several references to shyness,” read one. Another comment reads, “Seems a tad frothy.” One application—and at this point you can almost hear it going to the bottom of the pile—was notated, “Short with big ears.”

3.

Social scientists distinguish between what are known as treatment effects and selection effects. The Marine Corps, for instance, is largely a treatment-effect institution. It doesn’t have an enormous admissions office grading applicants along four separate dimensions of toughness and intelligence. It’s confident that the experience of undergoing Marine Corps basic training will turn you into a formidable soldier. A modelling agency, by contrast, is a selection-effect institution. You don’t become beautiful by signing up with an agency. You get signed up by an agency because you’re beautiful.

At the heart of the American obsession with the Ivy League is the belief that schools like Harvard provide the social and intellectual equivalent of Marine Corps basic training—that being taught by all those brilliant professors and meeting all those other motivated students and getting a degree with that powerful name on it will confer advantages that no local state university can provide. Fuelling the treatment-effect idea are studies showing that if you take two students with the same S.A.T. scores and grades, one of whom goes to a school like Harvard and one of whom goes to a less selective college, the Ivy Leaguer will make far more money ten or twenty years down the road.

The extraordinary emphasis the Ivy League places on admissions policies, though, makes it seem more like a modeling agency than like the Marine Corps, and, sure enough, the studies based on those two apparently equivalent students turn out to be flawed. How do we know that two students who have the same S.A.T. scores and grades really are equivalent? It’s quite possible that the student who goes to Harvard is more ambitious and energetic and personable than the student who wasn’t let in, and that those same intangibles are what account for his better career success. To assess the effect of the Ivies, it makes more sense to compare the student who got into a top school with the student who got into that same school but chose to go to a less selective one. Three years ago, the economists Alan Krueger and Stacy Dale published just such a study. And they found that when you compare apples and apples the income bonus from selective schools disappears.

“As a hypothetical example, take the University of Pennsylvania and Penn State, which are two schools a lot of students choose between,” Krueger said. “One is Ivy, one is a state school. Penn is much more highly selective. If you compare the students who go to those two schools, the ones who go to Penn have higher incomes. But let’s look at those who got into both types of schools, some of whom chose Penn and some of whom chose Penn State. Within that set it doesn’t seem to matter whether you go to the more selective school. Now, you would think that the more ambitious student is the one who would choose to go to Penn, and the ones choosing to go to Penn State might be a little less confident in their abilities or have a little lower family income, and both of those factors would point to people doing worse later on. But they don’t.”

Krueger says that there is one exception to this. Students from the very lowest economic strata do seem to benefit from going to an Ivy. For most students, though, the general rule seems to be that if you are a hardworking and intelligent person you’ll end up doing well regardless of where you went to school. You’ll make good contacts at Penn. But Penn State is big enough and diverse enough that you can make good contacts there, too. Having Penn on your résumé opens doors. But if you were good enough to get into Penn you’re good enough that those doors will open for you anyway. “I can see why families are really concerned about this,” Krueger went on. “The average graduate from a top school is making nearly a hundred and twenty thousand dollars a year, the average graduate from a moderately selective school is making ninety thousand dollars. That’s an enormous difference, and I can see why parents would fight to get their kids into the better school. But I think they are just assigning to the school a lot of what the student is bringing with him to the school.”

Bender was succeeded as the dean of admissions at Harvard by Fred Glimp, who, Karabel tells us, had a particular concern with academic underperformers. “Any class, no matter how able, will always have a bottom quarter,” Glimp once wrote. “What are the effects of the psychology of feeling average, even in a very able group? Are there identifiable types with the psychological or what—not tolerance to be “happy’ or to make the most of education while in the bottom quarter?” Glimp thought it was critical that the students who populated the lower rungs of every Harvard class weren’t so driven and ambitious that they would be disturbed by their status. “Thus the renowned (some would say notorious) Harvard admission practice known as the “happy-bottom-quarter’ policy was born,” Karabel writes.

It’s unclear whether or not Glimp found any students who fit that particular description. (He wondered, in a marvellously honest moment, whether the answer was “Harvard sons.”) But Glimp had the realism of the modelling scout. Glimp believed implicitly what Krueger and Dale later confirmed: that the character and performance of an academic class is determined, to a significant extent, at the point of admission; that if you want to graduate winners you have to admit winners; that if you want the bottom quarter of your class to succeed you have to find people capable of succeeding in the bottom quarter. Karabel is quite right, then, to see the events of the nineteen-twenties as the defining moment of the modern Ivy League. You are whom you admit in the élite-education business, and when Harvard changed whom it admitted, it changed Harvard. Was that change for the better or for the worse?

4.

In the wake of the Jewish crisis, Harvard, Yale, and Princeton chose to adopt what might be called the “best graduates” approach to admissions. France’s École Normale Supérieure, Japan’s University of Tokyo, and most of the world’s other élite schools define their task as looking for the best students—that is, the applicants who will have the greatest academic success during their time in college. The Ivy League schools justified their emphasis on character and personality, however, by arguing that they were searching for the students who would have the greatest success after college. They were looking for leaders, and leadership, the officials of the Ivy League believed, was not a simple matter of academic brilliance. “Should our goal be to select a student body with the highest possible proportions of high-ranking students, or should it be to select, within a reasonably high range of academic ability, a student body with a certain variety of talents, qualities, attitudes, and backgrounds?” Wilbur Bender asked. To him, the answer was obvious. If you let in only the brilliant, then you produced bookworms and bench scientists: you ended up as socially irrelevant as the University of Chicago (an institution Harvard officials looked upon and shuddered). “Above a reasonably good level of mental ability, above that indicated by a 550-600 level of S.A.T. score,” Bender went on, “the only thing that matters in terms of future impact on, or contribution to, society is the degree of personal inner force an individual has.”

It’s easy to find fault with the best-graduates approach. We tend to think that intellectual achievement is the fairest and highest standard of merit. The Ivy League process, quite apart from its dubious origins, seems subjective and opaque. Why should personality and athletic ability matter so much? The notion that “the ability to throw, kick, or hit a ball is a legitimate criterion in determining who should be admitted to our greatest research universities,” Karabel writes, is “a proposition that would be considered laughable in most of the world’s countries.” At the same time that Harvard was constructing its byzantine admissions system, Hunter College Elementary School, in New York, required simply that applicants take an exam, and if they scored in the top fifty they got in. It’s hard to imagine a more objective and transparent procedure.

But what did Hunter achieve with that best-students model? In the nineteen-eighties, a handful of educational researchers surveyed the students who attended the elementary school between 1948 and 1960. This was a group with an average I.Q. of 157—three and a half standard deviations above the mean—who had been given what, by any measure, was one of the finest classroom experiences in the world. As graduates, though, they weren’t nearly as distinguished as they were expected to be. “Although most of our study participants are successful and fairly content with their lives and accomplishments,” the authors conclude, “there are no superstars . . . and only one or two familiar names.” The researchers spend a great deal of time trying to figure out why Hunter graduates are so disappointing, and end up sounding very much like Wilbur Bender. Being a smart child isn’t a terribly good predictor of success in later life, they conclude. “Non-intellective” factors—like motivation and social skills—probably matter more. Perhaps, the study suggests, “after noting the sacrifices involved in trying for national or world-class leadership in a field, H.C.E.S. graduates decided that the intelligent thing to do was to choose relatively happy and successful lives.” It is a wonderful thing, of course, for a school to turn out lots of relatively happy and successful graduates. But Harvard didn’t want lots of relatively happy and successful graduates. It wanted superstars, and Bender and his colleagues recognized that if this is your goal a best-students model isn’t enough.

Most élite law schools, to cite another example, follow a best-students model. That’s why they rely so heavily on the L.S.A.T. Yet there’s no reason to believe that a person’s L.S.A.T. scores have much relation to how good a lawyer he will be. In a recent research project funded by the Law School Admission Council, the Berkeley researchers Sheldon Zedeck and Marjorie Shultz identified twenty-six “competencies” that they think effective lawyering demands—among them practical judgment, passion and engagement, legal-research skills, questioning and interviewing skills, negotiation skills, stress management, and so on—and the L.S.A.T. picks up only a handful of them. A law school that wants to select the best possible lawyers has to use a very different admissions process from a law school that wants to select the best possible law students. And wouldn’t we prefer that at least some law schools try to select good lawyers instead of good law students?

This search for good lawyers, furthermore, is necessarily going to be subjective, because things like passion and engagement can’t be measured as precisely as academic proficiency. Subjectivity in the admissions process is not just an occasion for discrimination; it is also, in better times, the only means available for giving us the social outcome we want. The first black captain of the Yale football team was a man named Levi Jackson, who graduated in 1950. Jackson was a hugely popular figure on campus. He went on to be a top executive at Ford, and is credited with persuading the company to hire thousands of African-Americans after the 1967 riots. When Jackson was tapped for the exclusive secret society Skull and Bones, he joked, “If my name had been reversed, I never would have made it.” He had a point. The strategy of discretion that Yale had once used to exclude Jews was soon being used to include people like Levi Jackson.

In the 2001 book “The Game of Life,” James L. Shulman and William Bowen (a former president of Princeton) conducted an enormous statistical analysis on an issue that has become one of the most contentious in admissions: the special preferences given to recruited athletes at selective universities. Athletes, Shulman and Bowen demonstrate, have a large and growing advantage in admission over everyone else. At the same time, they have markedly lower G.P.A.s and S.A.T. scores than their peers. Over the past twenty years, their class rankings have steadily dropped, and they tend to segregate themselves in an “athletic culture” different from the culture of the rest of the college. Shulman and Bowen think the preference given to athletes by the Ivy League is shameful.

Halfway through the book, however, Shulman and Bowen present “” finding. Male athletes, despite their lower S.A.T. scores and grades, and despite the fact that many of them are members of minorities and come from lower socioeconomic backgrounds than other students, turn out to earn a lot more than their peers. Apparently, athletes are far more likely to go into the high-paying financial-services sector, where they succeed because of their personality and psychological makeup. In what can only be described as a textbook example of burying the lead, Bowen and Shulman write:

One of these characteristics can be thought of as drive—a strong desire to succeed and unswerving determination to reach a goal, whether it be winning the next game or closing a sale. Similarly, athletes tend to be more energetic than the average person, which translates into an ability to work hard over long periods of time—to meet, for example, the workload demands placed on young people by an investment bank in the throes of analyzing a transaction. In addition, athletes are more likely than others to be highly competitive, gregarious and confident of their ability to work well in groups (on teams).

Shulman and Bowen would like to argue that the attitudes of selective colleges toward athletes are a perversion of the ideals of American élite education, but that’s because they misrepresent the actual ideals of American élite education. The Ivy League is perfectly happy to accept, among others, the kind of student who makes a lot of money after graduation. As the old saying goes, the definition of a well-rounded Yale graduate is someone who can roll all the way from New Haven to Wall Street.

5.

I once had a conversation with someone who worked for an advertising agency that represented one of the big luxury automobile brands. He said that he was worried that his client’s new lower-priced line was being bought disproportionately by black women. He insisted that he did not mean this in a racist way. It was just a fact, he said. Black women would destroy the brand’s cachet. It was his job to protect his client from the attentions of the socially undesirable.

This is, in no small part, what Ivy League admissions directors do. They are in the luxury-brand-management business, and “The Chosen,” in the end, is a testament to just how well the brand managers in Cambridge, New Haven, and Princeton have done their job in the past seventy-five years. In the nineteen twenties, when Harvard tried to figure out how many Jews they had on campus, the admissions office scoured student records and assigned each suspected Jew the designation j1 (for someone who was “conclusively Jewish”), j2 (where the “preponderance of evidence” pointed to Jewishness), or j3 (where Jewishness was a “possibility”). In the branding world, this is called customer segmentation. In the Second World War, as Yale faced plummeting enrollment and revenues, it continued to turn down qualified Jewish applicants. As Karabel writes, “In the language of sociology, Yale judged its symbolic capital to be even more precious than its economic capital.” No good brand manager would sacrifice reputation for short-term gain. The admissions directors at Harvard have always, similarly, been diligent about rewarding the children of graduates, or, as they are quaintly called, “legacies.” In the 1985-92 period, for instance, Harvard admitted children of alumni at a rate more than twice that of non-athlete, non-legacy applicants, despite the fact that, on virtually every one of the school’s magical ratings scales, legacies significantly lagged behind their peers. Karabel calls the practice “unmeritocratic at best and profoundly corrupt at worst,” but rewarding customer loyalty is what luxury brands do. Harvard wants good graduates, and part of their definition of a good graduate is someone who is a generous and loyal alumnus. And if you want generous and loyal alumni you have to reward them. Aren’t the tremendous resources provided to Harvard by its alumni part of the reason so many people want to go to Harvard in the first place? The endless battle over admissions in the United States proceeds on the assumption that some great moral principle is at stake in the matter of whom schools like Harvard choose to let in—that those who are denied admission by the whims of the admissions office have somehow been harmed. If you are sick and a hospital shuts its doors to you, you are harmed. But a selective school is not a hospital, and those it turns away are not sick. Élite schools, like any luxury brand, are an aesthetic experience—an exquisitely constructed fantasy of what it means to belong to an élite —and they have always been mindful of what must be done to maintain that experience.

In the nineteen-eighties, when Harvard was accused of enforcing a secret quota on Asian admissions, its defense was that once you adjusted for the preferences given to the children of alumni and for the preferences given to athletes, Asians really weren’t being discriminated against. But you could sense Harvard’s exasperation that the issue was being raised at all. If Harvard had too many Asians, it wouldn’t be Harvard, just as Harvard wouldn’t be Harvard with too many Jews or pansies or parlor pinks or shy types or short people with big ears.

© 2013 Malcolm Gladwell.

DAVID AND GOLIATH
 
WHAT THE DOG SAW
 
OUTLIERS
 
BLINK
 
THE TIPPING POINT
 
ARTICLES
 
ETC.
The Science of the Sleeper

Posted October 4, 1999 by MALCOLM GLADWELL & filed under ANNALS OF MARKETING, THE NEW YORKER - ARCHIVE.

How the Information Age could blow away the blockbuster.

1.

In 1992, a sometime actress named Rebecca Wells published a novel called “Little Altars Everywhere” with a small, now defunct press in Seattle. Wells was an unknown author, and the press had no money for publicity. She had a friend, however, who spent that Thanksgiving with a friend who was a producer of National Public Radio’s “All Things Considered.” The producer read the book and passed it on to Linda Wertheimer, a host of the show, and she liked it so much that she put Wells on her program. That interview, in turn, was heard by a man who was listening to the radio in Blytheville, Arkansas, and whose wife, Mary Gay Shipley, ran the town bookstore. He bought the book and gave it to her; she loved it, and, with that, the strange and improbable rise of Rebecca Wells, best-selling author, began. Blytheville is a sleepy little town about an hour or so up the Mississippi from Memphis, and Mary Gay Shipley’s bookstore–That Bookstore in Blytheville–sits between the Red Ball Barber Shop and Westbrook’s shoe store on a meandering stretch of Main Street. The store is just one long room in a slightly shabby storefront, with creaky floors and big overhead fans and subject headings on the shelves marked with Post-it notes. Shipley’s fiction section takes up about as much shelf space as a typical Barnes & Noble devotes to, say, homeopathic medicine. That’s because Shipley thinks that a book buyer ought to be able to browse and read the jacket flap of everything that might catch her eye, without being overwhelmed by thousands of choices. Mostly, though, people come to Mary Gay Shipley’s store in order to find out what Mary Gay thinks they ought to be reading, and in 1993 Mary Gay Shipley thought people ought to be reading “Little Altars Everywhere.” She began ordering it by the dozen, which, Shipley says, “for us, is huge.” She put it in the little rack out front where she lists her current favorites. She wrote about it in the newsletter she sends to her regular customers. “We could tell it was going to have a lot of word of mouth,” she says. “It was the kind of book where you could say, ‘You’ll love it. Take it home.’ ” The No. 1 author at That Bookstore in Blytheville in 1993 was John Grisham, as was the case in nearly every bookstore in the country. But No. 2 was Rebecca Wells.

“Little Altars Everywhere” was not a best-seller. But there were pockets of devotees around the country–in Blytheville; at the Garden District Book Shop, in New Orleans; at Parkplace books, in Kirkland, Washington–and those pockets created a buzz that eventually reached Diane Reverand, an editor in New York. Reverand published Wells’s next book, “Divine Secrets of the Ya-Ya Sisterhood,” and when it hit the bookshelves the readers and booksellers of Blytheville, the Garden District, and Kirkland were ready. “When ‘The Ya-Ya Sisterhood’ came out, I met with an in-store sales rep from HarperCollins,” Shipley said. She is a tall woman with graying hair and a quiet, dignified bearing. “I’m not real sure he knew what a hot book this was. When he came in the store, I just turned the page of the catalogue and said, ‘I want one hundred copies,’ and his jaw fell to the table, because I usually order four or two or one. And I said, ‘I want her to come here! And if you go anywhere, tell people this woman sells in Blytheville!’”

Wells made the trip to Arkansas and read in the back of Shipley’s store; the house was packed, and the women in the front row wore placards saying “Ya-Ya.” She toured the country, and the crowds grew steadily bigger. “Before the numbers really showed it, I’d be signing books and there would be groups of women who would come together, six or seven, and they would have me sign anywhere between three and ten books,” Wells recalls. “And then, after that, I started noticing mothers and daughters coming. Then I noticed that the crowds started to be three-generational–there would be teen-agers and sixth graders.” “Ya-Ya” sold fifteen thousand copies in hardcover. The paperback sold thirty thousand copies in its first two months. Diane Reverand took out a single- column ad next to the contents page of The New Yorker–the first dollar she’d spent on advertising for the paperback–and sales doubled to sixty thousand in a month. It sold and sold, and by February of 1998, almost two years after the book was published, it reached the best-seller lists. There are now nearly three million copies in print. Rebecca Wells, needless to say, has a warm spot in her heart for people like Mary Gay Shipley. “Mary Gay is a legend,” she says. “She just kept putting my books in people’s hands.”

2.

In the book business, as in the movie business, there are two kinds of hits: sleepers and blockbusters. John Grisham and Tom Clancy and Danielle Steel write blockbusters. Their books are announced with huge publicity campaigns. Within days of publication, they leap onto the best-seller lists. Sales start high–hundreds of thousands of copies in the first few weeks–and then taper off. People who buy or watch blockbusters have a clear sense of what they are going to get: a Danielle Steel novel is always–well, a Danielle Steel novel. Sleepers, on the other hand, are often unknown quantities. Sales start slowly and gradually build; publicity, at least early on, is often nonexistent. Sleepers come to your attention by a slow, serendipitous path: a friend who runs into a friend who sets up the interview that just happens to be heard by a guy married to a bookseller. Sleepers tend to emerge from the world of independent bookstores, because independent bookstores are the kinds of places where readers go to ask the question that launches all sleeper hits: Can you recommend a book to me? Shipley was plugging Terry Kay’s “To Dance with the White Dog” long before it became a best-seller. She had Melinda Haynes lined up to do a reading at her store before Oprah tapped “Mother of Pearl” as one of her recommended books and it shot onto the best-seller lists. She read David Guterson’s “Snow Falling on Cedars” in manuscript and went crazy for it. “I called the publisher, and they said, ‘We think it’s a regional book.’ And I said, ‘Write it down. “M.G.S. says this is an important book.”‘” All this makes it sound as if she has a sixth sense for books that will be successful, but that’s not quite right. People like Mary Gay Shipley don’t merely predict sleeper hits; they create sleeper hits.

Most of us, of course, don’t have someone like Mary Gay Shipley in our lives, and with the decline of the independent bookstore in recent years the number of Shipleys out there creating sleeper hits has declined as well. The big chain bookstores that have taken over the bookselling business are blockbuster factories, since the sheer number of titles they offer can make browsing an intimidating proposition. As David Gernert, who is John Grisham’s agent and editor, explains, “If you walk into a superstore, that’s where being a brand makes so much more of a difference. There is so much more choice it’s overwhelming. You see walls and walls of books. In that kind of environment, the reader is drawn to the known commodity. The brand-name author is now a safe haven.” Between 1986 and 1996, the share of book sales represented by the thirty top-selling hardcover books in America nearly doubled.

The new dominance of the blockbuster is part of a familiar pattern. The same thing has happened in the movie business, where a handful of heavily promoted films featuring “bankable” stars now command the lion’s share of the annual box-office. We live, as the economists Robert Frank and Philip Cook have argued, in a “winner-take-all society,” which is another way of saying that we live in the age of the blockbuster. But what if there were a way around the blockbuster? What if there were a simple way to build your very own Mary Gay Shipley? This is the promise of a new technology called collaborative filtering, one of the most intriguing developments to come out of the Internet age.

3.

If you want a recommendation about what product to buy, you might want to consult an expert in the field. That’s a function that magazines like Car and Driver and Sound & Vision perform. Another approach is to poll users or consumers of a particular product or service and tabulate their opinions. That’s what the Zagat restaurant guides and consumer-ratings services like J. D. Power and Associates do. It’s very helpful to hear what an “expert” audiophile has to say about the newest DVD player, or what the thousands of owners of the new Volkswagen Passat have to say about reliability and manufacturing defects. But when it comes to books or movies–what might be called “taste products”–these kinds of recommendations aren’t nearly as useful. Few moviegoers, for example, rely on the advice of a single movie reviewer. Most of us gather opinions from a variety of sources–from reviewers whom we have agreed with in the past, from friends who have already seen the movie, or from the presence of certain actors or directors whom we already like–and do a kind of calculation in our heads. It’s an imperfect procedure. You can find out a great deal about what various critics have to say. But they’re strangers, and, to predict correctly whether you’ll like something, the person making the recommendation really has to know something about you.

That’s why Shipley is such a powerful force in touting new books. She has lived in Blytheville all her life and has run the bookstore there for twenty-three years, and so her customers know who she is. They trust her recommendations. At the same time, she knows who they are, so she knows how to match up the right book with the right person. For example, she really likes David Guterson’s new novel, “East of the Mountains,” but she’s not about to recommend it to anyone. It’s about a doctor who has cancer and plans his own death and, she says, “there are some people dealing with a death in their family for whom this is not the book to read right now.” She had similar reservations about Charles Frazier’s “Cold Mountain.” “There were people I know who I didn’t think would like it,” Shipley said. “And I’d tell them that. It’s a journey story. It’s not what happens at the end that matters, and there are some people for whom that’s just not satisfying. I don’t want them to take it home, try to read it, not like it, then not go back to that writer.” Shipley knows what her customers will like because she knows who they are.

Collaborative filtering is an attempt to approximate this kind of insider knowledge. It works as a kind of doppelgänger search engine. All of us have had the experience of meeting people and discovering that they appear to have the very same tastes we do–that they really love the same obscure foreign films that we love, or that they are fans of the same little-known novelist whom we are obsessed with. If such a person recommended a book to you, you’d take that recommendation seriously, because cultural tastes seem to run in patterns. If you and your doppelgänger love the same ten books, chances are you’ll also like the eleventh book he likes. Collaborative filtering is simply a system that sifts through the opinions and preferences of thousands of people and systematically finds your doppelgänger–and then tells you what your doppelgänger’s eleventh favorite book is.

John Riedl, a University of Minnesota computer scientist who is one of the pioneers of this technology, has set up a Web site called MovieLens, which is a very elegant example of collaborative filtering at work. Everyone who logs on–and tens of thousands of people have already done so–is asked to rate a series of movies on a scale of 1 to 5, where 5 means “must see” and 1 means “awful.” For example, Irated “Rushmore” as a 5, which meant that I was put into the group of people who loved “Rushmore.” I then rated “Summer of Sam” as a 1, which put me into the somewhat smaller and more select group that both loved “Rushmore” and hated “Summer of Sam.” Collaborative-filtering systems don’t work all that well at first, because, obviously, in order to find someone’s cultural counterparts you need to know a lot more about them than how they felt about two movies. Even after I had given the system seven opinions (including “Election,” 4; “Notting Hill,” 2; “The Sting,” 4; and “Star Wars,” 1), it was making mistakes. It thought I would love “Titanic” and “Zero Effect,” and I disliked them both. But after I had plugged in about fifteen opinions–which Riedl says is probably the minimum–I began to notice that the rating that MovieLens predicted I would give a movie and the rating I actually gave it were nearly always, almost eerily, the same. The system had found a small group of people who feel exactly the same way I do about a wide range of popular movies.

What makes this collaborative-filtering system different from those you may have encountered on Amazon.com or Barnesandnoble.com? In order to work well, collaborative filtering requires a fairly representative sample of your interests or purchases. But most of us use retailers like Amazon only for a small percentage of our purchases. For example, I buy the fiction I read at the Barnes & Noble around the corner from where I live. I buy most of my nonfiction in secondhand bookstores, and I use Amazon for gifts and for occasional work-related books that I need immediately, often for a specific and temporary purpose. That’s why, bizarrely, Amazon currently recommends that I buy a number of books by the radical theorist Richard Bandler, none of which I have any desire to read. But if I were to buy a much bigger share of my books on-line, or if I “educated” the filter–as Amazon allows every customer to do–and told it what I think of its recommendations, it’s easy to see how, over time, it could turn out to be a powerful tool.

In a new book, “Net Worth,” John Hagel, an E-commerce consultant with McKinsey & Company, and his co-author, Marc Singer, suggest that we may soon see the rise of what they call “infomediaries,” which are essentially brokers who will handle our preference information. Imagine, for example, that I had set up a company that collected and analyzed all your credit-card transactions. That information could be run through a collaborative filter, and the recommendations could be sold to retailers in exchange for discounts. Steve Larsen, the senior vice-president of marketing for Net Perceptions–a firm specializing in collaborative filtering which was started by Riedl and the former Microsoft executive Steven Snyder, among others–says that someday there might be a kiosk at your local video store where you could rate a dozen or so movies and have the computer generate recommendations for you from the movies the store has in stock. “Better yet, when I go there with my wife we put in my card and her card and say, ‘Find us a movie we both like,’” he elaborates. “Or, even better yet, when we go with my fifteen-year-old daughter, ‘Find us a movie all three of us like.’” Among marketers, the hope is that such computerized recommendations will increase demand. Right now, for example, thirty-five per cent of all people who enter a video store leave empty-handed, because they can’t figure out what they want; the point of putting kiosks in those stores would be to lower that percentage. “It means that people might read more, or listen to music more, or watch videos more, because of the availability of an accurate and dependable and reliable method for them to learn about things that they might like,” Snyder says.

One of Net Perceptions’ clients is SkyMall, which is a company that gathers selections from dozens of mail-order catalogues–from Hammacher Schlemmer and L. L. Bean to the Wine Enthusiast–and advertises them in the magazines that you see in the seat pockets of airplanes. SkyMall licensed the system both for their Web site and for their 800-number call center, where the software looks for your doppelgänger while you are calling in with your order, and a few additional recommendations pop up on the operator’s screen. SkyMall’s system is still in its infancy, but, in a test, the company found that it has increased the total sales per customer somewhere between fifteen and twenty-five per cent. What’s remarkable about the SkyMall system is that it links products from many different categories. It’s one thing, after all, to surmise that if someone likes “The Remains of the Day” he is also going to like “A Room with a View.” But it’s quite another to infer that if you liked a particular item from the Orvis catalogue there’s a certain item from Reliable Home Office that you’ll also be interested in. “Their experience has been absolutely hilarious,” Larsen says. “One of the very first recommendations that came out of the engine was for a gentleman who was ordering a blue cloth shirt, a twenty-eight-dollar shirt. Our engine recommended a hundred-and-thirty-five-dollar cigar humidor–and he bought it! I don’t think anybody put those two together before.”

The really transformative potential of collaborative filtering, however, has to do with the way taste products–books, plays, movies, and the rest–can be marketed. Marketers now play an elaborate game of stereotyping. They create fixed sets of groups–middle-class-suburban, young-urban-professional, inner-city- working-class, rural-religious, and so on–and then find out enough about us to fit us into one of those groups. The collaborative-filtering process, on the other hand, starts with who we are, then derives our cultural “neighborhood” from those facts. And these groups aren’t permanent. They change as we change. I have never seen a film by Luis Buñuel, and I have no plans to. I don’t put myself in the group of people who like Buñuel. But if I were to see “That Obscure Object of Desire” tomorrow and love it, and enter my preference on MovieLens, the group of people they defined as “just like me” would immediately and subtly change.

A group at Berkeley headed by the computer scientist Ken Goldberg has, for instance, developed a collaborative-filtering system for jokes. If you log on to the site, known as Jester, you are given ten jokes to rate. (Q.: Did you hear about the dyslexic devil worshipper? A.: He sold his soul to Santa.) These jokes aren’t meant to be especially funny; they’re jokes that reliably differentiate one “sense of humor” from another. On the basis of the humor neighborhood you fall into, Jester gives you additional jokes that it thinks you’ll like. Goldberg has found that when he analyzes the data from the site–and thirty-six thousand people so far have visited Jester–the resulting neighborhoods are strikingly amorphous. In other words, you don’t find those thirty-six thousand people congregating into seven or eight basic humor groups–off-color, say, or juvenile, or literary. “What we’d like to see is nice little clusters,” Goldberg says. “But, when you look at the results, what you see is something like a cloud with sort of bunches, and nothing that is nicely defined. It’s kind of like looking into the night sky. It’s very hard to identify the constellations.” The better you understand someone’s particular taste pattern–the deeper you probe into what he finds interesting or funny–the less predictable and orderly his preferences become.

Collaborative filtering underscores a lesson that, for the better part of history, humans have been stubbornly resistant to learning: if you want to understand what one person thinks or feels or likes or does it isn’t enough to draw inferences from the general social or demographic category to which he belongs. You cannot tell, with any reasonable degree of certainty, whether someone will like “The Girl’s Guide to Hunting and Fishing” by knowing that the person is a single twenty-eight-year-old woman who lives in Manhattan, any more than you can tell whether somebody will commit a crime knowing only that he’s a twenty-eight- year-old African-American male who lives in the Bronx. Riedl has taken demographic data from the people who log on to MovieLens–such as their age and occupation and sex–but he has found that it hardly makes his predictions any more accurate. “What you tell us about what you like is far more predictive of what you will like in the future than anything else we’ve tried,” he says. “It seems almost dumb to say it, but you tell that to marketers sometimes and they look at you puzzled.”

None of this means that standard demographic data is useless. If you were trying to figure out how to market a coming- of-age movie, you’d be most interested in collaborative-filtering data from people below, say, the age of twenty-eight. Facts such as age and sex and place of residence are useful in sorting the kinds of information you get from a recommendation engine. But the central claim of the collaborative-filtering movement is that, head to head, the old demographic and “psychographic” data cannot compete with preference data. This is a potentially revolutionary argument. Traditionally, there has been almost no limit to the amount of information marketers have wanted about their customers: academic records, work experience, marital status, age, sex, race, Zip Code, credit records, focus-group sessions–everything has been relevant, because in trying to answer the question of what we want marketers have taken the long way around and tried to find out first who we are. Collaborative filtering shows that, in predicting consumer preferences, none of this information is all that important. In order to know what someone wants, what you really need to know is what they’ve wanted.

4.

How will this affect the so-called blockbuster complex? When a bookstore’s sales are heavily driven by the recommendations of a particular person–a Mary Gay Shipley–sleepers, relatively speaking, do better and blockbusters do worse. If you were going to read only Clancy and Grisham and Steel, after all, why would you need to ask Shipley what to read? This is what David Gernert, Grisham’s agent, meant when he said that in a Barnes & Noble superstore a brand like Grisham enjoys a “safe haven.” It’s a book you read when there is no one, like Shipley, with the credibility to tell you what else you ought to read. Gernert says that at this point in Grisham’s career each of his novels follows the same general sales pattern. It rides high on the best-seller lists for the first few months, of course, but, after that, “his sales pick up at very specific times–notably, Father’s Day and Mother’s Day, and then it will sell well again for Christmas.” That description makes it clear that Grisham’s books are frequently bought as gifts. And that’s because gifts are the trickiest of all purchases. They require a guess about what somebody else likes, and in conditions of uncertainty the logical decision is to buy the blockbuster, the known quantity.

Collaborative filtering is, in effect, anti-blockbuster. The more information the system has about you, the more narrow and exclusive its recommendations become. It’s just like Shipley: it uses its knowledge about you to steer you toward choices you wouldn’t normally know about. I gave MovieLens my opinions on fifteen very mainstream American movies. I’m a timid and unsophisticated moviegoer. I rarely see anything but very commercial Hollywood releases. It told me, in return, that I would love “C’est Arrivé Près de Chez Vous,” an obscure 1992 Belgian comedy, and “Shall We Dance,” the 1937 Fred and Ginger vehicle. In other words, among my moviegoing soul mates are a number of people who share my views on mainstream fare but who also have much greater familiarity with foreign and classic films. The system essentially put me in touch with people who share my tastes but who happen to know a good deal more about movies. Collaborative filtering gives voice to the expert in every preference neighborhood. A world where such customized recommendations were available would allow Shipley’s well-read opinions to be known not just in Blytheville but wherever there are people who share her taste in books.

Collaborative filtering, in short, has the ability to reshape the book market. When customized recommendations are available, choices become more heterogeneous. Big bookstores lose their blockbuster bias, because customers now have a way of narrowing down their choices to the point where browsing becomes easy again. Of the top hundred best-selling books of the nineteen-nineties, there are only a handful that can accurately be termed sleepers–Robert James Waller’s “The Bridges of Madison County,” James Redfield’s “The Celestine Prophecy,” John Berendt’s “Midnight in the Garden of Good and Evil,” Charles Frazier’s “Cold Mountain.” Just six authors–John Grisham, Tom Clancy, Stephen King, Michael Crichton, Dean Koontz, and Danielle Steel–account for sixty-three of the books on the list. In a world more dependent on collaborative filtering, Grisham, Clancy, King, and Steel would still sell a lot of books. But you’d expect to see many more books like “Divine Secrets of the Ya-Ya Sisterhood”–many more new writers–make their way onto the best- seller list. And the gap between the very best selling books and those in the middle would narrow. Collaborative filtering, Hagel says, “favors the smaller, the more talented, more quality products that may have a hard time getting visibility because they are not particularly good at marketing.”

5.

In recent years, That Bookstore in Blytheville has become a mecca for fiction in the South. Prominent writers drop by all the time to give readings in the back, by the potbellied stove. John Grisham himself has been there nine times, beginning with his tour for “The Firm,” which was the hit that turned him into a blockbuster author. Melinda Haynes, Bobbie Ann Mason, Roy Blount, Jr., Mary Higgins Clark, Billie Letts, Sandra Brown, Jill Conner Browne, and countless others have recently made the drive up from Memphis. Sometimes Shipley will host a supper for them after the reading, and send the proceeds from the event to a local literacy program.

There seems, in this era of mega-stores, something almost impossibly quaint about That Bookstore in Blytheville. The truth is, though, that the kind of personalized recommendation offered by Mary Gay Shipley represents the future of marketing, not its past. The phenomenal success in recent years of Oprah Winfrey’s book club–which created one best-seller after another on the strength of its nominations–suggests that, in this age of virtually infinite choice, readers are starved for real advice, desperate for a recommendation from someone they know and who they feel knows them. “Certain people don’t want to waste their time experimenting with new books, and the function we provide here is a filter,” Shipley says, and as she speaks you can almost hear the makings of another sleeper on the horizon. “If we like something, we get behind it. I’m reading a book right now called ‘Nissa’s Place,’ by Alexandria LaFaye. She’s a woman I think we’re going to be hearing more from.”

© 2013 Malcolm Gladwell.

DAVID AND GOLIATH
 
WHAT THE DOG SAW
 
OUTLIERS
 
BLINK
 
THE TIPPING POINT
 
ARTICLES
 
ETC.
The Sure Thing

Posted January 18, 2010 by MALCOLM GLADWELL & filed under ANNALS OF BUSINESS, THE NEW YORKER - ARCHIVE.

How entrepreneurs really succeed.

1.

In 1969, Ted Turner wanted to buy a television station. He was thirty years old. He had inherited a billboard business from his father, which was doing well. But he was bored, and television seemed exciting. “He knew absolutely nothing about it,” one of Turner’s many biographers, Christian Williams, writes in “Lead, Follow or Get Out of the Way” (1981). “It would be fun to risk everything he had built, scare the hell out of everybody, and get back in the front seat of the roller coaster.”

The station in question was WJRJ, Channel 17, in Atlanta. It was an independent station on the UHF band, the lonely part of the television spectrum which viewers needed a special antenna to find. It was housed in a run-down cinder-block building near a funeral home, leading to the joke that it was at death’s door. The equipment was falling apart. The staff was incompetent. It had no decent programming to speak of, and it was losing more than half a million dollars a year. Turner’s lawyer, Tench Coxe, and his accountant, Irwin Mazo, were firmly opposed to the idea. “We tried to make it clear that—yes—this thing might work, but if it doesn’t everything will collapse,” Mazo said, years later. “Everything you’ve got will be gone. . . . It wasn’t just us, either. Everybody told him not to do it.”

Turner didn’t listen. He was Captain Courageous, the man with nerves of steel who went on to win the America’s Cup, take on the networks, marry a movie star, and become a billionaire. He dressed like a cowboy. He gave the impression of signing contracts without looking at them. He was a drinker, a yeller, a man of unstoppable urges and impulses, the embodiment of the entrepreneur as risk-taker. He bought the station, and so began one of the great broadcasting empires of the twentieth century.

What is sometimes forgotten amid the mythology, however, is that Turner wasn’t the proprietor of any old billboard company. He had inherited the largest outdoor-advertising firm in the South, and billboards, in the nineteen-sixties and seventies, were enormously lucrative. They benefitted from favorable tax-depreciation rules, they didn’t require much capital investment, and they produced rivers of cash. WJRJ’s losses could be used to offset the taxes on the profits of Turner’s billboard business. A television station, furthermore, fit very nicely into his existing business. Television was about selling ads, and Turner was very experienced at ad-selling. WJRJ may have been a virtual unknown in the Atlanta market, but Turner had billboards all over the city that were blank about fifteen per cent of the time. He could advertise his new station free. As for programming, Turner had a fix for that, too. In those days, the networks offered their local affiliates a full slate of shows, and whenever an affiliate wanted to broadcast local programming, such as sports or news, the national shows were preëmpted. Turner realized that he could persuade the networks in New York to let him have whatever programming their affiliates weren’t running. That’s exactly what happened. “When we reached the point of having four preempted NBC shows running in our daytime lineup,” Turner writes in his autobiography, “Call Me Ted” (2008), “I had our people put up some billboards saying ‘THE NBC NETWORK MOVES TO CHANNEL 17.’ ”

Williams writes that Turner was “attracted to the risk” of the deal, but it seems just as plausible to say that he was attracted by the deal’s lack of risk. “We don’t want to put it all on the line, because the result can’t possibly be worth the risk,” Mazo recalls warning Turner. Put it all on the line? The purchase price for WJRJ was $2.5 million. Similar properties in that era went for many times that, and Turner paid with a stock swap engineered in such a way that he didn’t have to put a penny down. Within two years, the station was breaking even. By 1973, it was making a million dollars in profit.

In a recent study, “From Predators to Icons,” the French scholars Michel Villette and Catherine Vuillermot set out to discover what successful entrepreneurs have in common. They present case histories of businessmen who built their own empires—ranging from Sam Walton, of Wal-Mart, to Bernard Arnault, of the luxury-goods conglomerate L.V.M.H.—and chart what they consider the typical course of a successful entrepreneur’s career. There is almost always, they conclude, a moment of great capital accumulation—a particular transaction that catapults him into prominence. The entrepreneur has access to that deal by virtue of occupying a “structural hole,” a niche that gives him a unique perspective on a particular market. Villette and Vuillermot go on, “The businessman looks for partners to a transaction who do not have the same definition as he of the value of the goods exchanged, that is, who undervalue what they sell to him or overvalue what they buy from him in comparison to his own evaluation.” He moves decisively. He repeats the good deal over and over again, until the opportunity closes, and—most crucially—his focus throughout that sequence is on hedging his bets and minimizing his chances of failure. The truly successful businessman, in Villette and Vuillermot’s telling, is anything but a risk-taker. He is a predator, and predators seek to incur the least risk possible while hunting.

Giovanni Agnelli, the founder of Fiat, financed his young company with the money of investors—who were “subsequently excluded from the company by a maneuver by Agnelli,” the authors point out. Bernard Arnault took over the Boussac group at a personal cost of forty million francs, which was a fraction of the “immediate resale value of the assets.” The French industrialist Vincent Bolloré “took charge of the failing family company for almost nothing with other people’s money.” George Eastman, the founder of Kodak, shifted the financial risk of his new enterprise to his family and to his wealthy friend Henry Strong. IKEA’s founder, Ingvar Kamprad, arranged to get his furniture made in Communist Poland for half of what it would cost him in Sweden. Marcel Dassault, the French aviation pioneer, did a study for the French Army that pointed out the value of propellers, and then took over a propeller manufacturer. When he started making planes for the military, he made sure he was paid in advance.

People like Dassault and Eastman and Arnault and Turner are all successful entrepreneurs, businessmen whose insights and decisions have transformed the economy, but their entrepreneurial spirit could not have less in common with that of the daring risk-taker of popular imagination. Would we so revere risk-taking if we realized that the people who are supposedly taking bold risks in the cause of entrepreneurship are actually doing no such thing?

2.

The most successful entrepreneur on Wall Street—certainly of the past decade and perhaps even of the postwar era—is a hedge-fund manager named John Paulson. He started a small money-management business in the nineteen-nineties and built it into a juggernaut, and Gregory Zuckerman’s recent account of Paulson’s triumph, “The Greatest Trade Ever,” offers a fascinating perspective on the predator thesis.

Paulson grew up in middle-class Queens, the child of an immigrant father. His career on Wall Street started relatively slowly. He launched his firm in 1994, when he was nearly forty years old, specializing in merger arbitrage. By 2004, Paulson was managing about two billion dollars of other people’s money, putting him in the middle ranks of hedge funds. He was, Zuckerman writes, a “solid investor, careful and decidedly unspectacular.” The particular kinds of deal he did were “among the safest forms of investing.” One of Paulson’s mentors was an investor named Marty Gruss, and, Zuckerman writes, “the ideal Gruss investment had limited risk but held the promise of a potential fortune. Marty Gruss drilled a maxim into Paulson: ‘Watch the downside; the upside will take care of itself.’ At his firm, he asked his analysts repeatedly, ‘How much can we lose on this trade?’ ” Long after he became wealthy, he would take the bus to his offices in midtown, and the train out to his summer house on Long Island. He was known for getting around the Hamptons on his bicycle.

By 2004-05, Paulson was increasingly suspicious of the real-estate boom. He decided to short the mortgage market, using a financial tool known as the credit-default swap, or C.D.S. A credit-default swap is like an insurance policy. Wall Street banks combined hundreds of mortgages together in bundles, and investors could buy insurance on any of the bundles they chose. Suppose I put together a bundle of ten mortgages totalling a million dollars. I could sell you a one-year C.D.S. policy on that bundle for, say, a hundred thousand dollars. If after the year was up the ten homeowners holding those mortgages were all making their monthly payments, I’d pocket your hundred thousand. If, however, those homeowners all defaulted, I’d owe you the full value of the bundle—a million dollars. Throughout the boom, countless banks and investment firms sold C.D.S. policies on securities backed by subprime loans, happily pocketing the annual premiums in the belief that there was little chance of ever having to make good on the contract. Paulson, as often as not, was the one on the other side of the trade. He bought C.D.S. contracts by the truckload, and, when he ran out of money, he found new investors, raising billions of new dollars so he could buy even more. By the time the crash came, he was holding insurance on some twenty-five billion dollars’ worth of subprime mortgages.

Was Paulson’s trade risky? Conventional wisdom said that it was. This kind of deal is known, in Wall Street parlance, as a “negative-carry” trade, and, as Zuckerman writes, negative-carry trades are a “maneuver that investment pros detest almost as much as high taxes and coach-class seating.” Their problem with negative-carry is that if the trade doesn’t pay off quickly it can become ruinously expensive. It’s one thing if I pay you a hundred thousand dollars for one year’s insurance on a million dollars’ worth of mortgages, and the mortgages go belly up after six months. But what if I pay premiums for two years, and the bubble still hasn’t burst? Then I’m out two hundred thousand dollars, with nothing to show for my efforts. And what if the bubble hasn’t burst after three years? Now I have a very nervous group of investors. To win at a negative-carry trade, you have not only to correctly predict the presence of a bubble but also to correctly predict when the bubble is about to burst.

At one point before the crash, Zuckerman writes, a trader at Morgan Stanley “hung up the phone after yet another Paulson order and turned to a colleague in disbelief. ‘This guy is nuts,’ he said with a chuckle, amazed that Paulson was agreeing to make so many annual insurance payments. ‘He’s just going to pay it all out?’ ” Wall Street thought that Paulson was crazy.

But Paulson wasn’t crazy at all. In 2006, he had his firm undertake a rigorous analysis of the housing market, led by Paulson’s associate Paolo Pellegrini. At that point, it was unclear whether rising housing prices represented a bubble or a legitimate phenomenon. Pellegrini concluded that housing prices had risen on average 1.4 per cent annually between 1975 and 2000, once inflation had been accounted for. In the next five years, though, they had risen seven per cent a year—to the point where they would have to fall by forty per cent to be back in line with historical trends. That fact left Paulson certain that he was looking at a bubble.

Paulson’s next concern was with the volatility of the housing market. Was this bubble resilient? Or was everything poised to come crashing down? Zuckerman tells how Pellegrini and another Paulson associate, Sihan Shu, “purchased enormous databases tracking the historic performance of more than six million mortgages in various parts of the country.” Thus equipped,

they crunched the numbers, tinkered with logarithms and logistic functions, and ran different scenarios, trying to figure out what would happen if housing prices stopped rising. Their findings seemed surprising: Even if prices just flatlined, homeowners would feel so much financial pressure that it would result in losses of 7 percent of the value of a typical pool of subprime mortgages. And if home prices fell 5 percent, it would lead to losses as high as 17 percent.

This was a crucial finding. Most people at the time believed that widespread defaults on mortgages were a function of some combination of structural economic factors such as unemployment rates, interest rates, and regional economic health. That’s why so many on Wall Street were happy to sell Paulson C.D.S. policies: they thought it would take a perfect storm to bring the market to its knees. But Pellegrini’s data showed that the bubble was being inflated by a single, rickety factor—rising home prices. It wouldn’t take much for the bubble to burst.

Paulson then looked at what buying disaster insurance on mortgages would cost. C.D.S. contracts can sometimes be prohibitively expensive. In the months leading up to General Motors’ recent bankruptcy, for example, a year’s insurance on a million of the carmaker’s bonds sold for eight hundred thousand dollars. If Paulson had to pay anything like that amount, there wouldn’t be much room for error. To his amazement, though, he found that to insure a million dollars of mortgages would cost him just ten thousand dollars—and this was for some of the most dubious and high-risk subprime mortgages. Paulson didn’t even need a general housing-market collapse to make his money. He needed only the most vulnerable of all homeowners to start defaulting. It was a classic asymmetrical trade. If Paulson raised a billion dollars from investors, he could buy a year’s worth of insurance on twelve billion dollars of subprime loans for a hundred and twenty million. That’s an outlay of twelve per cent up front. But, Zuckerman explains,

because premiums on CDS contracts, like those on any other insurance product, are paid out over time, the new fund could keep most of its money in the bank until the CDS bills came due, and thereby earn about 5 percent a year. That would cut the annual cost to the fund to a more reasonable 7 percent. Since Paulson would charge 1 percent a year as a management fee, the most an investor could lose would be 8 percent a year. . . . And the upside? If Paulson purchased CDS contracts that fully protected $12 billion of subprime mortgage bonds and the bonds somehow became worthless, Paulson & Co. would make a cool $12 billion.

“There’s never been an opportunity like this,” Paulson gushed to a colleague, as he made one bet after another. By “never,” he meant never ever—not in his lifetime and not in anyone else’s, either. In one of the book’s many memorable scenes, Zuckerman describes how a five-point decline in what’s called the ABX index (a measure of mortgage health) once made Paulson $1.25 billion in one morning. In 2007 alone, Paulson & Co. took in fifteen billion dollars in profits, of which four billion went directly into Paulson’s pocket. In 2008, his firm made five billion dollars. Rarely in human history has anyone made so much money is so short a time.

What Paulson’s story makes clear is how different the predator is from our conventional notion of the successful businessman. The risk-taking model suggests that the entrepreneur’s chief advantage is one of temperament—he’s braver than the rest of us are. In the predator model, the entrepreneur’s advantage is analytical—he’s better at figuring out a sure thing than the rest of us. Paulson looked at the same marketplace as everyone else on Wall Street did. But he saw a different pattern. As an outsider, he had fresh eyes, and his line of investing made him a lot more comfortable with negative-carry trades than his competitors were. He looked for and found partners to the transaction who did not have the same definition as he of the value of the goods exchanged—that is, the banks selling credit-default swaps for a penny on the dollar—and he exploited that advantage ruthlessly. At one point, incredibly, Paulson got together with some investment banks to assemble bundles of the most absurdly toxic mortgages—which the banks then sold to some hapless investors and Paulson then promptly bet against. As Zuckerman points out, this is the equivalent of a game of football in which the defense calls the plays for the offense. It’s how a nerd would play football, not a jock.

This is exactly how Turner pulled off another of his legendary early deals—his 1976 acquisition of the Atlanta Braves baseball team. Turner’s Channel 17 was the Braves’ local broadcaster, having acquired the rights four years before—a brilliant move, as it turned out, because it forced every Braves fan in the region to go out and buy a UHF antenna. (Well before ESPN and Rupert Murdoch’s Sky TV, Turner had realized how important live sports programming could be in building a television brand.) The team was losing a million dollars a year, and the owners wanted ten million dollars to sell. That was four times the price of Channel 17. “I had no idea how I could afford it,” Turner told one of his biographers, although by this point the reader is wise to his aw-shucks modesty. First, he didn’t pay ten million dollars. He talked the Braves into taking a million down, and the rest over eight or so years. Second, he didn’t end up paying the million down. Somewhat mysteriously, Turner reports that he found a million dollars on the team’s books—money the previous owners somehow didn’t realize they had—and so, he says, “I bought it using its own money, which was quite a trick.” He now owed nine million dollars. But Turner had already been paying the Braves six hundred thousand dollars a year for the rights to broadcast sixty of the team’s games. What the deal consisted of, then, was his paying an additional six hundred thousand dollars or so a year, for eight years: in return, he would get the rights to all a hundred and sixty-two of the team’s games, plus the team itself.

You and I might not have made that deal. But that’s not because Turner is a risk-taker and we are cowards. It’s because Turner is a cold-blooded bargainer who could find a million dollars in someone’s back pocket that the person didn’t know he had. Once you get past the more flamboyant aspects of Turner’s personal and sporting life, in fact, there is little evidence that he had any real appetite for risk at all. In his memoir, Turner tells us that when he was starting out in the family business his father, Ed, bought another billboard firm, called General Outdoor. That was the acquisition that launched the Turner company as a major advertising player in the South, and it involved taking on a sizable amount of debt. Young Ted had no qualms, intellectually, about the decision. He could do the math. There were substantial economies of scale in the advertising business: the bigger you got, the lower your costs were, and paying off the debt from the General Outdoor purchase, Ted Turner realized, probably wasn’t going to be a problem. But Turner’s father did something that Turner, when he was building his empire, always went to extraordinary lengths to avoid: he put his own capital into the deal. In the highly unlikely event that it didn’t work out, Turner Advertising would be crippled. It was a good deal, not a perfect one, and that niggling imperfection, along with the toll that the uncertainty was taking on his father, left Turner worried sick. “During the first six months or so after the General Outdoor acquisition my weight dropped from 180 pounds to 135,” he writes. “I developed a pre-ulcerative condition and my doctor made me swear off coffee. I’d get so tired and agitated that one of my eyelids developed a twitch.”

Zuckerman profiles John Paulson alongside three others who made the same subprime bet—Greg Lippmann, a trader at Deutsche Bank; Jeffrey Greene, a real-estate mogul in Los Angeles; and Michael Burry, who ran a hedge fund in Silicon Valley—and finds the same pattern. All were supremely confident of their decision. All had done their homework. All had swooped down, like perfect predators, on a marketplace anomaly. But these were not men temperamentally suited to risk-taking. They worked so hard to find the sure thing because anything short of that gave them ulcers. Here is Zuckerman on Burry, as he waited for his trade to pan out:

In a tailspin, Burry withdrew from his friends, family, and employees. Each morning, Burry walked into his firm and made a beeline to his office, head down, locking the door behind him. He didn’t emerge all day, not even to eat or use the bathroom. His remaining employees, who were still pulling for Burry, turned worried. Sometimes he got into the office so early, and kept the door closed for so long, that when his staff left at the end of the day, they were unsure if their boss had ever come in. Other times, Burry pounded his fists on his desk, trying to release his tension, as heavy-metal music blasted from nearby speakers.

3.

Paulson’s story also casts a harsh light on the prevailing assumptions behind corporate compensation policies. One of the main arguments for the generous stock options that are so often given to C.E.O.s is that they are necessary to encourage risk-taking in the corporate suite. This notion comes from what is known as “agency theory,” which Freek Vermeulen, of the London Business School, calls “one of the few academic theories in management academia that has actually influenced the world of management practice.” Agency theory, Vermeulen observes, “says that managers are inherently risk-averse; much more risk-averse than shareholders would like them to be. And the theory prescribes that you should give them stock options, rather than stock, to stimulate them to take more risk.” Why do shareholders want managers to take more risks? Because they want stodgy companies to be more entrepreneurial, and taking risks is what everyone says that entrepreneurs do.

The result has been to turn executives into risk-takers. Paulson, for his part, was stunned at the reckless behavior of his Wall Street counterparts. Some of the mortgage bundles he was betting against—collections of some of the sketchiest subprime loans—were paying the investors who bought them six-per-cent interest. Treasury bonds, the safest investment in the world, were paying almost five per cent at that point. Nor could he comprehend why so many banks were willing to sell him C.D.S. insurance at such low prices. Why would someone, in the middle of a housing bubble, demand only one cent on the dollar? At the end of 2006, Merrill Lynch paid $1.3 billion for First Franklin Financial, one of the biggest subprime lenders in the country, bringing the total value of subprime mortgages on its books to eleven billion dollars. Paulson was so risk-averse that he didn’t so much as put a toe in the water of subprime-mortgage default swaps until Pellegrini had done months of analysis. But Merrill Lynch bought First Franklin even though the firm’s own economists were predicting that housing prices were about to drop by as much as five per cent. “It just doesn’t make sense,” an incredulous Paulson told his friend Howard Gurvitch. “These are supposedly the smart people.”

The economist Scott Shane, in his book “The Illusions of Entrepreneurship,” makes a similar argument. Yes, he says, many entrepreneurs take plenty of risks—but those are generally the failed entrepreneurs, not the success stories. The failures violate all kinds of established principles of new-business formation. New-business success is clearly correlated with the size of initial capitalization. But failed entrepreneurs tend to be wildly undercapitalized. The data show that organizing as a corporation is best. But failed entrepreneurs tend to organize as sole proprietorships. Writing a business plan is a must; failed entrepreneurs rarely take that step. Taking over an existing business is always the best bet; failed entrepreneurs prefer to start from scratch. Ninety per cent of the fastest-growing companies in the country sell to other businesses; failed entrepreneurs usually try selling to consumers, and, rather than serving customers that other businesses have missed, they chase the same people as their competitors do. The list goes on: they underemphasize marketing; they don’t understand the importance of financial controls; they try to compete on price. Shane concedes that some of these risks are unavoidable: would-be entrepreneurs take them because they have no choice. But a good many of these risks reflect a lack of preparation or foresight.

4.

Shane’s description of the pattern of entrepreneurial failure brings to mind the Harvard psychologist David McClelland’s famous experiment with kindergarten children in the nineteen-fifties. McClelland watched a group of kids play ringtoss—throwing a hoop over a pole. The children who played the game in the riskiest manner, who stood so far from the pole that success was unlikely, also scored lowest on what he called “achievement motive,” that is, the desire to succeed. (Another group of low scorers were at the other extreme, standing so close to the pole that the game ceased to be a game at all.) Taking excessive risks was, then, a psychologically protective strategy: if you stood far enough back from the pole, no one could possibly blame you if you failed. These children went out of their way to take a “professional” risk in order to avoid a personal risk. That’s what companies are buying with their bloated C.E.O. stock-options packages—gambles so wild that the gambler can lose without jeopardizing his social standing within the corporate world. “As long as the music is playing, you’ve got to get up and dance,” the now departed C.E.O. of Citigroup, Charles Prince, notoriously said, as his company continued to pile one dubious investment on another. He was more afraid of being a wallflower than he was of imperilling his firm.

The successful entrepreneur takes the opposite tack. Villette and Vuillermot point out that the predator is often quite happy to put his reputation on the line in the pursuit of the sure thing. Ingvar Kamprad, of IKEA, went to Poland in the nineteen-sixties to get his furniture manufactured. Since Polish labor was inexpensive, it gave Kamprad a huge price advantage. But doing business with a Communist country at the height of the Cold War was a scandal. Sam Walton financed his first retailing venture, in Newport, Arkansas, with money from his wealthy in-laws. That approach was safer than turning to a bank, especially since Walton was forced out of Newport and had to go back to his wife’s family for another round. But you can imagine that it made for some tense moments at family reunions for a while. Deutsche Bank’s Lippmann, meanwhile, was called Chicken Little and Bubble Boy to his face for his insistence that the mortgage market was going to burst.

Why are predators willing to endure this kind of personal abuse? Perhaps they are sufficiently secure and confident that they don’t need public approval. Or perhaps they are so caught up in their own calculations that they don’t notice. The simplest explanation, though, is that it’s just another manifestation of their relentlessly rational pursuit of the sure thing. If an awkward family reunion was the price Walton had to pay for a guaranteed line of credit, then so be it. He went out of his way to take a personal risk in order to avoid a professional risk. Reputation, after all, is a commodity that trades in the marketplace at a significant and often excessive premium. The predator shorts the dancers, and goes long on the wallflowers.

5.

When Pellegrini finally finished his research on the mortgage market—proving how profoundly inflated home prices had become—he rushed in to show his findings to his boss. Zuckerman writes:

“This is unbelievable!” Paulson said, unable to take his eyes off the chart. A mischievous smile formed on his face, as if Pellegrini had shared a secret no one else was privy to. Paulson sat back in his chair and turned to Pellegrini. “This is our bubble! This is proof. Now we can prove it!” Paulson said. Pellegrini grinned, unable to mask his pride. The chart was Paulson’s Rosetta stone, the key to making sense of the entire housing market. Years later, he would keep it atop a pile of papers on his desk, showing it off to his clients and updating it each month with new data, like a car collector gently waxing and caressing a prized antique auto. . . . “I still look at it. I love that chart,” Paulson says.

There are a number of moments like this in “The Greatest Trade Ever,” when it becomes clear just how much Paulson enjoyed his work. Yes, he wanted to make money. But he was fabulously wealthy long before he tackled the mortgage business. His real motivation was the challenge of figuring out a particularly knotty problem. He was a kid with a puzzle.

This is consistent with the one undisputed finding in all the research on entrepreneurship: people who work for themselves are far happier than the rest of us. Shane says that the average person would have to earn two and a half times as much to be as happy working for someone else as he would be working for himself. And people who like what they do are profoundly conservative. When the sociologists Hongwei Xu and Martin Ruef asked a large sample of entrepreneurs and non-entrepreneurs to choose among three alternatives—a business with a potential profit of five million dollars with a twenty-per-cent chance of success, or one with a profit of two million with a fifty-per-cent chance of success, or one with a profit of $1.25 million with an eighty-per-cent chance of success—it was the entrepreneurs who were more likely to go with the third, safe choice. They weren’t dazzled by the chance of making five million dollars. They were drawn to the eighty-per-cent chance of getting to do what they love doing. The predator is a supremely rational actor. But, deep down, he is also a romantic, motivated by the simple joy he finds in his work.

In “Call Me Ted,” Turner tells the story of one of his first great traumas. When Turner was twenty-four, his father committed suicide. He had been depressed and troubled for some months, and one day after breakfast he went upstairs and shot himself. After the funeral, it emerged that the day before his death Turner’s father had sold the crown jewels of the family business—the General Outdoor properties—to a man named Bob Naegele. Turner was grief-stricken. But he fought back. He hired away the General Outdoor leasing department. He began “jumping” the company’s leases—that is, persuading the people who owned the real estate on which the General Outdoor billboards sat to cancel the leases and sign up with Turner Advertising. Then he flew to Palm Springs and strong-armed Naegele into giving back the business. Turner the rational actor negotiated the deal. But it was Turner the romantic who had the will, at the moment of his greatest grief, to fight back. What Turner understood was that none of his grand ambitions were possible without the billboard cash machine. He had felt the joy that comes with figuring out a particularly knotty problem, and he couldn’t give that up. Naegele, by the way, asked for two hundred thousand dollars, which Turner didn’t have. But Turner realized that for someone in Naegele’s tax bracket a flat payment like that made no sense. He countered with two hundred thousand dollars in Turner Advertising stock. “So far so good,” Turner writes in his autobiography. “I had kept the company out of Naegele’s hands and it didn’t cost me a single dollar of cash.” Of course it didn’t. He’s a predator. Why on earth would he take a risk like that?

© 2013 Malcolm Gladwell.

DAVID AND GOLIATH
 
WHAT THE DOG SAW
 
OUTLIERS
 
BLINK
 
THE TIPPING POINT
 
ARTICLES
 
ETC.
Designs for Working

Posted December 11, 2000 by MALCOLM GLADWELL & filed under DEPT. OF HUMAN RESOURCES, THE NEW YORKER - ARCHIVE.

Why your bosses want to turn your new office into Greenwich Village.

1.

In the early nineteen-sixties, Jane Jacobs lived on Hudson Street, in Greenwich Village, near the intersection of Eighth Avenue and Bleecker Street. It was then, as now, a charming district of nineteenth-century tenements and town houses, bars and shops, laid out over an irregular grid, and Jacobs loved the neighborhood. In her 1961 masterpiece, “The Death and Life of Great American Cities,” she rhapsodized about the White Horse Tavern down the block, home to Irish longshoremen and writers and intellectuals–a place where, on a winter’s night, as “the doors open, a solid wave of conversation and animation surges out and hits you.” Her Hudson Street had Mr. Slube, at the cigar store, and Mr. Lacey, the locksmith, and Bernie, the candy-store owner, who, in the course of a typical day, supervised the children crossing the street, lent an umbrella or a dollar to a customer, held on to some keys or packages for people in the neighborhood, and “lectured two youngsters who asked for cigarettes.” The street had “bundles and packages, zigzagging from the drug store to the fruit stand and back over to the butcher’s,” and “teenagers, all dressed up, are pausing to ask if their slips show or their collars look right.” It was, she said, an urban ballet.

The miracle of Hudson Street, according to Jacobs, was created by the particular configuration of the streets and buildings of the neighborhood. Jacobs argued that when a neighborhood is oriented toward the street, when sidewalks are used for socializing and play and commerce, the users of that street are transformed by the resulting stimulation: they form relationships and casual contacts they would never have otherwise. The West Village, she pointed out, was blessed with a mixture of houses and apartments and shops and offices and industry, which meant that there were always people “outdoors on different schedules and… in the place for different purposes.” It had short blocks, and short blocks create the greatest variety in foot traffic. It had lots of old buildings, and old buildings have the low rents that permit individualized and creative uses. And, most of all, it had people, cheek by jowl, from every conceivable walk of life. Sparely populated suburbs may look appealing, she said, but without an active sidewalk life, without the frequent, serendipitous interactions of many different people, “there is no public acquaintanceship, no foundation of public trust, no cross-connections with the necessary people–and no practice or ease in applying the most ordinary techniques of city public life at lowly levels.”

Jane Jacobs did not win the battle she set out to fight. The West Village remains an anomaly. Most developers did not want to build the kind of community Jacobs talked about, and most Americans didn’t want to live in one. To reread “Death and Life” today, however, is to be struck by how the intervening years have given her arguments a new and unexpected relevance. Who, after all, has a direct interest in creating diverse, vital spaces that foster creativity and serendipity? Employers do. On the fortieth anniversary of its publication, “Death and Life” has been reborn as a primer on workplace design.

The parallels between neighborhoods and offices are striking. There was a time, for instance, when companies put their most valued employees in palatial offices, with potted plants in the corner, and secretaries out front, guarding access. Those offices were suburbs–gated communities, in fact–and many companies came to realize that if their best employees were isolated in suburbs they would be deprived of public acquaintanceship, the foundations of public trust, and cross-connections with the necessary people. In the eighties and early nineties, the fashion in corporate America was to follow what designers called “universal planning”–rows of identical cubicles, which resembled nothing so much as a Levittown. Today, universal planning has fallen out of favor, for the same reason that the postwar suburbs like Levittown did: to thrive, an office space must have a diversity of uses–it must have the workplace equivalent of houses and apartments and shops and industry.

If you visit the technology companies of Silicon Valley, or the media companies of Manhattan, or any of the firms that self-consciously identify themselves with the New Economy, you’ll find that secluded private offices have been replaced by busy public spaces, open-plan areas without walls, executives next to the newest hires. The hush of the traditional office has been supplanted by something much closer to the noisy, bustling ballet of Hudson Street. Forty years ago, people lived in neighborhoods like the West Village and went to work in the equivalent of suburbs. Now, in one of the odd reversals that mark the current economy, they live in suburbs and, increasingly, go to work in the equivalent of the West Village.

2.

The office used to be imagined as a place where employees punch clocks and bosses roam the halls like high-school principals, looking for miscreants. But when employees sit chained to their desks, quietly and industriously going about their business, an office is not functioning as it should. That’s because innovation–the heart of the knowledge economy–is fundamentally social. Ideas arise as much out of casual conversations as they do out of formal meetings. More precisely, as one study after another has demonstrated, the best ideas in any workplace arise out of casual contacts among different groups within the same company. If you are designing widgets for Acme.com, for instance, it is unlikely that a breakthrough idea is going to come from someone else on the widget team: after all, the other team members are as blinkered by the day-to- day demands of dealing with the existing product as you are. Someone from outside Acme.com–your old engineering professor, or a guy you used to work with at Apex.com–isn’t going to be that helpful, either. A person like that doesn’t know enough about Acme’s widgets to have a truly useful idea. The most useful insights are likely to come from someone in customer service, who hears firsthand what widget customers have to say, or from someone in marketing, who has wrestled with the problem of how to explain widgets to new users, or from someone who used to work on widgets a few years back and whose work on another Acme product has given him a fresh perspective. Innovation comes from the interactions of people at a comfortable distance from one another, neither too close nor too far. This is why–quite apart from the matter of logistics and efficiency–companies have offices to begin with. They go to the trouble of gathering their employees under one roof because they want the widget designers to bump into the people in marketing and the people in customer service and the guy who moved to another department a few years back.

The catch is that getting people in an office to bump into people from another department is not so easy as it looks. In the sixties and seventies, a researcher at M.I.T. named Thomas Allen conducted a decade-long study of the way in which engineers communicated in research-and-development laboratories. Allen found that the likelihood that any two people will communicate drops off dramatically as the distance between their desks increases: we are four times as likely to communicate with someone who sits six feet away from us as we are with someone who sits sixty feet away. And people seated more than seventy-five feet apart hardly talk at all.

Allen’s second finding was even more disturbing. When the engineers weren’t talking to those in their immediate vicinity, many of them spent their time talking to people outside their company–to their old computer-science professor or the guy they used to work with at Apple. He concluded that it was actually easier to make the outside call than to walk across the room. If you constantly ask for advice or guidance from people inside your organization, after all, you risk losing prestige. Your colleagues might think you are incompetent. The people you keep asking for advice might get annoyed at you. Calling an outsider avoids these problems. “The engineer can easily excuse his lack of knowledge by pretending to be an `expert in something else’ who needs some help in `broadening into this new area,’” Allen wrote. He did his study in the days before E-mail and the Internet, but the advent of digital communication has made these problems worse. Allen’s engineers were far too willing to go outside the company for advice and new ideas. E-mail makes it even easier to talk to people outside the company.

The task of the office, then, is to invite a particular kind of social interaction–the casual, nonthreatening encounter that makes it easy for relative strangers to talk to each other. Offices need the sort of social milieu that Jane Jacobs found on the sidewalks of the West Village. “It is possible in a city street neighborhood to know all kinds of people without unwelcome entanglements, without boredom, necessity for excuses, explanations, fears of giving offense, embarrassments respecting impositions or commitments, and all such paraphernalia of obligations which can accompany less limited relationships,” Jacobs wrote. If you substitute “office” for “city street neighborhood,” that sentence becomes the perfect statement of what the modern employer wants from the workplace.

3.

Imagine a classic big-city office tower, with a floor plate of a hundred and eighty feet by a hundred and eighty feet. The center part of every floor is given over to the guts of the building: elevators, bathrooms, electrical and plumbing systems. Around the core are cubicles and interior offices, for support staff and lower management. And around the edges of the floor, against the windows, are rows of offices for senior staff, each room perhaps two hundred or two hundred and fifty square feet. The best research about office communication tells us that there is almost no worse way to lay out an office. The executive in one corner office will seldom bump into any other executive in a corner office. Indeed, stringing the exterior offices out along the windows guarantees that there will be very few people within the critical sixty-foot radius of those offices. To maximize the amount of contact among employees, you really ought to put the most valuable staff members in the center of the room, where the highest number of people can be within their orbit. Or, even better, put all places where people tend to congregate–the public areas–in the center, so they can draw from as many disparate parts of the company as possible. Is it any wonder that creative firms often prefer loft-style buildings, which have usable centers?

Another way to increase communication is to have as few private offices as possible. The idea is to exchange private space for public space, just as in the West Village, where residents agree to live in tiny apartments in exchange for a wealth of nearby cafés and stores and bars and parks. The West Village forces its residents outdoors. Few people, for example, have a washer and dryer in their apartment, and so even laundry is necessarily a social event: you have to take your clothes to the laundromat down the street. In the office equivalent, designers force employees to move around, too. They build in “functional inefficiencies”; they put kitchens and copiers and printers and libraries in places that can be reached only by a circuitous journey.

A more direct approach is to create an office so flexible that the kinds of people who need to spontaneously interact can actually be brought together. For example, the Ford Motor Company, along with a group of researchers from the University of Michigan, recently conducted a pilot project on the effectiveness of “war rooms” in software development. Previously, someone inside the company who needed a new piece of software written would have a series of meetings with the company’s programmers, and the client and the programmers would send messages back and forth. In the war-room study, the company moved the client, the programmers, and a manager into a dedicated conference room, and made them stay there until the project was done. Using the war room cut the software-development time by two-thirds, in part because there was far less time wasted on formal meetings or calls outside the building: the people who ought to have been bumping into each other were now sitting next to each other.

Two years ago, the advertising agency TBWAChiatDay moved into new offices in Los Angeles, out near the airport. In the preceding years, the firm had been engaged in a radical, and in some ways disastrous, experiment with a “nonterritorial” office: no one had a desk or any office equipment of his own. It was a scheme that courted failure by neglecting all the ways in which an office is a sort of neighborhood. By contrast, the new office is an almost perfect embodiment of Jacobsian principles of community. The agency is in a huge old warehouse, three stories high and the size of three football fields. It is informally known as Advertising City, and that’s what it is: a kind of artfully constructed urban neighborhood. The floor is bisected by a central corridor called Main Street, and in the center of the room is an open space, with café tables and a stand of ficus trees, called Central Park. There’s a basketball court, a game room, and a bar. Most of the employees are in snug workstations known as nests, and the nests are grouped together in neighborhoods that radiate from Main Street like Paris arrondissements. The top executives are situated in the middle of the room. The desk belonging to the chairman and creative director of the company looks out on Central Park. The offices of the chief financial officer and the media director abut the basketball court. Sprinkled throughout the building are meeting rooms and project areas and plenty of nooks where employees can closet themselves when they need to. A small part of the building is elevated above the main floor on a mezzanine, and if you stand there and watch the people wander about with their portable phones, and sit and chat in Central Park, and play basketball in the gym, and you feel on your shoulders the sun from the skylights and listen to the gentle buzz of human activity, it is quite possible to forget that you are looking at an office.

4.

In “The Death and Life of Great American Cities,” Jacobs wrote of the importance of what she called “public characters”–people who have the social position and skills to orchestrate the movement of information and the creation of bonds of trust:

A public character is anyone who is in frequent contact with a wide circle of people and who is sufficiently interested to make himself a public character….The director of a settlement on New York’s Lower East Side, as an example, makes a regular round of stores. He learns from the cleaner who does his suits about the presence of dope pushers in the neighborhood. He learns from the grocer that the Dragons are working up to something and need attention. He learns from the candy store that two girls are agitating the Sportsmen toward a rumble. One of his most important information spots is an unused breadbox on Rivington Street…. A message spoken there for any teen-ager within many blocks will reach his ears unerringly and surprisingly quickly, and the opposite flow along the grapevine similarly brings news quickly in to the breadbox.

A vital community, in Jacobs’s view, required more than the appropriate physical environment. It also required a certain kind of person, who could bind together the varied elements of street life. Offices are no different. In fact, as office designers have attempted to create more vital workplaces, they have become increasingly interested in identifying and encouraging public characters.

One of the pioneers in this way of analyzing offices is Karen Stephenson, a business-school professor and anthropologist who runs a New York-based consulting company called Netform. Stephenson studies social networks. She goes into a company–her clients include J.P. Morgan, the Los Angeles Police Department, T.R.W., and I.B.M.–and distributes a questionnaire to its employees, asking about which people they have contact with. Whom do you like to spend time with? Whom do you talk to about new ideas? Where do you go to get expert advice? Every name in the company becomes a dot on a graph, and Stephenson draws lines between all those who have regular contact with each other. Stephenson likens her graphs to X-rays, and her role to that of a radiologist. What she’s depicting is the firm’s invisible inner mechanisms, the relationships and networks and patterns of trust that arise as people work together over time, and that are hidden beneath the organization chart. Once, for example, Stephenson was doing an “X-ray” of a Head Start organization. The agency was mostly female, and when Stephenson analyzed her networks she found that new hires and male staffers were profoundly isolated, communicating with the rest of the organization through only a handful of women. “I looked at tenure in the organization, office ties, demographic data. I couldn’t see what tied the women together, and why the men were talking only to these women,” Stephenson recalls. “Nor could the president of the organization. She gave me a couple of ideas. She said, `Sorry I can’t figure it out.’ Finally, she asked me to read the names again, and I could hear her stop, and she said, `My God, I know what it is. All those women are smokers.’” The X- ray revealed that the men–locked out of the formal power structure of the organization–were trying to gain access and influence by hanging out in the smoking area with some of the more senior women.

What Stephenson’s X-rays do best, though, is tell you who the public characters are. In every network, there are always one or two people who have connections to many more people than anyone else. Stephenson calls them “hubs,” and on her charts lines radiate out from them like spokes on a wheel. (Bernie the candy-store owner, on Jacobs’s Hudson Street, was a hub.) A few people are also what Stephenson calls “gatekeepers”: they control access to critical people, and link together a strategic few disparate groups. Finally, if you analyze the graphs there are always people who seem to have lots of indirect links to other people–who are part of all sorts of networks without necessarily being in the center of them. Stephenson calls those people “pulsetakers.” (In Silicon Valleyspeak, the person in a sea of cubicles who pops his or her head up over the partition every time something interesting is going on is called a prairie dog: prairie dogs are pulsetakers.)

5.

In the past year, Stephenson has embarked on a partnership with Steelcase, the world’s largest manufacturer of office furniture, in order to use her techniques in the design of offices. Traditionally, office designers would tell a company what furniture should go where. Stephenson and her partners at Steelcase propose to tell a company what people should go where, too. At Steelcase, they call this “floor-casting.”

One of the first projects for the group is the executive level at Steelcase’s headquarters, a five-story building in Grand Rapids, Michigan. The executive level, on the fourth floor, is a large, open room filled with small workstations. (Jim Hackett, the head of the company, occupies what Steelcase calls a Personal Harbor, a black, freestanding metal module that may be–at seven feet by eight–the smallest office of a Fortune 500 C.E.O.) One afternoon recently, Stephenson pulled out a laptop and demonstrated how she had mapped the communication networks of the leadership group onto a seating chart of the fourth floor. The dots and swirls are strangely compelling–abstract representations of something real and immediate. One executive, close to Hackett, was inundated with lines from every direction. “He’s a hub, a gatekeeper, and a pulsetaker across all sorts of different dimensions,” Stephenson said. “What that tells you is that he is very strategic. If there is no succession planning around that person, you have got a huge risk to the knowledge base of this company. If he’s in a plane accident, there goes your knowledge.” She pointed to another part of the floor plan, with its own thick overlay of lines. “That’s sales and marketing. They have a pocket of real innovation here. The guy who runs it is very good, very smart.” But then she pointed to the lines connecting that department with other departments. “They’re all coming into this one place,” she said, and she showed how all the lines coming out of marketing converged on one senior executive. “There’s very little path redundancy. In human systems, you need redundancy, you need communication across multiple paths.” What concerned Stephenson wasn’t just the lack of redundancy but the fact that, in her lingo, many of the paths were “unconfirmed”: they went only one way. People in marketing were saying that they communicated with the senior management, but there weren’t as many lines going in the other direction. The sales-and-marketing team, she explained, had somehow become isolated from senior management. They couldn’t get their voices heard when it came to innovation–and that fact, she said, ought to be a big consideration when it comes time to redo the office. “If you ask the guy who heads sales and marketing who he wants to sit next to, he’ll pick out all the people he trusts,” she said. “But do you sit him with those people? No. What you want to do is put people who don’t trust each other near each other. Not necessarily next to each other, because they get too close. But close enough so that when you pop your head up, you get to see people, they are in your path, and all of a sudden you build an inviting space where they can hang out, kitchens and things like that. Maybe they need to take a hub in an innovation network and place the person with a pulsetaker in an expert network–to get that knowledge indirectly communicated to a lot of people.”

The work of translating Stephenson’s insights onto a new floor plan is being done in a small conference room–a war room–on the second floor of Steelcase headquarters. The group consists of a few key people from different parts of the firm, such as human resources, design, technology, and space-planning research. The walls of the room are cluttered with diagrams and pictures and calculations and huge, blownup versions of Stephenson’s X-rays. Team members stress that what they are doing is experimental. They don’t know yet how directly they want to translate findings from the communications networks to office plans. After all, you don’t want to have to redo the entire office every time someone leaves or joins the company. But it’s clear that there are some very simple principles from the study of public characters which ought to drive the design process. “You want to place hubs at the center,” Joyce Bromberg, the director of space planning, says. “These are the ones other people go to in order to get information. Give them an environment that allows access. But there are also going to be times that they need to have control–so give them a place where they can get away. Gatekeepers represent the fit between groups. They transmit ideas. They are brokers, so you might want to put them at the perimeter, and give them front porches”–areas adjoining the workspace where you might put little tables and chairs. “Maybe they could have swinging doors with white boards, to better transmit information. As for pulsetakers, they are the roamers. Rather than give them one fixed work location, you might give them a series of touchdown spots–where you want them to stop and talk. You want to enable their meandering.”

One of the other team members was a tall, thoughtful man named Frank Graziano. He had a series of pencil drawings–with circles representing workstations of all the people whose minds, as he put it, he wanted to make “explicit.” He said that he had done the plan the night before. “I think we can thread innovation through the floor,” he went on, and with a pen drew a red line that wound its way through the maze of desks. It was his Hudson Street.

6.

“The Death and Life of Great American Cities” was a controversial book, largely because there was always a whiff of paternalism in Jacobs’s vision of what city life ought to be. Chelsea–the neighborhood directly to the north of her beloved West Village–had “mixtures and types of buildings and densities of dwelling units per acre… almost identical with those of Greenwich Village,” she noted. But its long-predicted renaissance would never happen, she maintained, because of the “barriers of long, self-isolating blocks.” She hated Chatham Village, a planned “garden city” development in Pittsburgh. It was a picturesque green enclave, but it suffered, in Jacobs’s analysis, from a lack of sidewalk life. She wasn’t concerned that some people might not want an active street life in their neighborhood; that what she saw as the “self-isolating blocks” of Chelsea others would see as a welcome respite from the bustle of the city, or that Chatham Village would appeal to some people precisely because one did not encounter on its sidewalks a “solid wave of conversation and animation.” Jacobs felt that city dwellers belonged in environments like the West Village, whether they realized it or not.

The new workplace designers are making the same calculation, of course. The point of the new offices is to compel us to behave and socialize in ways that we otherwise would not–to overcome our initial inclination to be office suburbanites. But, in all the studies of the new workplaces, the reservations that employees have about a more social environment tend to diminish once they try it. Human behavior, after all, is shaped by context, but how it is shaped–and whether we’ll be happy with the result–we can understand only with experience. Jane Jacobs knew the virtues of the West Village because she lived there. What she couldn’t know was that her ideas about community would ultimately make more sense in the workplace. From time to time, social critics have bemoaned the falling rates of community participation in American life, but they have made the same mistake. The reason Americans are content to bowl alone (or, for that matter, not bowl at all) is that, increasingly, they receive all the social support they need–all the serendipitous interactions that serve to make them happy and productive–from nine to five.

© 2013 Malcolm Gladwell.

DAVID AND GOLIATH
 
WHAT THE DOG SAW
 
OUTLIERS
 
BLINK
 
THE TIPPING POINT
 
ARTICLES
 
ETC.
The Science of Shopping

Posted November 4, 1996 by MALCOLM GLADWELL & filed under REPORTER AT LARGE, THE NEW YORKER - ARCHIVE.

The American shopper has never been so fickle. What are stores, including the new flagship designer boutiques, doing about it? Applying science.

1.

Human beings walk the way they drive, which is to say that Americans tend to keep to the right when they stroll down shopping-mall concourses or city sidewalks. This is why in a well-designed airport travellers drifting toward their gate will always find the fast-food restaurants on their left and the gift shops on their right: people will readily cross a lane of pedestrian traffic to satisfy their hunger but rarely to make an impulse buy of a T-shirt or a magazine. This is also why Paco Underhill tells his retail clients to make sure that their window displays are canted, preferably to both sides but especially to the left, so that a potential shopper approaching the store on the inside of the sidewalk-the shopper, that is, with the least impeded view of the store window-can see the display from at least twenty-five feet away.

Of course, a lot depends on how fast the potential shopper is walking. Paco, in his previous life, as an urban geographer in Manhattan, spent a great deal of time thinking about walking speeds as he listened in on the great debates of the nineteen-seventies over whether the traffic lights in midtown should be timed to facilitate the movement of cars or to facilitate the movement of pedestrians and so break up the big platoons that move down Manhattan sidewalks. He knows that the faster you walk the more your peripheral vision narrows, so you become unable to pick up visual cues as quickly as someone who is just ambling along. He knows, too, that people who walk fast take a surprising amount of time to slow down-just as it takes a good stretch of road to change gears with a stick-shift automobile. On the basis of his research, Paco estimates the human downshift period to be anywhere from twelve to twenty-five feet, so if you own a store, he says, you never want to be next door to a bank: potential shoppers speed up when they walk past a bank (since there’s nothing to look at), and by the time they slow down they’ve walked right past your business. The downshift factor also means that when potential shoppers enter a store it’s going to take them from five to fifteen paces to adjust to the light and refocus and gear down from walking speed to shopping speed-particularly if they’ve just had to navigate a treacherous parking lot or hurry to make the light at Fifty- seventh and Fifth. Paco calls that area inside the door the Decompression Zone, and something he tells clients over and over again is never, ever put anything of value in that zone- not shopping baskets or tie racks or big promotional displays- because no one is going to see it. Paco believes that, as a rule of thumb, customer interaction with any product or promotional display in the Decompression Zone will increase at least thirty per cent once it’s moved to the back edge of the zone, and even more if it’s placed to the right, because another of the fundamental rules of how human beings shop is that upon entering a store-whether it’s Nordstrom or K mart, Tiffany or the Gap-the shopper invariably and reflexively turns to the right. Paco believes in the existence of the Invariant Right because he has actually verified it. He has put cameras in stores trained directly on the doorway, and if you go to his office, just above Union Square, where videocassettes and boxes of Super-eight film from all his work over the years are stacked in plastic Tupperware containers practically up to the ceiling, he can show you reel upon reel of grainy entryway video-customers striding in the door, downshifting, refocussing, and then, again and again, making that little half turn.

Paco Underhill is a tall man in his mid-forties, partly bald, with a neatly trimmed beard and an engaging, almost goofy manner. He wears baggy khakis and shirts open at the collar, and generally looks like the academic he might have been if he hadn’t been captivated, twenty years ago, by the ideas of the urban anthropologist William Whyte. It was Whyte who pioneered the use of time-lapse photography as a tool of urban planning, putting cameras in parks and the plazas in front of office buildings in midtown Manhattan, in order to determine what distinguished a public space that worked from one that didn’t. As a Columbia undergraduate, in 1974, Paco heard a lecture on Whyte’s work and, he recalls, left the room “walking on air.” He immediately read everything Whyte had written. He emptied his bank account to buy cameras and film and make his own home movie, about a pedestrian mall in Poughkeepsie. He took his “little exercise” to Whyte’s advocacy group, the Project for Public Spaces, and was offered a job. Soon, however, it dawned on Paco that Whyte’s ideas could be taken a step further-that the same techniques he used to establish why a plaza worked or didn’t work could also be used to determine why a store worked or didn’t work. Thus was born the field of retail anthropology, and, not long afterward, Paco founded Envirosell, which in just over fifteen years has counselled some of the most familiar names in American retailing, from Levi Strauss to Kinney, Starbucks, McDonald’s, Blockbuster, Apple Computer, A.T. & T., and a number of upscale retailers that Paco would rather not name. When Paco gets an assignment, he and his staff set up a series of video cameras throughout the test store and then back the cameras up with Envirosell staffers-trackers, as they’re known-armed with clipboards. Where the cameras go and how many trackers Paco deploys depends on exactly what the store wants to know about its shoppers. Typically, though, he might use six cameras and two or three trackers, and let the study run for two or three days, so that at the end he would have pages and pages of carefully annotated tracking sheets and anywhere from a hundred to five hundred hours of film. These days, given the expansion of his business, he might tape fifteen thousand hours in a year, and, given that he has been in operation since the late seventies, he now has well over a hundred thousand hours of tape in his library. Even in the best of times, this would be a valuable archive. But today, with the retail business in crisis, it is a gold mine. The time per visit that the average American spends in a shopping mall was sixty-six minutes last year-down from seventy-two minutes in 1992-and is the lowest number ever recorded. The amount of selling space per American shopper is now more than double what it was in the mid-seventies, meaning that profit margins have never been narrower, and the costs of starting a retail business-and of failing-have never been higher. In the past few years, countless dazzling new retailing temples have been built along Fifth and Madison Avenues- Barneys, Calvin Klein, Armani, Valentino, Banana Republic, Prada, Chanel, Nike Town, and on and on-but it is an explosion of growth based on no more than a hunch, a hopeful multimillion-dollar gamble that the way to break through is to provide the shopper with spectacle and more spectacle. “The arrogance is gone,” Millard Drexler, the president and CEO of the Gap, told me. “Arrogance makes failure. Once you think you know the answer, it’s almost always over.” In such a competitive environment, retailers don’t just want to know how shoppers behave in their stores. They have to know. And who better to ask than Paco Underhill, who in the past decade and a half has analyzed tens of thousands of hours of shopping videotape and, as a result, probably knows more about the strange habits and quirks of the species Emptor americanus than anyone else alive?

2.

Paco is considered the originator, for example, of what is known in the trade as the butt-brush theory-or, as Paco calls it, more delicately, le facteur bousculade-which holds that the likelihood of a woman’s being converted from a browser to a buyer is inversely proportional to the likelihood of her being brushed on her behind while she’s examining merchandise. Touch-or brush or bump or jostle-a woman on the behind when she has stopped to look at an item, and she will bolt. Actually, calling this a theory is something of a misnomer, because Paco doesn’t offer any explanation for why women react that way, aside from venturing that they are “more sensitive back there.” It’s really an observation, based on repeated and close analysis of his videotape library, that Paco has transformed into a retailing commandment: a women’s product that requires extensive examination should never be placed in a narrow aisle.

Paco approaches the problem of the Invariant Right the same way. Some retail thinkers see this as a subject crying out for interpretation and speculation. The design guru Joseph Weishar, for example, argues, in his magisterial “Design for Effective Selling Space,” that the Invariant Right is a function of the fact that we “absorb and digest information in the left part of the brain” and “assimilate and logically use this information in the right half,” the result being that we scan the store from left to right and then fix on an object to the right “essentially at a 45 degree angle from the point that we enter.” When I asked Paco about this interpretation, he shrugged, and said he thought the reason was simply that most people are right-handed. Uncovering the fundamentals of “why” is clearly not a pursuit that engages him much. He is not a theoretician but an empiricist, and for him the important thing is that in amassing his huge library of in- store time-lapse photography he has gained enough hard evidence to know how often and under what circumstances the Invariant Right is expressed and how to take advantage of it.

What Paco likes are facts. They come tumbling out when he talks, and, because he speaks with a slight hesitation-lingering over the first syllable in, for example, “re-tail” or “de-sign”-he draws you in, and you find yourself truly hanging on his words. “We have reached a historic point in American history,” he told me in our very first conversation. “Men, for the first time, have begun to buy their own underwear.” He then paused to let the comment sink in, so that I could absorb its implications, before he elaborated: “Which means that we have to totally rethink the way we sell that product.” In the parlance of Hollywood scriptwriters, the best endings must be surprising and yet inevitable; and the best of Paco’s pronouncements take the same shape. It would never have occurred to me to wonder about the increasingly critical role played by touching-or, as Paco calls it, petting- clothes in the course of making the decision to buy them. But then I went to the Gap and to Banana Republic and saw people touching and fondling and, one after another, buying shirts and sweaters laid out on big wooden tables, and what Paco told me-which was no doubt based on what he had seen on his videotapes-made perfect sense: that the reason the Gap and Banana Republic have tables is not merely that sweaters and shirts look better there, or that tables fit into the warm and relaxing residential feeling that the Gap and Banana Republic are trying to create in their stores, but that tables invite-indeed, symbolize-touching. “Where do we eat?” Paco asks. “We eat, we pick up food, on tables.”

Paco produces for his clients a series of carefully detailed studies, totalling forty to a hundred and fifty pages, filled with product-by-product breakdowns and bright-colored charts and graphs. In one recent case, he was asked by a major clothing retailer to analyze the first of a new chain of stores that the firm planned to open. One of the things the client wanted to know was how successful the store was in drawing people into its depths, since the chances that shoppers will buy something are directly related to how long they spend shopping, and how long they spend shopping is directly related to how deep they get pulled into the store. For this reason, a supermarket will often put dairy products on one side, meat at the back, and fresh produce on the other side, so that the typical shopper can’t just do a drive-by but has to make an entire circuit of the store, and be tempted by everything the supermarket has to offer. In the case of the new clothing store, Paco found that ninety-one per cent of all shoppers penetrated as deep as what he called Zone 4, meaning more than three-quarters of the way in, well past the accessories and shirt racks and belts in the front, and little short of the far wall, with the changing rooms and the pants stacked on shelves. Paco regarded this as an extraordinary figure, particularly for a long, narrow store like this one, where it is not unusual for the rate of penetration past, say, Zone 3 to be under fifty per cent. But that didn’t mean the store was perfect-far from it. For Paco, all kinds of questions remained.

Purchasers, for example, spent an average of eleven minutes and twenty-seven seconds in the store, nonpurchasers two minutes and thirty-six seconds. It wasn’t that the nonpurchasers just cruised in and out: in those two minutes and thirty-six seconds, they went deep into the store and examined an average of 3.42 items. So why didn’t they buy? What, exactly, happened to cause some browsers to buy and other browsers to walk out the door?

Then, there was the issue of the number of products examined. The purchasers were looking at an average of 4.81 items but buying only 1.33 items. Paco found this statistic deeply disturbing. As the retail market grows more cutthroat, store owners have come to realize that it’s all but impossible to increase the number of customers coming in, and have concentrated instead on getting the customers they do have to buy more. Paco thinks that if you can sell someone a pair of pants you must also be able to sell that person a belt, or a pair of socks, or a pair of underpants, or even do what the Gap does so well: sell a person a complete outfit. To Paco, the figure 1.33 suggested that the store was doing something very wrong, and one day when I visited him in his office he sat me down in front of one of his many VCRs to see how he looked for the 1.33 culprit.

It should be said that sitting next to Paco is a rather strange experience. “My mother says that I’m the best-paid spy in America,” he told me. He laughed, but he wasn’t entirely joking. As a child, Paco had a nearly debilitating stammer, and, he says, “since I was never that comfortable talking I always relied on my eyes to understand things.” That much is obvious from the first moment you meet him: Paco is one of those people who look right at you, soaking up every nuance and detail. It isn’t a hostile gaze, because Paco isn’t hostile at all. He has a big smile, and he’ll call you “chief” and use your first name a lot and generally act as if he knew you well. But that’s the awkward thing: he has looked at you so closely that you’re sure he does know you well, and you, meanwhile, hardly know him at all. This kind of asymmetry is even more pronounced when you watch his shopping videos with him, because every movement or gesture means something to Paco-he has spent his adult life deconstructing the shopping experience-but nothing to the outsider, or, at least, not at first. Paco had to keep stopping the video to get me to see things through his eyes before I began to understand. In one sequence, for example, a camera mounted high on the wall outside the changing rooms documented a man and a woman shopping for a pair of pants for what appeared to be their daughter, a girl in her mid-teens. The tapes are soundless, but the basic steps of the shopping dance are so familiar to Paco that, once I’d grasped the general idea, he was able to provide a running commentary on what was being said and thought. There is the girl emerging from the changing room wearing her first pair. There she is glancing at her reflection in the mirror, then turning to see herself from the back. There is the mother looking on. There is the father-or, as fathers are known in the trade, the “wallet carrier”-stepping forward and pulling up the jeans. There’s the girl trying on another pair. There’s the primp again. The twirl. The mother. The wallet carrier. And then again, with another pair. The full sequence lasted twenty minutes, and at the end came the take-home lesson, for which Paco called in one of his colleagues, Tom Moseman, who had supervised the project. “This is a very critical moment,” Tom, a young, intense man wearing little round glasses, said, and he pulled up a chair next to mine. “She’s saying, ‘I don’t know whether I should wear a belt.’ Now here’s the salesclerk. The girl says to him, ‘I need a belt,’ and he says, ‘Take mine.’ Now there he is taking her back to the full-length mirror.” A moment later, the girl returns, clearly happy with the purchase. She wants the jeans. The wallet carrier turns to her, and then gestures to the salesclerk. The wallet carrier is telling his daughter to give back the belt. The girl gives back the belt. Tom stops the tape. He’s leaning forward now, a finger jabbing at the screen. Beside me, Paco is shaking his head. I don’t get it-at least, not at first-and so Tom replays that last segment. The wallet carrier tells the girl to give back the belt. She gives back the belt. And then, finally, it dawns on me why this store has an average purchase number of only 1.33. “Don’t you see?” Tom said. “She wanted the belt. A great opportunity to make an add-on sale . . . lost!”

3.

Should we be afraid of Paco Underhill? One of the fundamental anxieties of the American consumer, after all, has always been that beneath the pleasure and the frivolity of the shopping experience runs an undercurrent of manipulation, and that anxiety has rarely seemed more justified than today. The practice of prying into the minds and habits of American consumers is now a multibillion-dollar business. Every time a product is pulled across a supermarket checkout scanner, information is recorded, assembled, and sold to a market-research firm for analysis. There are companies that put tiny cameras inside frozen-food cases in supermarket aisles; market-research firms that feed census data and behavioral statistics into algorithms and come out with complicated maps of the American consumer; anthropologists who sift through the garbage of carefully targeted households to analyze their true consumption patterns; and endless rounds of highly organized focus groups and questionnaire takers and phone surveyors. That some people are now tracking our every shopping move with video cameras seems in many respects the last straw: Paco’s movies are, after all, creepy. They look like the surveillance videos taken during convenience-store holdups-hazy and soundless and slightly warped by the angle of the lens. When you watch them, you find yourself waiting for something bad to happen, for someone to shoplift or pull a gun on a cashier.

The more time you spend with Paco’s videos, though, the less scary they seem. After an hour or so, it’s no longer clear whether simply by watching people shop-and analyzing their every move-you can learn how to control them. The shopper that emerges from the videos is not pliable or manipulable. The screen shows people filtering in and out of stores, petting and moving on, abandoning their merchandise because checkout lines are too long, or leaving a store empty-handed because they couldn’t fit their stroller into the aisle between two shirt racks. Paco’s shoppers are fickle and headstrong, and are quite unwilling to buy anything unless conditions are perfect-unless the belt is presented at exactly the right moment. His theories of the butt-brush and petting and the Decompression Zone and the Invariant Right seek not to make shoppers conform to the desires of sellers but to make sellers conform to the desires of shoppers. What Paco is teaching his clients is a kind of slavish devotion to the shopper’s every whim. He is teaching them humility. Paco has worked with supermarket chains, and when you first see one of his videos of grocery aisles it looks as if he really had- at least in this instance-got one up on the shopper. The clip he showed me was of a father shopping with a small child, and it was an example of what is known in the trade as “advocacy,” which basically means what happens when your four-year-old goes over and grabs a bag of cookies that the store has conveniently put on the bottom shelf, and demands that it be purchased. In the clip, the father takes what the child offers him. “Generally, dads are not as good as moms at saying no,” Paco said as we watched the little boy approach his dad. “Men tend to be more impulse-driven than women in grocery stores. We know that they tend to shop less often with a list. We know that they tend to shop much less frequently with coupons, and we know, simply by watching them shop, that they can be marching down the aisle and something will catch their eye and they will stop and buy.” This kind of weakness on the part of fathers might seem to give the supermarket an advantage in the cookie-selling wars, particularly since more and more men go grocery shopping with their children. But then Paco let drop a hint about a study he’d just done in which he discovered, to his and everyone else’s amazement, that shoppers had already figured this out, that they were already one step ahead-that families were avoiding the cookie aisle. This may seem like a small point. But it begins to explain why, even though retailers seem to know more than ever about how shoppers behave, even though their efforts at intelligence-gathering have rarely seemed more intrusive and more formidable, the retail business remains in crisis. The reason is that shoppers are a moving target. They are becoming more and more complicated, and retailers need to know more and more about them simply to keep pace. This fall, for example, Estée Lauder is testing in a Toronto shopping mall a new concept in cosmetics retailing. Gone is the enclosed rectangular counter, with the sales staff on one side, customers on the other, and the product under glass in the middle. In its place the company has provided an assortment of product-display, consultation, and testing kiosks arranged in a broken circle, with a service desk and a cashier in the middle. One of the kiosks is a “makeup play area,” which allows customers to experiment on their own with a hundred and thirty different shades of lipstick. There are four self-service displays-for perfumes, skin-care products, and makeup-which are easily accessible to customers who have already made up their minds. And, for those who haven’t, there is a semiprivate booth for personal consultations with beauty advisers and makeup artists. The redesign was prompted by the realization that the modern working woman no longer had the time or the inclination to ask a salesclerk to assist her in every purchase, that choosing among shades of lipstick did not require the same level of service as, say, getting up to speed on new developments in skin care, that a shopper’s needs were now too diverse to be adequately served by just one kind of counter. “I was going from store to store, and the traffic just wasn’t there,” Robin Burns, the president and C.E.O. of Estée Lauder U.S.A. and Canada, told me. “We had to get rid of the glass barricade.” The most interesting thing about the new venture, though, is what it says about the shifting balance of power between buyer and seller. Around the old rectangular counter, the relationship of clerk to customer was formal and subtly paternalistic. If you wanted to look at a lipstick, you had to ask for it. “Twenty years ago, the sales staff would consult with you and tell you what you needed, as opposed to asking and recommending,” Burns said. “And in those days people believed what the salesperson told them.” Today, the old hierarchy has been inverted. “Women want to draw their own conclusions,” Burns said. Even the architecture of the consultation kiosk speaks to the transformation: the beauty adviser now sits beside the customer, not across from her.

4.

This doesn’t mean that marketers and retailers have stopped trying to figure out what goes on in the minds of shoppers. One of the hottest areas in market research, for example, is something called typing, which is a sophisticated attempt to predict the kinds of products that people will buy or the kind of promotional pitch they will be susceptible to on the basis of where they live or how they score on short standardized questionnaires. One market-research firm in Virginia, Claritas, has divided the entire country, neighborhood by neighborhood, into sixty-two different categories-Pools & Patios, Shotguns & Pickups, Bohemia Mix, and so on-using census data and results from behavioral surveys. On the basis of my address in Greenwich Village, Claritas classifies me as Urban Gold Coast, which means that I like Kellogg’s Special K, spend more than two hundred and fifty dollars on sports coats, watch “Seinfeld,” and buy metal polish. Such typing systems-and there are a number of them- can be scarily accurate. I actually do buy Kellogg’s Special K, have spent more than two hundred and fifty dollars on a sports coat, and watch “Seinfeld.” (I don’t buy metal polish.) In fact, when I was typed by a company called Total Research, in Princeton, the results were so dead-on that I got the same kind of creepy feeling that I got when I first watched Paco’s videos. On the basis of a seemingly innocuous multiple-choice test, I was scored as an eighty-nine-per-cent Intellect and a seven-per-cent Relief Seeker (which I thought was impressive until John Morton, who developed the system, told me that virtually everyone who reads The New Yorker is an Intellect). When I asked Morton to guess, on the basis of my score, what kind of razor I used, he riffed, brilliantly, and without a moment’s hesitation. “If you used an electric razor, it would be a Braun,” he began. “But, if not, you’re probably shaving with Gillette, if only because there really isn’t an Intellect safety-razor positioning out there. Schick and Bic are simply not logical choices for you, although I’m thinking, You’re fairly young, and you’ve got that Relief Seeker side. It’s possible you would use Bic because you don’t like that all- American, overly confident masculine statement of Gillette. It’s a very, very conventional positioning that Gillette uses. But then they’ve got the technological angle with the Gillette Sensor. . . . I’m thinking Gillette. It’s Gillette.”

He was right. I shave with Gillette-though I didn’t even know that I do. I had to go home and check. But information about my own predilections may be of limited usefulness in predicting how I shop. In the past few years, market researchers have paid growing attention to the role in the shopping experience of a type of consumer known as a Market Maven. “This is a person you would go to for advice on a car or a new fashion,” said Linda Price, a marketing professor at the University of South Florida, who first came up with the Market Maven concept, in the late eighties. “This is a person who has information on a lot of different products or prices or places to shop. This is a person who likes to initiate discussions with consumers and respond to requests. Market Mavens like to be helpers in the marketplace. They take you shopping. They go shopping for you, and it turns out they are a lot more prevalent than you would expect.” Mavens watch more television than almost anyone else does, and they read more magazines and open their junk mail and look closely at advertisements and have an awful lot of influence on everyone else. According to Price, sixty per cent of Americans claim to know a Maven.

The key question, then, is not what I think but what my Mavens think. The challenge for retailers and marketers, in turn, is not so much to figure out and influence my preferences as to figure out and influence the preferences of my Mavens, and that is a much harder task. “What’s really interesting is that the distribution of Mavens doesn’t vary by ethnic category, by income, or by professional status,” Price said. “A working woman is just as likely to be a Market Maven as a nonworking woman. You might say that Mavens are likely to be older, unemployed people, but that’s wrong, too. There is simply not a clear demographic guide to how to find these people.” More important, Mavens are better consumers than most of the rest of us. In another of the typing systems, developed by the California-based SRI International, Mavens are considered to be a subcategory of the consumer type known as Fulfilled, and Fulfilleds, one SRI official told me, are “the consumers from Hell-they are very feature oriented.” He explained, “They are not pushed by promotions. You can reach them, but it’s an intellectual argument.” As the complexity of the marketplace grows, in other words, we have responded by appointing the most skeptical and the most savvy in our midst to mediate between us and sellers. The harder stores and manufacturers work to sharpen and refine their marketing strategies, and the harder they try to read the minds of shoppers, the more we hide behind Mavens.

5.

Imagine that you want to open a clothing store, men’s and women’s, in the upper-middle range-say, khakis at fifty dollars, dress shirts at forty dollars, sports coats and women’s suits at two hundred dollars and up. The work of Paco Underhill would suggest that in order to succeed you need to pay complete and concentrated attention to the whims of your customers. What does that mean, in practical terms? Well, let’s start with what’s called the shopping gender gap. In the retail-store study that Paco showed me, for example, male buyers stayed an average of nine minutes and thirty-nine seconds in the store and female buyers stayed twelve minutes and fifty-seven seconds. This is not atypical. Women always shop longer than men, which is one of the major reasons that in the standard regional mall women account for seventy per cent of the dollar value of all purchases. “Women have more patience than men,” Paco says. “Men are more distractible. Their tolerance level for confusion or time spent in a store is much shorter than women’s.” If you wanted, then, you could build a store designed for men, to try to raise that thirty-per-cent sales figure to forty or forty-five per cent. You could make the look more masculine-more metal, darker woods. You could turn up the music. You could simplify the store, put less product on the floor. “I’d go narrow and deep,” says James Adams, the design director for NBBJ Retail Concepts, a division of one of the country’s largest retail- design firms. “You wouldn’t have fifty different cuts of pants. You’d have your four basics with lots of color. You know the Garanimals they used to do to help kids pick out clothes, where you match the giraffe top with the giraffe bottom? I’m sure every guy is like ‘I wish I could get those, too.’ You’d want to stick with the basics. Making sure most of the color story goes together. That is a big deal with guys, because they are always screwing the colors up.” When I asked Carrie Gennuso, the Gap’s regional vice-president for New York, what she would do in an all-male store, she laughed and said, “I might do fewer displays and more signage. Big signs. Men! Smalls! Here!” As a rule, though, you wouldn’t want to cater to male customers at the expense of female ones. It’s no accident that many clothing stores have a single look in both men’s and women’s sections, and that the quintessential nineties look-light woods, white walls-is more feminine than masculine. Women are still the shoppers in America, and the real money is to be made by making retailing styles more female-friendly, not less. Recently, for example, NBBJ did a project to try to increase sales of the Armstrong flooring chain. Its researchers found that the sales staff was selling the flooring based on its functional virtues-the fact that it didn’t scuff, that it was long-lasting, that it didn’t stain, that it was easy to clean. It was being sold by men to men, as if it were a car or a stereo. And that was the problem. “It’s a wonder product technologically,” Adams says. “But the woman is the decision-maker on flooring, and that’s not what’s she’s looking for. This product is about fashion, about color and design. You don’t want to get too caught up in the man’s way of thinking.”

To appeal to men, then, retailers do subtler things. At the Banana Republic store on Fifth Avenue in midtown, the men’s socks are displayed near the shoes and between men’s pants and the cash register (or cash/wrap, as it is known in the trade), so that the man can grab them easily as he rushes to pay. Women’s accessories are by the fitting rooms, because women are much more likely to try on pants first, and then choose an item like a belt or a bag. At the men’s shirt table, the display shirts have matching ties on them-the tie table is next to it-in a grownup version of the Garanimals system. But Banana Republic would never match scarves with women’s blouses or jackets. “You don’t have to be that direct with women,” Jeanne Jackson, the president of Banana Republic, told me. “In fact, the Banana woman is proud of her sense of style. She puts her own looks together.” Jackson said she liked the Fifth Avenue store because it’s on two floors, so she can separate men’s and women’s sections and give men what she calls “clarity of offer,” which is the peace of mind that they won’t inadvertently end up in, say, women’s undergarments. In a one-floor store, most retailers would rather put the menswear up front and the women’s wear at the back (that is, if they weren’t going to split the sexes left and right), because women don’t get spooked navigating through apparel of the opposite sex, whereas men most assuredly do. (Of course, in a store like the Gap at Thirty- ninth and Fifth, where, Carrie Gennuso says, “I don’t know if I’ve ever seen a man,” the issue is moot. There, it’s safe to put the women’s wear out front.)

The next thing retailers want to do is to encourage the shopper to walk deep into the store. The trick there is to put “destination items”-basics, staples, things that people know you have and buy a lot of-at the rear of the store. Gap stores, invariably, will have denim, which is a classic destination item for them, on the back wall. Many clothing stores also situate the cash/wrap and the fitting rooms in the rear of the store, to compel shoppers to walk back into Zone 3 or 4. In the store’s prime real estate-which, given Paco’s theory of the Decompression Zone and the Invariant Right, is to the right of the front entrance and five to fifteen paces in-you always put your hottest and newest merchandise, because that’s where the maximum number of people will see it. Right now, in virtually every Gap in the country, the front of the store is devoted to the Gap fall look-casual combinations in black and gray, plaid shirts and jackets, sweaters, black wool and brushed-twill pants. At the Gap at Fifth Avenue and Seventeenth Street, for example, there is a fall ensemble of plaid jacket, plaid shirt, and black pants in the first prime spot, followed, three paces later, by an ensemble of gray sweater, plaid shirt, T-shirt, and black pants, followed, three paces after that, by an ensemble of plaid jacket, gray sweater, white T-shirt, and black pants. In all, three variations on the same theme, each placed so that the eye bounces naturally from the first to the second to the third, and then, inexorably, to a table deep inside Zone 1 where merchandise is arrayed and folded for petting. Every week or ten days, the combinations will change, the “look” highlighted at the front will be different, and the entryway will be transformed.

Through all of this, the store environment-the lighting, the colors, the fixtures-and the clothes have to work together. The point is not so much beauty as coherence. The clothes have to match the environment. “In the nineteen-seventies, you didn’t have to have a complete wardrobe all the time,” Gabriella Forte, the president and chief operating officer of Calvin Klein, says. “I think now the store has to have a complete point of view. It has to have all the options offered, so people have choices. It’s the famous one-stop shopping. People want to come in, be serviced, and go out. They want to understand the clear statement the designer is making.”

At the new Versace store on Fifth Avenue, in the restored neoclassical Vanderbilt mansion, Gianni Versace says that the “statement” he is making with the elaborate mosaic and parquet floors, the marble façade and the Corinthian columns is “quality-my message is always a scream for quality.” At her two new stores in London, Donna Karan told me, she never wants “customers to think that they are walking into a clothing store.” She said, “I want them to think that they are walking into an environment, that I am transforming them out of their lives and into an experience, that it’s not about clothes, it’s about who they are as people.” The first thing the shopper sees in her stark, all-white DKNY store is a video monitor and café: “It’s about energy,” Karan said, “and nourishment.” In her more sophisticated, “collection” store, where the walls are black and ivory and gold, the first thing that the customer notices is the scent of a candle: “I wanted a nurturing environment where you feel that you will be taken care of.” And why, at a Giorgio Armani store, is there often only a single suit in each style on display? Not because the store has only the one suit in stock but because the way the merchandise is displayed has to be consistent with the message of the designers: that Armani suits are exclusive, that the Armani customer isn’t going to run into another man wearing his suit every time he goes to an art opening at Gagosian.

The best stores all have an image-or what retailers like to call a “point of view.” The flagship store for Ralph Lauren’s Polo collection, for example, is in the restored Rhinelander mansion, on Madison Avenue and Seventy-second Street. The Polo Mansion, as it is known, is alive with color and artifacts that suggest a notional prewar English gentility. There are fireplaces and comfortable leather chairs and deep-red Oriental carpets and soft, thick drapes and vintage photographs and paintings of country squires and a color palette of warm crimsons and browns and greens-to the point that after you’ve picked out a double-breasted blazer or a cashmere sweater set or an antique silver snuffbox you feel as though you ought to venture over to Central Park for a vigorous morning of foxhunting. The Calvin Klein flagship store, twelve blocks down Madison Avenue, on the other hand, is a vast, achingly beautiful minimalist temple, with white walls, muted lighting, soaring ceilings, gray stone flooring, and, so it seems, less merchandise in the entire store than Lauren puts in a single room. The store’s architect, John Pawson, says, “People who enter are given a sense of release. They are getting away from the hustle and bustle of the street and New York. They are in a calm space. It’s a modern idea of luxury, to give people space.”

The first thing you see when you enter the Polo Mansion is a display of two hundred and eight sweaters, in twenty- eight colors, stacked in a haberdasher’s wooden fixture, behind an antique glass counter; the first thing you see at the Klein store is a white wall, and then, if you turn to the right, four clear-glass shelves, each adorned with three solitary- looking black handbags. The Polo Mansion is an English club. The Klein store, Pawson says, is the equivalent of an art gallery, a place where “neutral space and light make a work of art look the most potent.” When I visited the Polo Mansion, the stereo was playing Bobby Short. At Klein, the stereo was playing what sounded like Brian Eno. At the Polo Mansion, I was taken around by Charles Fagan, a vice-president at Polo Ralph Lauren. He wore pale-yellow socks, black loafers, tight jeans, a pale-purple polo shirt, blue old-school tie, and a brown plaid jacket-which sounds less attractive on paper than it was in reality. He looked, in a very Ralph Lauren way, fabulous. He was funny and engaging and bounded through the store, keeping up a constant patter (“This room is sort of sportswear, Telluride-y, vintage”), all the while laughing and hugging people and having his freshly cut red hair tousled by the sales assistants in each section. At the Calvin Klein store, the idea that the staff-tall, austere, sombre-suited-might laugh and hug and tousle each other’s hair is unthinkable. Lean over and whisper, perhaps. At the most, murmur discreetly into tiny black cellular phones. Visiting the Polo Mansion and the Calvin Klein flagship in quick succession is rather like seeing a “Howards End”-”The Seventh Seal” double feature.

Despite their differences, though, these stores are both about the same thing-communicating the point of view that shoppers are now thought to demand. At Polo, the “life style” message is so coherent and all-encompassing that the store never has the 1.33 items-per-purchase problem that Paco saw in the retailer he studied. “We have multiple purchases in excess-it’s the cap, it’s the tie, it’s the sweater, it’s the jacket, it’s the pants,” Fagan told me, plucking each item from its shelf and tossing it onto a tartan-covered bench seat. “People say, ‘I have to have the belt.’ It’s a life-style decision.”

As for the Klein store, it’s really concerned with setting the tone for the Calvin Klein clothes and products sold outside the store-including the designer’s phenomenally successful underwear line, the sales of which have grown nearly fivefold in the past two and a half years, making it one of the country’s dominant brands. Calvin Klein underwear is partly a design triumph: lowering the waistband just a tad in order to elongate, and flatter, the torso. But it is also a triumph of image-transforming, as Gabriella Forte says, a “commodity good into something desirable,” turning a forgotten necessity into fashion. In the case of women’s underwear, Bob Mazzoli, president of Calvin Klein Underwear, told me that the company “obsessed about the box being a perfect square, about the symmetry of it all, how it would feel in a woman’s hand.” He added, “When you look at the boxes they are little works of art.” And the underwear itself is without any of the usual busyness-without, in Mazzoli’s words, “the excessive detail” of most women’s undergarments. It’s a clean look, selling primarily in white, heather gray, and black. It’s a look, in other words, not unlike that of the Calvin Klein flagship store, and it exemplifies the brilliance of the merchandising of the Calvin Klein image: preposterous as it may seem, once you’ve seen the store and worn the underwear, it’s difficult not to make a connection between the two.

All this imagemaking seeks to put the shopping experience in a different context, to give it a story line. “I wish that the customers who come to my stores feel the same comfort they would entering a friend’s house-that is to say, that they feel at ease, without the impression of having to deal with the ‘sanctum sanctorum’ of a designer,” Giorgio Armani told me. Armani has a house. Donna Karan has a kitchen and a womb. Ralph Lauren has a men’s club. Calvin Klein has an art gallery. These are all very different points of view. What they have in common is that they have nothing to do with the actual act of shopping. (No one buys anything at a friend’s house or a men’s club.) Presumably, by engaging in this kind of misdirection designers aim to put us at ease, to create a kind of oasis. But perhaps they change the subject because they must, because they cannot offer an ultimate account of the shopping experience itself. After all, what do we really know, in the end, about why people buy? We know about the Invariant Right and the Decompression Zone. We know to put destination items at the back and fashion at the front, to treat male shoppers like small children, to respect the female derrière, and to put the socks between the cash/wrap and the men’s pants. But this is grammar; it’s not prose. It is enough. But it is not much.

6.

One of the best ways to understand the new humility in shopping theory is to go back to the work of William Whyte. Whyte put his cameras in parks and in the plazas in front of office buildings because he believed in the then radical notion that the design of public spaces had been turned inside out- that planners were thinking of their designs first and of people second, when they should have been thinking of people first and of design second. In his 1980 classic, “The Social Life of Small Urban Spaces,” for example, Whyte trained his cameras on a dozen or so of the public spaces and small parks around Manhattan, like the plaza in front of the General Motors Building, on Fifth Avenue, and the small park at 77 Water Street, downtown, and Paley Park, on Fifty-third Street, in order to determine why some, like the tiny Water Street park, averaged well over a hundred and fifty people during a typical sunny lunch hour and others, like the much bigger plaza at 280 Park Avenue, were almost empty. He concluded that all the things used by designers to attempt to lure people into their spaces made little or no difference. It wasn’t the size of the space, or its beauty, or the presence of waterfalls, or the amount of sun, or whether a park was a narrow strip along the sidewalk or a pleasing open space. What mattered, overwhelmingly, was that there were plenty of places to sit, that the space was in some way connected to the street, and-the mystical circularity-that it was already well frequented. “What attracts people most, it would appear, is other people,” Whyte noted:

If I labor the point, it is because many urban spaces still are being designed as though the opposite were true-as though what people liked best were the places they stay away from. People often do talk along such lines, and therefore their responses to questionnaires can be entirely misleading. How many people would say they like to sit in the middle of a crowd? Instead, they speak of “getting away from it all,” and use words like “escape,” “oasis,” “retreat.” What people do, however, reveals a different priority.

Whyte’s conclusions demystified the question of how to make public space work. Places to sit, streets to enjoy, and people to watch turned out to be the simple and powerful rules for park designers to follow, and these rules demolished the orthodoxies and theoretical principles of conventional urban design. But in a more important sense-and it is here that Whyte’s connection with Paco Underhill and retail anthropology and the stores that line Fifth and Madison is most striking-what Whyte did was to remystify the art of urban planning. He said, emphatically, that people could not be manipulated, that they would enter a public space only on their own terms, that the goal of observers like him was to find out what people wanted, not why they wanted it. Whyte, like Paco, was armed with all kinds of facts and observations about what it took to build a successful public space. He had strict views on how wide ledges had to be to lure passersby (at least thirty inches, or two backsides deep), and what the carrying capacity of prime outdoor sitting space is (total number of square feet divided by three). But, fundamentally, he was awed by the infinite complexity and the ultimate mystery of human behavior. He took people too seriously to think that he could control them. Here is Whyte, in “The Social Life of Small Urban Spaces,” analyzing hours of videotape and describing what he has observed about the way men stand in public. He’s talking about feet. He could just as easily be talking about shopping:

Foot movements . . . seem to be a silent language. Often, in a schmoozing group, no one will be saying anything. Men stand bound in amiable silence, surveying the passing scene. Then, slowly, rhythmically, one of the men rocks up and down; first on the ball of the foot, then back on the heel. He stops. Another man starts the same movement. Sometimes there are reciprocal gestures. One man makes a half turn to the right. Then, after a rhythmic interval, another responds with a half turn to the left. Some kind of communication seems to be taking place here, but I’ve never broken the code.

© 2013 Malcolm Gladwell.

DAVID AND GOLIATH
 
WHAT THE DOG SAW
 
OUTLIERS
 
BLINK
 
THE TIPPING POINT
 
ARTICLES
 
ETC.
The Formula

Posted October 16, 2006 by MALCOLM GLADWELL & filed under ANNALS OF ENTERTAINMENT, THE NEW YORKER - ARCHIVE.

What if you built a machine to predict hit movies?

1.

One sunny afternoon not long ago, Dick Copaken sat in a booth at Daniel, one of those hushed, exclusive restaurants on Manhattan’s Upper East Side where the waiters glide spectrally from table to table. He was wearing a starched button-down shirt and a blue blazer. Every strand of his thinning hair was in place, and he spoke calmly and slowly, his large pink Charlie Brown head bobbing along evenly as he did. Copaken spent many years as a partner at the white-shoe Washington, D.C., firm Covington & Burling, and he has a lawyer’s gravitas. One of his best friends calls him, admiringly, “relentless.” He likes to tell stories. Yet he is not, strictly, a storyteller, because storytellers are people who know when to leave things out, and Copaken never leaves anything out: each detail is adduced, considered, and laid on the table—and then adjusted and readjusted so that the corners of the new fact are flush with the corners of the fact that preceded it. This is especially true when Copaken is talking about things that he really cares about, such as questions of international law or his grandchildren or, most of all, the movies.

Dick Copaken loves the movies. His friend Richard Light, a statistician at Harvard, remembers summer vacations on Cape Cod with the Copakens, when Copaken would take his children and the Light children to the movies every day. “Fourteen nights out of fourteen,” Light said. “Dick would say at seven o’clock, ‘Hey, who’s up for the movies?’ And, all by himself, he would take the six kids to the movies. The kids had the time of their lives. And Dick would come back and give, with a completely straight face, a rigorous analysis of how each movie was put together, and the direction and the special effects and the animation.” This is a man who has seen two or three movies a week for the past fifty years, who has filed hundreds of plots and characters and scenes away in his mind, and at Daniel he was talking about a movie that touched him as much as any he’d ever seen.

“Nobody’s heard of it,” he said, and he clearly regarded this fact as a minor tragedy. “It’s called ‘Dear Frankie.’ I watched it on a Virgin Atlantic flight because it was the only movie they had that I hadn’t already seen. I had very low expectations. But I was blown away.” He began, in his lawyer-like manner, to lay out the plot. It takes place in Scotland. A woman has fled an abusive relationship with her infant son and is living in a port town. The boy, now nine, is deaf, and misses the father he has never known. His mother has told him that his father is a sailor on a ship that rarely comes to shore, and has suggested that he write his father letters. These she intercepts, and replies to, writing as if she were the father. One day, the boy finds out that what he thinks is his father’s ship is coming to shore. The mother has to find a man to stand in for the father. She does. The two fall in love. Unexpectedly, the real father reëmerges. He’s dying, and demands to see his son. The mother panics. Then the little boy reveals his secret: he knew about his mother’s ruse all along.

“I was in tears over this movie,” Copaken said. “You know, sometimes when you see a movie in the air you’re in such an out-of-body mood that things get exaggerated. So when I got home I sat down and saw it another time. I was bawling again, even though I knew what was coming.” Copaken shook his head, and then looked away. His cheeks were flushed. His voice was suddenly thick. There he was, a buttoned-down corporate lawyer, in a hushed restaurant where there is practically a sign on the wall forbidding displays of human emotion—and he was crying, a third time. “That absolutely hits me,” he said, his face still turned away. “He knew all along what the mother was doing.” He stopped to collect himself. “I can’t even retell the damn story without getting emotional.”

He tried to explain why he was crying. There was the little boy, first of all. He was just about the same age as Copaken’s grandson Jacob. So maybe that was part of it. Perhaps, as well, he was reacting to the idea of an absent parent. His own parents, Albert and Silvia, ran a modest community-law practice in Kansas City, and would shut down their office whenever Copaken or his brother had any kind of school activity or performance. In the Copaken world, it was an iron law that parents had to be present. He told a story about representing the Marshall Islands in negotiations with the U.S. government during the Cold War. A missile-testing range on the island was considered to be strategically critical. The case was enormously complex—involving something like fifty federal agencies and five countries—and, just as the negotiations were scheduled to begin, Copaken learned of a conflict: his eldest daughter was performing the lead role in a sixth-grade production of “The Wiz.” “I made an instant decision,” Copaken said. He told the President of the Marshall Islands that his daughter had to come first. Half an hour passed. “I get a frantic call from the State Department, very high levels: ‘Dick, I got a call from the President of the Marshall Islands. What’s going on?’ I told him. He said, ‘Dick, are you putting in jeopardy the national security of the United States for a sixth-grade production?’ ” In the end, the negotiations were suspended while Copaken flew home from Hawaii. “The point is,” Copaken said, “that absence at crucial moments has been a worry to me, and maybe this movie just grabbed at that issue.”

He stopped, seemingly dissatisfied. Was that really why he’d cried? Hollywood is awash in stories of bad fathers and abandoned children, and Copaken doesn’t cry in fancy restaurants every time he thinks of one of them. When he tried to remember the last time he cried at the movies, he was stumped. So he must have been responding to something else, too—some detail, some unconscious emotional trigger in the combination of the mother and the boy and the Scottish seaside town and the ship and the hired surrogate and the dying father. To say that he cried at “Dear Frankie” because of that lonely fatherless boy was as inadequate as saying that people cried at the death of Princess Diana because she was a beautiful princess. Surely it mattered as well that she was killed in the company of her lover, a man distrusted by the Royal Family. ”t this “Romeo and Juliet”? And surely it mattered that she died in a tunnel, and that the tunnel was in Paris, and that she was chased by motorbikes, and that she was blond and her lover was dark—because each one of those additional narrative details has complicated emotional associations, and it is the subtle combination of all these associations that makes us laugh or choke up when we remember a certain movie, every single time, even when we’re sitting in a fancy restaurant.

Of course, the optimal combination of all those elements is a mystery. That’s why it’s so hard to make a really memorable movie, and why we reward so richly the few people who can. But suppose you really, really loved the movies, and suppose you were a relentless type, and suppose you used all of the skills you’d learned during the course of your career at the highest rungs of the law to put together an international team of story experts. Do you think you could figure it out?

2.

The most famous dictum about Hollywood belongs to the screenwriter William Goldman. “Nobody knows anything,” Goldman wrote in “Adventures in the Screen Trade” a couple of decades ago. “Not one person in the entire motion picture field knows for a certainty what’s going to work. Every time out it’s a guess.” One of the highest-grossing movies in history, “”Raiders of the Lost Ark,” was offered to every studio in Hollywood, Goldman writes, and every one of them turned it down except Paramount: “Why did Paramount say yes? Because nobody knows anything. And why did all the other studios say no? Because nobody knows anything. And why did Universal, the mightiest studio of all, pass on Star Wars? . . . Because nobody, nobody—not now, not ever—knows the least goddamn thing about what is or isn’t going to work at the box office.”

What Goldman was saying was a version of something that has long been argued about art: that there is no way of getting beyond one’s own impressions to arrive at some larger, objective truth. There are no rules to art, only the infinite variety of subjective experience. “Beauty is no quality in things themselves,” the eighteenth-century Scottish philosopher David Hume wrote. “It exists merely in the mind which contemplates them; and each mind perceives a different beauty.” Hume might as well have said that nobody knows anything.

But Hume had a Scottish counterpart, Lord Kames, and Lord Kames was equally convinced that traits like beauty, sublimity, and grandeur were indeed reducible to a rational system of rules and precepts. He devised principles of congruity, propriety, and perspicuity: an elevated subject, for instance, must be expressed in elevated language; sound and signification should be in concordance; a woman was most attractive when in distress; depicted misfortunes must never occur by chance. He genuinely thought that the superiority of Virgil’s hexameters to Horace’s could be demonstrated with Euclidean precision, and for every Hume, it seems, there has always been a Kames—someone arguing that if nobody knows anything it is only because nobody’s looking hard enough.

In a small New York loft, just below Union Square, for example, there is a tech startup called Platinum Blue that consults for companies in the music business. Record executives have tended to be Humean: though they can tell you how they feel when they listen to a song, they don’t believe anyone can know with confidence whether a song is going to be a hit, and, historically, fewer than twenty per cent of the songs picked as hits by music executives have fulfilled those expectations. Platinum Blue thinks it can do better. It has a proprietary computer program that uses “spectral deconvolution software” to measure the mathematical relationships among all of a song’s structural components: melody, harmony, beat, tempo, rhythm, octave, pitch, chord progression, cadence, sonic brilliance, frequency, and so on. On the basis of that analysis, the firm believes it can predict whether a song is likely to become a hit with eighty-per-cent accuracy. Platinum Blue is staunchly Kamesian, and, if you have a field dominated by those who say there are no rules, it is almost inevitable that someone will come along and say that there are. The head of Platinum Blue is a man named Mike McCready, and the service he is providing for the music business is an exact model of what Dick Copaken would like to do for the movie business.

McCready is in his thirties, baldish and laconic, with rectangular hipster glasses. His offices are in a large, open room, with a row of windows looking east, across the rooftops of downtown Manhattan. In the middle of the room is a conference table, and one morning recently McCready sat down and opened his laptop to demonstrate the Platinum Blue technology. On his screen was a cluster of thousands of white dots, resembling a cloud. This was a “map” of the songs his group had run through its software: each dot represented a single song, and each song was positioned in the cloud according to its particular mathematical signature. “You could have one piano sonata by Beethoven at this end and another one here,” McCready said, pointing at the opposite end, “as long as they have completely different chord progressions and completely different melodic structures.”

McCready then hit a button on his computer, which had the effect of eliminating all the songs that had not made the Billboard Top 30 in the past five years. The screen went from an undifferentiated cloud to sixty discrete clusters. This is what the universe of hit songs from the past five years looks like structurally; hits come out of a small, predictable, and highly conserved set of mathematical patterns. “We take a new CD far in advance of its release date,” McCready said. “We analyze all twelve tracks. Then we overlay them on top of the already existing hit clusters, and what we can tell a record company is which of those songs conform to the mathematical pattern of past hits. Now, that doesn’t mean that they will be hits. But what we are saying is that, almost certainly, songs that fall outside these clusters will not be —regardless of how much they sound and feel like hit songs, and regardless of how positive your call-out research or focus-group research is.” Four years ago, when McCready was working with a similar version of the program at a firm in Barcelona, he ran thirty just-released albums, chosen at random, through his system. One stood out. The computer said that nine of the fourteen songs on the album had clear hit potential—which was unheard of. Nobody in his group knew much about the artist or had even listened to the record before, but the numbers said the album was going to be big, and McCready and his crew were of the belief that numbers do not lie. “Right around that time, a local newspaper came by and asked us what we were doing,” McCready said. “We explained the hit-prediction thing, and that we were really turned on to a record by this artist called Norah Jones.” The record was “Come Away with Me.” It went on to sell twenty million copies and win eight Grammy awards.

3.

The strength of McCready’s analysis is its precision. This past spring, for instance, he analyzed “Crazy,” by Gnarls Barkley. The computer calculated, first of all, the song’s Hit Grade—that is, how close it was to the center of any of those sixty hit clusters. Its Hit Grade was 755, on a scale where anything above 700 is exceptional. The computer also found that “Crazy” belonged to the same hit cluster as Dido’s “Thank You,” James Blunt’s “You’re Beautiful,” and Ashanti’s “Baby,” as well as older hits like “Let Me Be There,” by Olivia Newton-John, and “One Sweet Day,” by Mariah Carey, so that listeners who liked any of those songs would probably like “Crazy,” too. Finally, the computer gave “Crazy” a Periodicity Grade—which refers to the fact that, at any given time, only twelve to fifteen hit clusters are “active,” because from month to month the particular mathematical patterns that excite music listeners will shift around. “Crazy” ‘s periodicity score was 658—which suggested a very good fit with current tastes. The data said, in other words, that “Crazy” was almost certainly going to be huge—and, sure enough, it was.

If “Crazy” hadn’t scored so high, though, the Platinum Blue people would have given the song’s producers broad suggestions for fixing it. McCready said, “We can tell a producer, ‘These are the elements that seem to be pushing your song into the hit cluster. These are the variables that are pulling your song away from the hit cluster. The problem seems to be in your bass line.’ And the producer will make a bunch of mixes, where they do something different with the bass lines—increase the decibel level, or muddy it up. Then they come back to us. And we say, ‘Whatever you were doing with mix No. 3, do a little bit more of that and you’ll be back inside the hit cluster.’”

McCready stressed that his system didn’t take the art out of hit-making. Someone still had to figure out what to do with mix No. 3, and it was entirely possible that whatever needed to be done to put the song in the hit cluster wouldn’t work, because it would make the song sound wrong—and in order to be a hit a song had to sound right. Still, for the first time you wouldn’t be guessing about what needed to be done. You would know. And what you needed to know in order to fix the song was much simpler than anyone would have thought. McCready didn’t care about who the artist was, or the cleverness of the lyrics. He didn’t even have a way of feeding lyrics into his computer. He cared only about a song’s underlying mathematical structure. “If you go back to the popular melodies written by Beethoven and Mozart three hundred years ago,” he went on, “they conform to the same mathematical patterns that we are looking at today. What sounded like a beautiful melody to them sounds like a beautiful melody to us. What has changed is simply that we have come up with new styles and new instruments. Our brains are wired in a way—we assume—that keeps us coming back, again and again, to the same answers, the same pleasure centers.” He had sales data and Top 30 lists and deconvolution software, and it seemed to him that if you put them together you had an objective way of measuring something like beauty. “We think we’ve figured out how the brain works regarding musical taste,” McCready said.

It requires a very particular kind of person, of course, to see the world as a code waiting to be broken. Hume once called Kames “the most arrogant man in the world,” and to take this side of the argument you have to be. Kames was also a brilliant lawyer, and no doubt that matters as well, because to be a good lawyer is to be invested with a reverence for rules. (Hume defied his family’s efforts to make him a lawyer.) And to think like Kames you probably have to be an outsider. Kames was born Henry Home, to a farming family, and grew up in the sparsely populated cropping-and-fishing county of Berwickshire; he became Lord Kames late in life, after he was elevated to the bench. (Hume was born and reared in Edinburgh.) His early published work was about law and its history, but he soon wandered into morality, religion, anthropology, soil chemistry, plant nutrition, and the physical sciences, and once asked his friend Benjamin Franklin to explain the movement of smoke in chimneys. Those who believe in the power of broad patterns and rules, rather than the authority of individuals or institutions, are not intimidated by the boundaries and hierarchies of knowledge. They don’t defer to the superior expertise of insiders; they set up shop in a small loft somewhere downtown and take on the whole music industry at once. The difference between Hume and Kames is, finally, a difference in kind, not degree. You’re either a Kamesian or you’re not. And if you were to create an archetypal Kamesian—to combine lawyerliness, outsiderness, and supreme self-confidence in one dapper, Charlie Brown-headed combination? You’d end up with Dick Copaken.

“I remember when I was a sophomore in high school and I went into the bathroom once to wash my hands,” Copaken said. “I noticed the bubbles on the sink, and it fascinated me the way these bubbles would form and move around and float and reform, and I sat there totally transfixed. My father called me, and I didn’t hear him. Finally, he comes in. ‘Son. What the . . . are you all right?’ I said, ‘Bubbles, Dad, look what they do.’ He said, ‘Son, if you’re going to waste your time, waste it on something that may have some future consequence.’ Well, I kind of rose to the challenge. That summer, I bicycled a couple of miles to a library in Kansas City and I spent every day reading every book and article I could find on bubbles.”

Bubbles looked completely random, but young Copaken wasn’t convinced. He built a bubble-making device involving an aerator from a fish tank, and at school he pleaded with the math department to teach him the quadratic equations he needed to show why the bubbles formed the way they did. Then he devised an experiment, and ended up with a bronze medal at the International Science Fair. His interest in bubbles was genuine, but the truth is that almost anything could have caught Copaken’s eye: pop songs, movies, the movement of chimney smoke. What drew him was not so much solving this particular problem as the general principle that problems were solvable—that he, little Dick Copaken from Kansas City, could climb on his bicycle and ride to the library and figure out something that his father thought wasn’t worth figuring out.

Copaken has written a memoir of his experience defending the tiny Puerto Rican islands of Culebra and Vieques against the U.S. Navy, which had been using their beaches for target practice. It is a riveting story. Copaken takes on the vast Navy bureaucracy, armed only with arcane provisions of environmental law. He investigates the nesting grounds of the endangered hawksbill turtle, and the mating habits of a tiny yet extremely loud tree frog known as the coqui, and at one point he transports four frozen whale heads from the Bahamas to Harvard Medical School. Copaken wins. The Navy loses.

The memoir reads like a David-and-Goliath story. It isn’t. David changed the rules on Goliath. He brought a slingshot to a sword fight. People like Copaken, though, don’t change the rules; they believe in rules. Copaken would have agreed to sword-on-sword combat. But then he would have asked the referee for a stay, deposed Goliath and his team at great length, and papered him with brief after brief until he conceded that his weapon did not qualify as a sword under §48(B)(6)(e) of the Samaria Convention of 321 B.C. (The Philistines would have settled.) And whereas David knew that he couldn’t win a conventional fight with Goliath, the conviction that sustained Copaken’s long battle with the Navy was, to the contrary, that so long as the battle remained conventional—so long as it followed the familiar pathways of the law and of due process—he really could win. Dick Copaken didn’t think he was an underdog at all. If you believe in rules, Goliath is just another Philistine, and the Navy is just another plaintiff. As for the ineffable mystery of the Hollywood blockbuster? Well, Mr. Goldman, you may not know anything. But I do.

4.

Dick Copaken has a friend named Nick Meaney. They met on a case years ago. Meaney has thick dark hair. He is younger and much taller than Copaken, and seems to regard his friend with affectionate amusement. Meaney’s background is in risk management, and for years he’d been wanting to bring the principles of that world to the movie business. In 2003, Meaney and Copaken were driving through the English countryside to Durham when Meaney told Copaken about a friend of his from college. The friend and his business partner were students of popular narrative: the sort who write essays for obscure journals serving the small band of people who think deeply about, say, the evolution of the pilot episode in transnational TV crime dramas. And, for some time, they had been developing a system for evaluating the commercial potential of stories. The two men, Meaney told Copaken, had broken down the elements of screenplay narrative into multiple categories, and then drawn on their encyclopedic knowledge of television and film to assign scripts a score in each of those categories—creating a giant screenplay report card. The system was extraordinarily elaborate. It was under constant refinement. It was also top secret. Henceforth, Copaken and Meaney would refer to the two men publicly only as “Mr. Pink” and “Mr. Brown,” an homage to “Reservoir Dogs.”

“The guy had a big wall, and he started putting up little Post-its covering everything you can think of,” Copaken said. It was unclear whether he was talking about Mr. Pink or Mr. Brown or possibly some Obi-Wan Kenobi figure from whom Mr. Pink and Mr. Brown first learned their trade. “You know, the star wears a blue shirt. The star doesn’t zip up his pants. Whatever. So he put all these factors up and began moving them around as the scripts were either successful or unsuccessful, and he began grouping them and eventually this evolved to a kind of ad-hoc analytical system. He had no theory as to what would work, he just wanted to know what did work.”

Copaken and Meaney also shared a fascination with a powerful kind of computerized learning system called an artificial neural network. Neural networks are used for data mining—to look for patterns in very large amounts of data. In recent years, they have become a critical tool in many industries, and what Copaken and Meaney realized, when they thought about Mr. Pink and Mr. Brown, was that it might now be possible to bring neural networks to Hollywood. They could treat screenplays as mathematical propositions, using Mr. Pink and Mr. Brown’s categories and scores as the motion-picture equivalents of melody, harmony, beat, tempo, rhythm, octave, pitch, chord progression, cadence, sonic brilliance, and frequency.

Copaken and Meaney brought in a former colleague of Meaney’s named Sean Verity, and the three of them signed up Mr. Pink and Mr. Brown. They called their company Epagogix—a reference to Aristotle’s discussion of epagogic, or inductive, learning—and they started with a “training set” of screenplays that Mr. Pink and Mr. Brown had graded. Copaken and Meaney won’t disclose how many scripts were in the training set. But let’s say it was two hundred. Those scores—along with the U.S. box-office receipts for each of the films made from those screenplays—were fed into a neural network built by a computer scientist of Meaney’s acquaintance. “I can’t tell you his name,” Meaney said, “but he’s English to his bootstraps.” Mr. Bootstraps then went to work, trying to use Mr. Pink and Mr. Brown’s scoring data to predict the box-office receipts of every movie in the training set. He started with the first film and had the neural network make a guess: maybe it said that the hero’s moral crisis in act one, which rated a 7 on the 10-point moral-crisis scale, was worth $7 million, and having a gorgeous red-headed eighteen-year-old female lead whose characterization came in at 6.5 was worth $3 million and a 9-point bonding moment between the male lead and a four-year-old boy in act three was worth $2 million, and so on, putting a dollar figure on every grade on Mr. Pink and Mr. Brown’s report card until the system came up with a prediction. Then it compared its guess with how that movie actually did. Was it close? Of course not. The neural network then went back and tried again. If it had guessed $20 million and the movie actually made $110 million, it would reweight the movie’s Pink/Brown scores and run the numbers a second time. And then it would take the formula that worked best on Movie One and apply it to Movie Two, and tweak that until it had a formula that worked on Movies One and Two, and take that formula to Movie Three, and then to four and five, and on through all two hundred movies, whereupon it would go back through all the movies again, through hundreds of thousands of iterations, until it had worked out a formula that did the best possible job of predicting the financial success of every one of the movies in its database.

That formula, the theory goes, can then be applied to new scripts. If you were developing a $75-million buddy picture for Bruce Willis and Colin Farrell, Epagogix says, it can tell you, based on past experience, what that script’s particular combination of narrative elements can be expected to make at the box office. If the formula says it’s a $50-million script, you pull the plug. “We shoot turkeys,” Meaney said. He had seen Mr. Bootstraps and the neural network in action: “It can sometimes go on for hours. If you look at the computer, you see lots of flashing numbers in a gigantic grid. It’s like ‘The Matrix.’ There are a lot of computations. The guy is there, the whole time, looking at it. It eventually stops flashing, and it tells us what it thinks the American box-office will be. A number comes out.”

The way the neural network thinks is not that different from the way a Hollywood executive thinks: if you pitch a movie to a studio, the executive uses an ad-hoc algorithm—perfected through years of trial and error—to put a value on all the components in the story. Neural networks, though, can handle problems that have a great many variables, and they never play favorites—which means (at least in theory) that as long as you can give the neural network the same range of information that a human decision-maker has, it ought to come out ahead. That’s what the University of Arizona computer scientist Hsinchun Chen demonstrated ten years ago, when he built a neural network to predict winners at the dog track. Chen used the ten variables that greyhound experts told him they used in making their bets—like fastest time and winning percentage and results for the past seven races—and trained his system with the results of two hundred races. Then he went to the greyhound track in Tucson and challenged three dog-racing handicappers to a contest. Everyone picked winners in a hundred races, at a modest two dollars a bet. The experts lost $71.40, $61.20, and $70.20, respectively. Chen won $124.80. It wasn’t close, and one of the main reasons was the special interest the neural network showed in something called “race grade”: greyhounds are moved up and down through a number of divisions, according to their ability, and dogs have a big edge when they’ve just been bumped down a level and a big handicap when they’ve just been bumped up. “The experts know race grade exists, but they don’t weight it sufficiently,” Chen said. “They are all looking at win percentage, place percentage, or thinking about the dogs’ times.”

Copaken and Meaney figured that Hollywood’s experts also had biases and skipped over things that really mattered. If a neural network won at the track, why not Hollywood? “One of the most powerful aspects of what we do is the ruthless objectivity of our system,” Copaken said. “It doesn’t care about maintaining relationships with stars or agents or getting invited to someone’s party. It doesn’t care about climbing the corporate ladder. It has one master and one master only: how do you get to bigger box-office? Nobody else in Hollywood is like that.”

In the summer of 2003, Copaken approached Josh Berger, a senior executive at Warner Bros. in Europe. Meaney was opposed to the idea: in his mind, it was too early. “I just screamed at Dick,” he said. But Copaken was adamant. He had Mr. Bootstraps, Mr. Pink, and Mr. Brown run sixteen television pilots through the neural network, and try to predict the size of each show’s eventual audience. “I told Josh, ‘Stick this in a drawer, and I’ll come back at the end of the season and we can check to see how we did,’ ” Copaken said. In January of 2004, Copaken tabulated the results. In six cases, Epagogix guessed the number of American homes that would tune in to a show to within .06 per cent. In thirteen of the sixteen cases, its predictions were within two per cent. Berger was floored. “It was incredible,” he recalls. “It was like someone saying to you, ‘We’re going to show you how to count cards in Vegas.’ It had that sort of quality.”

Copaken then approached another Hollywood studio. He was given nine unreleased movies to analyze. Mr. Pink, Mr. Brown, and Mr. Bootstraps worked only from the script—without reference to the stars or the director or the marketing budget or the producer. On three of the films—two of which were low-budget—the Epagogix estimates were way off. On the remaining six—including two of the studio’s biggest-budget productions—they correctly identified whether the film would make or lose money. On one film, the studio thought it had a picture that would make a good deal more than $100 million. Epagogix said $49 million. The movie made less than $40 million. On another, a big-budget picture, the team’s estimate came within $1.2 million of the final gross. On a number of films, they were surprisingly close. “They were basically within a few million,” a senior executive at the studio said. “It was shocking. It was kind of weird.” Had the studio used Epagogix on those nine scripts before filming started, it could have saved tens of millions of dollars. “I was impressed by a couple of things,” another executive at the same studio said. “I was impressed by the things they thought mattered to a movie. They weren’t the things that we typically give credit to. They cared about the venue, and whether it was a love story, and very specific things about the plot that they were convinced determined the outcome more than anything else. It felt very objective. And they could care less about whether the lead was Tom Cruise or Tom Jones.”

The Epagogix team knocked on other doors that weren’t quite so welcoming. This was the problem with being a Kamesian. Your belief in a rule-bound universe was what gave you, an outsider, a claim to real expertise. But you were still an outsider. You were still Dick Copaken, the blue-blazered corporate lawyer who majored in bubbles as a little boy in Kansas City, and a couple of guys from the risk-management business, and three men called Pink, Brown, and Bootstraps—and none of you had ever made a movie in your life. And what were you saying? That stars didn’t matter, that the director didn’t matter, and that all that mattered was story—and, by the way, that you understood story the way the people on the inside, people who had spent a lifetime in the motion-picture business, didn’t. “They called, and they said they had a way of predicting box-office success or failure, which is everyone’s fantasy,” one former studio chief recalled. “I said to them, ‘I hope you’re right.’ ” The executive seemed to think of the Epagogix team as a small band of Martians who had somehow slipped their U.F.O. past security. “In reality, there are so many circumstances that can affect a movie’s success,” the executive went on. “Maybe the actor or actress has an external problem. Or this great actor, for whatever reason, just fails. You have to fire a director. Or September 11th or some other thing happens. There are many people who have come forward saying they have a way of predicting box-office success, but so far nobody has been able to do it. I think we know something. We just don’t know enough. I still believe in something called that magical thing—talent, the unexpected. The movie god has to shine on you.” You were either a Kamesian or you weren’t, and this person wasn’t: “My first reaction to those guys? Bullshit.”

5.

A few months ago, Dick Copaken agreed to lift the cloud of unknowing surrounding Epagogix, at least in part. He laid down three conditions: the meeting was to be in London, Mr. Pink and Mr. Brown would continue to be known only as Mr. Pink and Mr. Brown, and no mention was to be made of the team’s current projects. After much discussion, an agreement was reached. Epagogix would analyze the 2005 movie “The Interpreter,” which was directed by Sydney Pollack and starred Sean Penn and Nicole Kidman. “The Interpreter” had a complicated history, having gone through countless revisions, and there was a feeling that it could have done much better at the box office. If ever there was an ideal case study for the alleged wizardry of Epagogix, this was it.

The first draft of the movie was written by Charles Randolph, a philosophy professor turned screenwriter. It opened in the fictional African country of Matobo. Two men in a Land Rover pull up to a soccer stadium. A group of children lead them to a room inside the building. On the ground is a row of corpses.

Cut to the United Nations, where we meet Silvia Broome, a young woman who works as an interpreter. She goes to the U.N. Security Service and relates a terrifying story. The previous night, while working late in the interpreter’s booth, she overheard two people plotting the assassination of Matobo’s murderous dictator, Edmund Zuwanie, who is coming to New York to address the General Assembly. She says that the plotters saw her, and that her life may be in danger. The officer assigned to her case, Tobin Keller, is skeptical, particularly when he learns that she, too, is from Matobo, and that her parents were killed in the country’s civil war. But after Broome suffers a series of threatening incidents Keller starts to believe her. His job is to protect Zuwanie, but he now feels moved to act as Broome’s bodyguard as well. A quiet, slightly ambiguous romantic attraction begins to develop between them. Zuwanie’s visit draws closer. Broome’s job is to be his interpreter. On the day of the speech, Broome ends up in the greenroom with Zuwanie. Keller suddenly realizes the truth: that she has made up the whole story as a way of bringing Zuwanie to justice. He rushes to the greenroom. Broome, it seems, has poisoned Zuwanie and is withholding the antidote unless he goes onstage and confesses to the murder of his countrymen. He does. Broome escapes. A doctor takes a look at the poison. It’s harmless. The doctor turns to the dictator, who has just been tricked into writing his own prison sentence: “You were never in danger, Mr. Zuwanie.”

Randolph says that the film he was thinking of while he was writing “The Interpreter” was Francis Ford Coppola’s classic “The Conversation.” He wanted to make a spare, stark movie about an isolated figure. “She’s a terrorist,” Randolph said of Silvia Broome. “She comes to this country to do a very specific task, and when that task is done she’s gone again. I wanted to write about this idea of a noble terrorist, who tried to achieve her ends with a character assassination, not a real assassination.” Randolph realized that most moviegoers—and most Hollywood executives—prefer characters who have psychological motivations. But he wasn’t trying to make “Die Hard.” “Look, I’m the son of a preacher,” he said. “I believe that ideology motivates people.”

In 2004, Sydney Pollack signed on to direct the project. He loved the idea of an interpreter at the United Nations and the conceit of an overheard conversation. But he wanted to make a commercial movie, and parts of the script didn’t feel right to him. He didn’t like the twist at the end, for instance. “I felt like I had been tricked, because in fact there was no threat,” Pollack said. “As much as I liked the original script, I felt like an audience would somehow, at the end, feel cheated.” Pollack also felt that audiences would want much more from Silvia Broome’s relationship with Tobin Keller. “I’ve never been able to do a movie without a love story in it,” he said. “For me, the heart of it is always the man and the woman and who they are and what they are going through.” Pollack brought Randolph back for rewrites. He then hired Scott Frank and Steven Zaillian, two of the most highly sought-after screenwriters in Hollywood—and after several months the story was turned inside out. Now Broome didn’t tell the story of overhearing that conversation. It actually happened. She wasn’t a terrorist anymore. She was a victim. She ”t an isolated figure. She was given a social life. She wasn’t manipulating Keller. Their relationship was more prominent. A series of new characters—political allies and opponents of Zuwanie’s—were added, as was a scene in Brooklyn where a bus explodes, almost killing Broome. “I remember when I came on ‘Minority Report,’ and started over,” said Frank, who wrote many of the new scenes for “The Interpreter.” “There weren’t many characters. When I finished, there were two mysteries and a hundred characters. I have diarrhea of the plot. This movie cried out for that. There are never enough suspects and red herrings.”

The lingering problem, though, was the ending. If Broome wasn’t after Zuwanie, who was? “We struggled,” Pollack said. “It was a long process, to the point where we almost gave up.” In the end, Zuwanie was made the engineer of the plot: he fakes the attempt on his life in order to justify his attacks on his enemies back home. Zuwanie hires a man to shoot him, and then another of Zuwanie’s men shoots the assassin before he can do the job—and in the chaos Broome ends up with a gun in her hand, training it on Zuwanie. “The end was the hardest part,” Frank said. “All these balls were in the air. But I couldn’t find a satisfying way to resolve it. We had to put a gun in the hand of a pacifist. I couldn’t quite sew it up in the right way. Sydney kept saying, ‘You’re so close.’ But I kept saying, ‘Yeah, but I don’t believe what I’m writing.’ I wonder if I did a disservice to ‘The Interpreter.’ I don’t know that I made it better. I may have just made it different.”

This, then, was the question for Epagogix: If Pollack’s goal was to make “The Interpreter” a more commercial movie, how well did he succeed? And could he have done better?

6.

The debriefing took place in central London, behind the glass walls of the private dining room of a Mayfair restaurant. The waiters came in waves, murmuring their announcements of the latest arrival from the kitchen. The table was round. Copaken, dapper as always in his navy blazer, sat next to Sean Verity, followed by Meaney, Mr. Brown, and Mr. Pink. Mr. Brown was very tall, and seemed to have a northern English accent. Mr. Pink was slender and graying, and had an air of authority about him. His academic training was in biochemistry. He said he thought that, in the highly emotional business of Hollywood, having a scientific background was quite useful. There was no sign of Mr. Bootstraps.

Mr. Pink began by explaining the origins of their system. “There were certain historical events that allowed us to go back and test how appealing one film was against another,” he said. “The very simple one is that in the English market, in the sixties on Sunday night, religious programming aired on the major networks. Nobody watched it. And, as soon as that finished, movies came on. There were no lead-ins, and only two competing channels. Plus, across the country you had a situation where the commercial sector was playing a whole variety of movies against the standard, the BBC. It might be a John Wayne movie in Yorkshire, and a musical in Somerset, and the BBC would be the same movie everywhere. So you had a control. It was very pure and very simple. That was a unique opportunity to try and make some guesstimates as to why movies were doing what they were doing.”

Brown nodded. “We built a body of evidence until we had something systematic,” he said.

Pink estimated that they had analyzed thousands of movies. “The thing is that not everything comes to you as a script. For a long period, we worked for a broadcaster who used to send us a couple of paragraphs. We made our predictions based on that much. Having the script is actually too much information sometimes. You’re trying to replicate what the audience is doing. They’re trying to make a choice between three movies, and all they have at that point is whatever they’ve seen in TV Guide or on any trailer they’ve seen. We have to take a piece here and a piece here. Take a couple of reference points. When I look at a story, there are certain things I’m looking for—certain themes, and characters you immediately focus on.” He thought for a moment. “That’s not to deny that it matters whether the lead character wears a hat,” he added, in a way that suggested he and Mr. Brown had actually thought long and hard about leads and hats.

“There’s always a pattern,” he went on. “There are certain stories that come back, time and time again, and that always work. You know, whenever we go into a market—and we work in fifty markets—the initial thing people say is ‘What do you know about our market?’ The assumption is that, say, Japan is different from us—that there has to be something else going on there. But, basically, they’re just like us. It’s the consistency of these reappearing things that I find amazing.”

“Biblical stories are a classic case,” Mr. Brown put in. “There is something about what they’re telling and the message that’s coming out that seems to be so universal. With Mel Gibson’s ‘The Passion,’ people always say, ‘Who could have predicted that?’ And the answer is, we could have.”

They had looked at “The Interpreter” scripts a few weeks earlier. The process typically takes them a day. They read, they graded, and then they compared notes, because Mr. Pink was the sort who went for “Yojimbo” and Mr. Brown’s favorite movie was “Alien” (the first one), so they didn’t always agree. Mr. Brown couldn’t remember a single script he’d read where he thought there wasn’t room for improvement, and Mr. Pink, when asked the same question, could come up with just one: “Lethal Weapon.” “A friend of mine gave me the shooting script before it came out, and I remember reading it and thinking, It’s all there. It was all on the page.” Once Mr. Pink and Mr. Brown had scored “The Interpreter,” they gave their analyses to Mr. Bootstraps, who did fifteen runs through the neural network: the original Randolph script, the shooting script, and certain variants of the plot that Epagogix devised. Mr. Bootstraps then passed his results to Copaken, who wrote them up. The Epagogix reports are always written by Copaken, and they are models of lawyerly thoroughness. This one ran to thirty-eight pages. He had finished the final draft the night before, very late. He looked fresh as a daisy.

Mr. Pink started with the original script. “My pure reaction? I found it very difficult to read. I got confused. I had to reread bits. We do this a lot. If a project takes more than an hour to read, then there’s something going on that I’m not terribly keen on.”

“It didn’t feel to me like a mass-appeal movie,” Mr. Brown added. “It seemed more niche.”

When Mr. Bootstraps ran Randolph’s original draft through the neural network, the computer called it a $33-million movie—an “intelligent” thriller, in the same commercial range as “The Constant Gardener” or “Out of Sight.” According to the formula, the final shooting script was a $69-million picture (an estimate that came within $4 million of the actual box-office). Mr. Brown wasn’t surprised. The shooting script, he said, “felt more like an American movie, where the first one seemed European in style.”

Everyone agreed, though, that Pollack could have done much better. There was, first of all, the matter of the United Nations. “They had a unique opportunity to get inside the building,” Mr. Pink said. “But I came away thinking that it could have been set in any boxy office tower in Manhattan. An opportunity was missed. That’s when we get irritated—when there are opportunities that could very easily be turned into something that would actually have had an impact.”

“Locale is an extra character,” Mr. Brown said. “But in this case it’s a very bland character that didn’t really help.”

In the Epagogix secret formula, it seemed, locale matters a great deal. “You know, there’s a big difference between city and countryside,” Mr. Pink said. “It can have a huge effect on a movie’s ability to draw in viewers. And writers just do not take advantage of it. We have a certain set of values that we attach to certain places.”

Mr. Pink and Mr. Brown ticked off the movies and television shows that they thought understood the importance of locale: “Crimson Tide,” “Lawrence of Arabia,” “Lost,” “Survivor,” “Castaway,” “Deliverance.” Mr. Pink said, “The desert island is something that we have always recognized as a pungent backdrop, but it’s not used that often. In the same way, prisons can be a powerful environment, because they are so well defined.” The U.N. could have been like that, but it wasn’t. Then there was the problem of starting, as both scripts did, in Africa—and not just Africa but a fictional country in Africa. The whole team found that crazy. “Audiences are pretty parochial, by and large,” Mr. Pink said. “If you start off by telling them, ‘We’re going to begin this movie in Africa,’ you’re going to lose them. They’ve bought their tickets. But when they come out they’re going to say, ‘It was all right. But it was Africa.’ ” The whole thing seemed to leave Mr. Pink quite distressed. He looked at Mr. Brown beseechingly.

Mr. Brown changed the subject. “It’s amazing how often quite little things, quite small aspects, can spoil everything,” he said. “I remember seeing the trailer for ‘V for Vendetta’ and deciding against it right there, for one very simple reason: there was a ridiculous mask on the main character. If you can’t see the face of the character, you can’t tell what that person is thinking. You can’t tell who they are. With ‘Spider-Man’ and ‘Superman,’ though, you do see the face, so you respond to them.”

The team once gave a studio a script analysis in which almost everything they suggested was, in Hollywood terms, small. They wanted the lead to jump off the page a little more. They wanted the lead to have a young sidekick—a relatively minor character—to connect with a younger demographic, and they wanted the city where the film was set to be much more of a presence. The neural network put the potential value of better characterization at an extra $2.46 million in U.S. box-office revenue; the value of locale adjustment at $4.92 million; the value of a sidekick at $12.3 million—and the value of all three together (given the resulting synergies) at $24.6 million. That’s another $25 million for a few weeks of rewrites and maybe a day or two of extra filming. Mr. Bootstraps, incidentally, ran the numbers and concluded that the script would make $47 million if the suggested changes were not made. The changes were not made. The movie made $50 million.

Mr. Pink and Mr. Brown went on to discuss the second “Interpreter” screenplay, the shooting script. They thought the ending was implausible. Charles Randolph had originally suggested that the Tobin Keller character be black, not white, in order to create the frisson of bringing together a white African and a black American. Mr. Pink and Mr. Brown independently came to the same conclusion. Apparently, the neural network ran the numbers on movies that paired black and white leads—”Lethal Weapon,” “The Crying Game,” “Independence Day,” “Men in Black,” “Die Another Day,” “The Pelican Brief”—and found that the black-white combination could increase box-office revenue. The computer did the same kind of analysis on Scott Frank’s “diarrhea of the plot,” and found that there were too many villains. And if Silvia Broome was going to be in danger, Mr. Bootstraps made clear, she really had to be in danger.

“Our feeling—and Dick, you may have to jump in here—is that the notion of a woman in peril is a very powerful narrative element,” Mr. Pink said. He glanced apprehensively at Copaken, evidently concerned that what he was about to say might fall in the sensitive category of the proprietary. “How powerful?” He chose his words carefully. “Well above average. And the problem is that we lack a sense of how much danger she is in, so an opportunity is missed. There were times when you were thinking, Is this something she has created herself? Is someone actually after her? You are confused. There is an element of doubt, and that ambiguity makes it possible to doubt the danger of the situation.” Of course, all that ambiguity was there because in the Randolph script she was making it all up, and we were supposed to doubt the danger of the situation. But Mr. Pink and Mr. Brown believed that, once you decided you weren’t going to make a European-style niche movie, you had to abandon ambiguity altogether.

“You’ve got to make the peril real,” Mr. Pink said.

The Epagogix revise of “The Interpreter” starts with an upbeat Silvia Broome walking into the United Nations, flirting with the security guard. The two men plotting the assassination later see her and chase her through the labyrinthine cor-ridors of what could only be the U.N. building. The ambiguous threats to Broome’s life are now explicit. At one point in the Epagogix version, a villain pushes Broome’s Vespa off one of Manhattan’s iconic East River bridges. She hangs on to her motorbike for dear life, as it swings precariously over the edge of the parapet. Tobin Keller, in a police helicopter, swoops into view: “As she clings to Tobin’s muscular body while the two of them are hoisted up into the hovering helicopter, we sense that she is feeling more than relief.” In the Epagogix ending, Broome stabs one of Zuwanie’s security men with a knife. Zuwanie storms off the stage, holds a press conference, and is shot dead by a friend of Broome’s brother. Broome cradles the dying man in her arms. He ” dies peacefully,” with ” a smile on his blood-spattered face.” Then she gets appointed Matobo’s U.N. ambassador. She turns to Keller. “‘This time,’ she notes with a wry smile . . . ‘you will have to protect me.’ ” Bootstraps’s verdict was that this version would result in a U.S. box-office of $111 million.

“It’s funny,” Mr. Pink said. “This past weekend, ‘The Bodyguard’ was on TV. Remember that piece of”—he winced—”entertainment? Which is about a bodyguard and a woman. The final scene is that they are right back together. It is very clearly and deliberately sown. That is the commercial way, if you want more bodies in the seats.”

“You have to either consummate it or allow for the possibility of that,” Copaken agreed.

They were thinking now of what would happen if they abandoned all fealty to the original, and simply pushed the movie’s premise as far as they could possibly go.

Mr. Pink went on, “If Dick had said, ‘You can take this project wherever you want,’ we probably would have ended up with something a lot closer to ‘The Bodyguard’—where you have a much more romantic film, a much more powerful focus to the two characters—without all the political stuff going on in the background. You go for the emotions on a very basic level. What would be the upper limit on that? You know, the upper limit of anything these days is probably still ‘Titanic.’ I’m not saying we could do six hundred million dollars. But it could be two hundred million.”

7.

It was clear that the whole conversation was beginning to make Mr. Pink uncomfortable. He didn’t like “The Bodyguard.” Even the title made him wince. He was the sort who liked “Yojimbo,” after all. The question went around the room: What would you do with “The Interpreter”? Sean Verity wanted to juice up the action-adventure elements and push it to the $150- to $160-million range. Meaney wanted to do without expensive stars: he didn’t think they were worth the money. Copaken wanted more violence, and he also favored making Keller black. But he didn’t want to go all the way to “The Bodyguard,” either. This was a man who loved “Dear Frankie” as much as any film he’d seen in recent memory, and “Dear Frankie” had a domestic box-office gross of $1.3 million. If you followed the rules of Epagogix, there wouldn’t be any movies like “Dear Frankie.” The neural network had one master, the market, and answered one question: how do you get to bigger box-office? But once a movie had made you vulnerable—once you couldn’t even retell the damn story without getting emotional—you couldn’t be content with just one master anymore.

That was the thing about the formula: it didn’t make the task of filmmaking easier. It made it harder. So long as nobody knows anything, you’ve got license to do whatever you want. You can start a movie in Africa. You can have male and female leads not go off together—all in the name of making something new. Once you came to think that you knew something, though, you had to decide just how much money you were willing to risk for your vision. Did the Epagogix team know what the answer to that question was? Of course not. That question required imagination, and they weren’t in the imagination business. They were technicians with tools: computer programs and analytical systems and proprietary software that calculated mathematical relationships among a laundry list of structural variables. At Platinum Blue, Mike McCready could tell you that the bass line was pushing your song out of the center of hit cluster 31. But he couldn’t tell you exactly how to fix the bass line, and he couldn’t guarantee that the redone version would still sound like a hit, and you didn’t see him releasing his own album of computer-validated pop music. A Kamesian had only to read Lord Kames to appreciate the distinction. The most arrogant man in the world was a terrible writer: clunky, dense, prolix. He knew the rules of art. But that didn’t make him an artist.

Mr. Brown spoke last. “I don’t think it needs to be a big-budget picture,” he said. “I think we do what we can with the original script to make it a strong story, with an ending that is memorable, and then do a slow release. A low-budget picture. One that builds through word of mouth—something like that.” He was confident that he had the means to turn a $69-million script into a $111-million movie, and then again into a $150- to $200-million blockbuster. But it had been a long afternoon, and part of him had a stubborn attachment to “The Interpreter” in something like its original form. Mr. Bootstraps might have disagreed. But Mr. Bootstraps was nowhere to be seen.

© 2013 Malcolm Gladwell.

DAVID AND GOLIATH
 
WHAT THE DOG SAW
 
OUTLIERS
 
BLINK
 
THE TIPPING POINT
 
ARTICLES
 
ETC.
Creation Myth

Posted May 16, 2011 by MALCOLM GLADWELL & filed under ANNALS OF BUSINESS, THE NEW YORKER - ARCHIVE.

Xerox PARC, Apple, and the truth about innovation.

1.

In late 1979, a twenty-four-year-old entrepreneur paid a visit to a research center in Silicon Valley called Xerox PARC. He was the co-founder of a small computer startup down the road, in Cupertino. His name was Steve Jobs.

Xerox PARC was the innovation arm of the Xerox Corporation. It was, and remains, on Coyote Hill Road, in Palo Alto, nestled in the foothills on the edge of town, in a long, low concrete building, with enormous terraces looking out over the jewels of Silicon Valley. To the northwest was Stanford University’s Hoover Tower. To the north was Hewlett-Packard’s sprawling campus. All around were scores of the other chip designers, software firms, venture capitalists, and hardware-makers. A visitor to PARC, taking in that view, could easily imagine that it was the computer world’s castle, lording over the valley below—and, at the time, this wasn’t far from the truth. In 1970, Xerox had assembled the world’s greatest computer engineers and programmers, and for the next ten years they had an unparalleled run of innovation and invention. If you were obsessed with the future in the seventies, you were obsessed with Xerox PARC—which was why the young Steve Jobs had driven to Coyote Hill Road.

Apple was already one of the hottest tech firms in the country. Everyone in the Valley wanted a piece of it. So Jobs proposed a deal: he would allow Xerox to buy a hundred thousand shares of his company for a million dollars—its highly anticipated I.P.O. was just a year away—if PARC would “open its kimono.” A lot of haggling ensued. Jobs was the fox, after all, and PARC was the henhouse. What would he be allowed to see? What wouldn’t he be allowed to see? Some at PARC thought that the whole idea was lunacy, but, in the end, Xerox went ahead with it. One PARC scientist recalls Jobs as “rambunctious”—a fresh-cheeked, caffeinated version of today’s austere digital emperor. He was given a couple of tours, and he ended up standing in front of a Xerox Alto, PARC’s prized personal computer.

An engineer named Larry Tesler conducted the demonstration. He moved the cursor across the screen with the aid of a “mouse.” Directing a conventional computer, in those days, meant typing in a command on the keyboard. Tesler just clicked on one of the icons on the screen. He opened and closed “windows,” deftly moving from one task to another. He wrote on an elegant word-processing program, and exchanged e-mails with other people at PARC, on the world’s first Ethernet network. Jobs had come with one of his software engineers, Bill Atkinson, and Atkinson moved in as close as he could, his nose almost touching the screen. “Jobs was pacing around the room, acting up the whole time,” Tesler recalled. “He was very excited. Then, when he began seeing the things I could do onscreen, he watched for about a minute and started jumping around the room, shouting, ‘Why aren’t you doing anything with this? This is the greatest thing. This is revolutionary!’”

Xerox began selling a successor to the Alto in 1981. It was slow and underpowered—and Xerox ultimately withdrew from personal computers altogether. Jobs, meanwhile, raced back to Apple, and demanded that the team working on the company’s next generation of personal computers change course. He wanted menus on the screen. He wanted windows. He wanted a mouse. The result was the Macintosh, perhaps the most famous product in the history of Silicon Valley.

“If Xerox had known what it had and had taken advantage of its real opportunities,” Jobs said, years later, “it could have been as big as I.B.M. plus Microsoft plus Xerox combined—and the largest high-technology company in the world.”

This is the legend of Xerox PARC. Jobs is the Biblical Jacob and Xerox is Esau, squandering his birthright for a pittance. In the past thirty years, the legend has been vindicated by history. Xerox, once the darling of the American high-technology community, slipped from its former dominance. Apple is now ascendant, and the demonstration in that room in Palo Alto has come to symbolize the vision and ruthlessness that separate true innovators from also-rans. As with all legends, however, the truth is a bit more complicated.

2.

After Jobs returned from PARC, he met with a man named Dean Hovey, who was one of the founders of the industrial-design firm that would become known as IDEO. “Jobs went to Xerox PARC on a Wednesday or a Thursday, and I saw him on the Friday afternoon,” Hovey recalled. “I had a series of ideas that I wanted to bounce off him, and I barely got two words out of my mouth when he said, ‘No, no, no, you’ve got to do a mouse.’ I was, like, ‘What’s a mouse?’ I didn’t have a clue. So he explains it, and he says, ‘You know, [the Xerox mouse] is a mouse that cost three hundred dollars to build and it breaks within two weeks. Here’s your design spec: Our mouse needs to be manufacturable for less than fifteen bucks. It needs to not fail for a couple of years, and I want to be able to use it on Formica and my bluejeans.’ From that meeting, I went to Walgreens, which is still there, at the corner of Grant and El Camino in Mountain View, and I wandered around and bought all the underarm deodorants that I could find, because they had that ball in them. I bought a butter dish. That was the beginnings of the mouse.”

I spoke with Hovey in a ramshackle building in downtown Palo Alto, where his firm had started out. He had asked the current tenant if he could borrow his old office for the morning, just for the fun of telling the story of the Apple mouse in the place where it was invented. The room was the size of someone’s bedroom. It looked as if it had last been painted in the Coolidge Administration. Hovey, who is lean and healthy in a Northern California yoga-and-yogurt sort of way, sat uncomfortably at a rickety desk in a corner of the room. “Our first machine shop was literally out on the roof,” he said, pointing out the window to a little narrow strip of rooftop, covered in green outdoor carpeting. “We didn’t tell the planning commission. We went and got that clear corrugated stuff and put it across the top for a roof. We got out through the window.”

He had brought a big plastic bag full of the artifacts of that moment: diagrams scribbled on lined paper, dozens of differently sized plastic mouse shells, a spool of guitar wire, a tiny set of wheels from a toy train set, and the metal lid from a jar of Ralph’s preserves. He turned the lid over. It was filled with a waxlike substance, the middle of which had a round indentation, in the shape of a small ball. “It’s epoxy casting resin,” he said. “You pour it, and then I put Vaseline on a smooth steel ball, and set it in the resin, and it hardens around it.” He tucked the steel ball underneath the lid and rolled it around the tabletop. “It’s a kind of mouse.”

The hard part was that the roller ball needed to be connected to the housing of the mouse, so that it didn’t fall out, and so that it could transmit information about its movements to the cursor on the screen. But if the friction created by those connections was greater than the friction between the tabletop and the roller ball, the mouse would skip. And the more the mouse was used the more dust it would pick up off the tabletop, and the more it would skip. The Xerox PARC mouse was an elaborate affair, with an array of ball bearings supporting the roller ball. But there was too much friction on the top of the ball, and it couldn’t deal with dust and grime.

At first, Hovey set to work with various arrangements of ball bearings, but nothing quite worked. “This was the ‘aha’ moment,” Hovey said, placing his fingers loosely around the sides of the ball, so that they barely touched its surface. “So the ball’s sitting here. And it rolls. I attribute that not to the table but to the oldness of the building. The floor’s not level. So I started playing with it, and that’s when I realized: I want it to roll. I don’t want it to be supported by all kinds of ball bearings. I want to just barely touch it.”

The trick was to connect the ball to the rest of the mouse at the two points where there was the least friction—right where his fingertips had been, dead center on either side of the ball. “If it’s right at midpoint, there’s no force causing it to rotate. So it rolls.”

Hovey estimated their consulting fee at thirty-five dollars an hour; the whole project cost perhaps a hundred thousand dollars. “I originally pitched Apple on doing this mostly for royalties, as opposed to a consulting job,” he recalled. “I said, ‘I’m thinking fifty cents apiece,’ because I was thinking that they’d sell fifty thousand, maybe a hundred thousand of them.” He burst out laughing, because of how far off his estimates ended up being. ‘s pretty savvy. He said no. Maybe if I’d asked for a nickel, I would have been fine.”

3.

Here is the first complicating fact about the Jobs visit. In the legend of Xerox PARC, Jobs stole the personal computer from Xerox. But the striking thing about Jobs’s instructions to Hovey is that he didn’t want to reproduce what he saw at PARC. “You know, there were disputes around the number of buttons—three buttons, two buttons, one-button mouse,” Hovey went on. “The mouse at Xerox had three buttons. But we came around to the fact that learning to mouse is a feat in and of itself, and to make it as simple as possible, with just one button, was pretty important.”

So was what Jobs took from Xerox the idea of the mouse? Not quite, because Xerox never owned the idea of the mouse. The PARC researchers got it from the computer scientist Douglas Engelbart, at Stanford Research Institute, fifteen minutes away on the other side of the university campus. Engelbart dreamed up the idea of moving the cursor around the screen with a stand-alone mechanical “animal” back in the mid- nineteen-sixties. His mouse was a bulky, rectangular affair, with what looked like steel roller-skate wheels. If you lined up Engelbart’s mouse, Xerox’s mouse, and Apple’s mouse, you would not see the serial reproduction of an object. You would see the evolution of a concept.

The same is true of the graphical user interface that so captured Jobs’s imagination. Xerox PARC’s innovation had been to replace the traditional computer command line with onscreen icons. But when you clicked on an icon you got a pop-up menu: this was the intermediary between the user’s intention and the computer’s response. Jobs’s software team took the graphical interface a giant step further. It emphasized “direct manipulation.” If you wanted to make a window bigger, you just pulled on its corner and made it bigger; if you wanted to move a window across the screen, you just grabbed it and moved it. The Apple designers also invented the menu bar, the pull-down menu, and the trash can—all features that radically simplified the original Xerox PARC idea.

The difference between direct and indirect manipulation—between three buttons and one button, three hundred dollars and fifteen dollars, and a roller ball supported by ball bearings and a free-rolling ball—is not trivial. It is the difference between something intended for experts, which is what Xerox PARC had in mind, and something that’s appropriate for a mass audience, which is what Apple had in mind. PARC was building a personal computer. Apple wanted to build a popular computer.

In a recent study, “The Culture of Military Innovation,” the military scholar Dima Adamsky makes a similar argument about the so-called Revolution in Military Affairs. R.M.A. refers to the way armies have transformed themselves with the tools of the digital age—such as precision-guided missiles, surveillance drones, and real-time command, control, and communications technologies—and Adamsky begins with the simple observation that it is impossible to determine who invented R.M.A. The first people to imagine how digital technology would transform warfare were a cadre of senior military intellectuals in the Soviet Union, during the nineteen-seventies. The first country to come up with these high-tech systems was the United States. And the first country to use them was Israel, in its 1982 clash with the Syrian Air Force in Lebanon’s Bekaa Valley, a battle commonly referred to as “the Bekaa Valley turkey shoot.” Israel coördinated all the major innovations of R.M.A. in a manner so devastating that it destroyed nineteen surface-to-air batteries and eighty-seven Syrian aircraft while losing only a handful of its own planes.

That’s three revolutions, not one, and Adamsky’s point is that each of these strands is necessarily distinct, drawing on separate skills and circumstances. The Soviets had a strong, centralized military bureaucracy, with a long tradition of theoretical analysis. It made sense that they were the first to understand the military implications of new information systems. But they didn’t do anything with it, because centralized military bureaucracies with strong intellectual traditions aren’t very good at connecting word and deed.

The United States, by contrast, has a decentralized, bottom-up entrepreneurial culture, which has historically had a strong orientation toward technological solutions. The military’s close ties to the country’ high-tech community made it unsurprising that the U.S. would be the first to invent precision-guidance and next-generation command-and-control communications. But those assets also meant that Soviet-style systemic analysis wasn’t going to be a priority. As for the Israelis, their military culture grew out of a background of resource constraint and constant threat. In response, they became brilliantly improvisational and creative. But, as Adamsky points out, a military built around urgent, short-term “fire extinguishing” is not going to be distinguished by reflective theory. No one stole the revolution. Each party viewed the problem from a different perspective, and carved off a different piece of the puzzle.

In the history of the mouse, Engelbart was the Soviet Union. He was the visionary, who saw the mouse before anyone else did. But visionaries are limited by their visions. “Engelbart’s self-defined mission was not to produce a product, or even a prototype; it was an open-ended search for knowledge,” Matthew Hiltzik writes, in “Dealers of Lightning” (1999), his wonderful history of Xerox PARC. “Consequently, no project in his lab ever seemed to come to an end.” Xerox PARC was the United States: it was a place where things got made. “Xerox created this perfect environment,” recalled Bob Metcalfe, who worked there through much of the nineteen-seventies, before leaving to found the networking company 3Com. “There wasn’t any hierarchy. We built out our own tools. When we needed to publish papers, we built a printer. When we needed to edit the papers, we built a computer. When we needed to connect computers, we figured out how to connect them. We had big budgets. Unlike many of our brethren, we didn’t have to teach. We could just research. It was heaven.”

But heaven is not a good place to commercialize a product. “We built a computer and it was a beautiful thing,” Metcalfe went on. “We developed our computer language, our own display, our own language. It was a gold-plated product. But it cost sixteen thousand dollars, and it needed to cost three thousand dollars.” For an actual product, you need threat and constraint—and the improvisation and creativity necessary to turn a gold-plated three-hundred-dollar mouse into something that works on Formica and costs fifteen dollars. Apple was Israel.

Xerox couldn’t have been I.B.M. and Microsoft combined, in other words. “You can be one of the most successful makers of enterprise technology products the world has ever known, but that doesn’t mean your instincts will carry over to the consumer market,” the tech writer Harry McCracken recently wrote. “They’re really different, and few companies have ever been successful in both.” He was talking about the decision by the networking giant Cisco System, this spring, to shut down its Flip camera business, at a cost of many hundreds of millions of dollars. But he could just as easily have been talking about the Xerox of forty years ago, which was one of the most successful makers of enterprise technology the world has ever known. The fair question is whether Xerox, through its research arm in Palo Alto, found a better way to be Xerox—and the answer is that it did, although that story doesn’t get told nearly as often.

4.

One of the people at Xerox PARC when Steve Jobs visited was an optical engineer named Gary Starkweather. He is a solid and irrepressibly cheerful man, with large, practical hands and the engineer’s gift of pretending that what is impossibly difficult is actually pretty easy, once you shave off a bit here, and remember some of your high-school calculus, and realize that the thing that you thought should go in left to right should actually go in right to left. Once, before the palatial Coyote Hill Road building was constructed, a group that Starkweather had to be connected to was moved to another building, across the Foothill Expressway, half a mile away. There was no way to run a cable under the highway. So Starkweather fired a laser through the air between the two buildings, an improvised communications system that meant that, if you were driving down the Foothill Expressway on a foggy night and happened to look up, you might see a mysterious red beam streaking across the sky. When a motorist drove into the median ditch, “we had to turn it down,” Starkweather recalled, with a mischievous smile.

Lasers were Starkweather’s specialty. He started at Xerox’s East Coast research facility in Webster, New York, outside Rochester. Xerox built machines that scanned a printed page of type using a photographic lens, and then printed a duplicate. Starkweather’s idea was to skip the first step—to run a document from a computer directly into a photocopier, by means of a laser, and turn the Xerox machine into a printer. It was a radical idea. The printer, since Gutenberg, had been limited to the function of re-creation: if you wanted to print a specific image or letter, you had to have a physical character or mark corresponding to that image or letter. What Starkweather wanted to do was take the array of bits and bytes, ones and zeros that constitute digital images, and transfer them straight into the guts of a copier. That meant, at least in theory, that he could print anything.

“One morning, I woke up and I thought, Why don’t we just print something out directly?” Starkweather said. “But when I flew that past my boss he thought it was the most brain-dead idea he had ever heard. He basically told me to find something else to do. The feeling was that lasers were too expensive. They didn’t work that well. Nobody wants to do this, computers aren’t powerful enough. And I guess, in my naïveté, I kept thinking, He’s just not right—there’s something about this I really like. It got to be a frustrating situation. He and I came to loggerheads over the thing, about late 1969, early 1970. I was running my experiments in the back room behind a black curtain. I played with them when I could. He threatened to lay off my people if I didn’t stop. I was having to make a decision: do I abandon this, or do I try and go up the ladder with it?”

Then Starkweather heard that Xerox was opening a research center in Palo Alto, three thousand miles away from its New York headquarters. He went to a senior vice-president of Xerox, threatening to leave for I.B.M. if he didn’t get a transfer. In January of 1971, his wish was granted, and, within ten months, he had a prototype up and running.

Starkweather is retired now, and lives in a gated community just north of Orlando, Florida. When we spoke, he was sitting at a picnic table, inside a screened-in porch in his back yard. Behind him, golfers whirred by in carts. He was wearing white chinos and a shiny black short-sleeved shirt, decorated with fluorescent images of vintage hot rods. He had brought out two large plastic bins filled with the artifacts of his research, and he spread the contents on the table: a metal octagonal disk, sketches on lab paper, a black plastic laser housing that served as the innards for one of his printers.

“There was still a tremendous amount of opposition from the Webster group, who saw no future in computer printing,” he went on. “They said, ‘I.B.M. is doing that. Why do we need to do that?’ and so forth. Also, there were two or three competing projects, which I guess I have the luxury of calling ridiculous. One group had fifty people and another had twenty. I had two.” Starkweather picked up a picture of one of his in-house competitors, something called an “optical carriage printer.” It was the size of one of those modular Italian kitchen units that you see advertised in fancy design magazines. “It was an unbelievable device,” he said, with a rueful chuckle. “It had a ten-inch drum, which turned at five thousand r.p.m., like a super washing machine. It had characters printed on its surface. I think they only ever sold ten of them. The problem was that it was spinning so fast that the drum would blow out and the characters would fly off. And there was only this one lady in Troy, New York, who knew how to put the characters on so that they would stay.

“So we finally decided to have what I called a fly-off. There was a full page of text—where some of them were non-serif characters, Helvetica, stuff like that—and then a page of graph paper with grid lines, and pages with pictures and some other complex stuff—and everybody had to print all six pages. Well, once we decided on those six pages, I knew I’d won, because I knew there wasn’t anything I couldn’t print. Are you kidding? If you can translate it into bits, I can print it. Some of these other machines had to go through hoops just to print a curve. A week after the fly-off, they folded those other projects. I was the only game in town.” The project turned into the Xerox 9700, the first high-speed, cut-paper laser printer in the world.

5.

In one sense, the Starkweather story is of a piece with the Steve Jobs visit. It is an example of the imaginative poverty of Xerox management. Starkweather had to hide his laser behind a curtain. He had to fight for his transfer to PARC. He had to endure the indignity of the fly-off, and even then Xerox management remained skeptical. The founder of PARC, Jack Goldman, had to bring in a team from Rochester for a personal demonstration. After that, Starkweather and Goldman had an idea for getting the laser printer to market quickly: graft a laser onto a Xerox copier called the 7000. The 7000 was an older model, and Xerox had lots of 7000s sitting around that had just come off lease. Goldman even had a customer ready: the Lawrence Livermore laboratory was prepared to buy a whole slate of the machines. Xerox said no. Then Starkweather wanted to make what he called a photo-typesetter, which produced camera-ready copy right on your desk. Xerox said no. “I wanted to work on higher-performance scanners,” Starkweather continued. “In other words, what if we print something other than documents? For example, I made a high-resolution scanner and you could print on glass plates.” He rummaged in one of the boxes on the picnic table and came out with a sheet of glass, roughly six inches square, on which a photograph of a child’s face appeared. The same idea, he said, could have been used to make “masks” for the semiconductor industry—the densely patterned screens used to etch the designs on computer chips. “No one would ever follow through, because Xerox said, ‘Now you’re in Intel’s market, what are you doing that for?’ They just could not seem to see that they were in the information business. This”—he lifted up the plate with the little girl’s face on it— “is a copy. It’s just not a copy of an office document.” But he got nowhere. “Xerox had been infested by a bunch of spreadsheet experts who thought you could decide every product based on metrics. Unfortunately, creativity wasn’t on a metric.”

A few days after that afternoon in his back yard, however, Starkweather e-mailed an addendum to his discussion of his experiences at PARC. “Despite all the hassles and risks that happened in getting the laser printer going, in retrospect the journey was that much more exciting,” he wrote. “Often difficulties are just opportunities in disguise.” Perhaps he felt that he had painted too negative a picture of his time at Xerox, or suffered a pang of guilt about what it must have been like to be one of those Xerox executives on the other side of the table. The truth is that Starkweather was a difficult employee. It went hand in hand with what made him such an extraordinary innovator. When his boss told him to quit working on lasers, he continued in secret. He was disruptive and stubborn and independent-minded—and he had a thousand ideas, and sorting out the good ideas from the bad wasn’t always easy. Should Xerox have put out a special order of laser printers for Lawrence Livermore, based on the old 7000 copier? In “Fumbling the Future: How Xerox Invented, Then Ignored, the First Personal Computer” (1988)—a book dedicated to the idea that Xerox was run by the blind—Douglas Smith and Robert Alexander admit that the proposal was hopelessly impractical: “The scanty Livermore proposal could not justify the investment required to start a laser printing business…. How and where would Xerox manufacture the laser printers? Who would sell and service them? Who would buy them and why?” Starkweather, and his compatriots at Xerox PARC, weren’t the source of disciplined strategic insights. They were wild geysers of creative energy.

The psychologist Dean Simonton argues that this fecundity is often at the heart of what distinguishes the truly gifted. The difference between Bach and his forgotten peers isn’t necessarily that he had a better ratio of hits to misses. The difference is that the mediocre might have a dozen ideas, while Bach, in his lifetime, created more than a thousand full-fledged musical compositions. A genius is a genius, Simonton maintains, because he can put together such a staggering number of insights, ideas, theories, random observations, and unexpected connections that he almost inevitably ends up with something great. “Quality,” Simonton writes, is “a probabilistic function of quantity.”

Simonton’s point is that there is nothing neat and efficient “The more successes there are,” he says, “the more failures there are as well” — meaning that the person who had far more ideas than the rest of us will have far more bad ideas than the rest of us, too. This is why managing the creative process is so difficult. The making of the classic Rolling Stones album “Exile on Main Street” was an ordeal, Keith Richards writes in his new memoir, because the band had too many ideas. It had to fight from under an avalanche of mediocrity: “Head in the Toilet Blues,” “Leather Jackets,” “Windmill,” “I Was Just a Country Boy,” “Bent Green Needles,” “Labour Pains,” and “Pommes de Terre”—the last of which Richards explains with the apologetic, “Well, we were in France at the time.”

At one point, Richards quotes a friend, Jim Dickinson, remembering the origins of the song “Brown Sugar”:

I watched Mick write the lyrics. . . . He wrote it down as fast as he could move his hand. I’d never seen anything like it. He had one of those yellow legal pads, and he’d write a verse a page, just write a verse and then turn the page, and when he had three pages filled, they started to cut it. It was amazing.

Richards goes on to marvel, “It’s unbelievable how prolific he was.” Then he writes, “Sometimes you’d wonder how to turn the fucking tap off. The odd times he would come out with so many lyrics, you’re crowding the airwaves, boy.” Richards clearly saw himself as the creative steward of the Rolling Stones (only in a rock-and-roll band, by the way, can someone like Keith Richards perceive himself as the responsible one), and he came to understand that one of the hardest and most crucial parts of his job was to “turn the fucking tap off,” to rein in Mick Jagger’s incredible creative energy.

The more Starkweather talked, the more apparent it became that his entire career had been a version of this problem. Someone was always trying to turn his tap off. But someone had to turn his tap off: the interests of the innovator aren’t perfectly aligned with the interests of the corporation. Starkweather saw ideas on their own merits. Xerox was a multinational corporation, with shareholders, a huge sales force, and a vast corporate customer base, and it needed to consider every new idea within the context of what it already had.

Xerox’s managers didn’t always make the right decisions when they said no to Starkweather. But he got to PARC, didn’t he? And Xerox, to its great credit, had a PARC—a place where, a continent away from the top managers, an engineer could sit and dream, and get every purchase order approved, and fire a laser across the Foothill Expressway if he was so inclined. Yes, he had to pit his laser printer against lesser ideas in the contest. But he won the contest. And, the instant he did, Xerox cancelled the competing projects and gave him the green light.

“I flew out there and gave a presentation to them on what I was looking at,” Starkweather said of his first visit to PARC. “They really liked it, because at the time they were building a personal computer, and they were beside themselves figuring out how they were going to get whatever was on the screen onto a sheet of paper. And when I showed them how I was going to put prints on a sheet of paper it was a marriage made in heaven.” The reason Xerox invented the laser printer, in other words, is that it invented the personal computer. Without the big idea, it would never have seen the value of the small idea. If you consider innovation to be efficient and ideas precious, that is a tragedy: you give the crown jewels away to Steve Jobs, and all you’re left with is a printer. But in the real, messy world of creativity, giving away the thing you don’t really understand for the thing that you do is an inevitable tradeoff.

“When you have a bunch of smart people with a broad enough charter, you will always get something good out of it,” Nathan Myhrvold, formerly a senior executive at Microsoft, argues. “It’s one of the best investments you could possibly make—but only if you chose to value it in terms of successes. If you chose to evaluate it in terms of how many times you failed, or times you could have succeeded and didn’t, then you are bound to be unhappy. Innovation is an unruly thing. There will be some ideas that don’t get caught in your cup. But that’s not what the game is about. The game is what you catch, not what you spill.”

In the nineteen-nineties, Myhrvold created a research laboratory at Microsoft modelled in part on what Xerox had done in Palo Alto in the nineteen-seventies, because he considered PARC a triumph, not a failure. “Xerox did research outside their business model, and when you do that you should not be surprised that you have a hard time dealing with it—any more than if some bright guy at Pfizer wrote a word processor. Good luck to Pfizer getting into the word-processing business. Meanwhile, the thing that they invented that was similar to their own business—a really big machine that spit paper —they made a lot of money on it.” And so they did. Gary Starkweather’s laser printer made billions for Xerox. It paid for every other single project at Xerox PARC, many times over.

6.

In 1988, Starkweather got a call from the head of one of Xerox’s competitors, trying to lure him away. It was someone whom he had met years ago. “The decision was painful,” he said. “I was a year from being a twenty-five-year veteran of the company. I mean, I’d done enough for Xerox that unless I burned the building down they would never fire me. But that wasn’t the issue. It’s about having ideas that are constantly squashed. So I said, ‘Enough of this,’ and I left.”

He had a good many years at his new company, he said. It was an extraordinarily creative place. He was part of decision-making at the highest level. “Every employee from technician to manager was hot for the new, exciting stuff,” he went on. “So, as far as buzz and daily environment, it was far and away the most fun I’ve ever had.” But it wasn’t perfect. “I remember I called in the head marketing guy and I said, ‘I want you to give me all the information you can come up with on when people buy one of our products—what software do they buy, what business are they in—so I can see the model of how people are using the machines.’ He looked at me and said, ‘I have no idea about that.’ ” Where was the rigor? Then Starkweather had a scheme for hooking up a high-resolution display to one of his new company’s computers. “I got it running and brought it into management and said, ‘Why don’t we show this at the tech expo in San Francisco? You’ll be able to rule the world.’ They said, ‘I don’t know. We don’t have room for it.’ It was that sort of thing. It was like me saying I’ve discovered a gold mine and you saying we can’t afford a shovel.”

He shrugged a little wearily. It was ever thus. The innovator says go. The company says stop—and maybe the only lesson of the legend of Xerox PARC is that what happened there happens, in one way or another, everywhere. By the way, the man who hired Gary Starkweather away to the company that couldn’t afford a shovel? His name was Steve Jobs.

© 2013 Malcolm Gladwell.

DAVID AND GOLIATH
 
WHAT THE DOG SAW
 
OUTLIERS
 
BLINK
 
THE TIPPING POINT
 
ARTICLES
 
ETC.
Priced to Sell

Posted July 6, 2009 by MALCOLM GLADWELL & filed under BOOKS, THE NEW YORKER - ARCHIVE.

Is free the future?

1.

At a hearing on Capitol Hill in May, James Moroney, the publisher of the Dallas Morning News, told Congress about negotiations he’d just had with the online retailer Amazon. The idea was to license his newspaper’s content to the Kindle, Amazon’s new electronic reader. “They want seventy per cent of the subscription revenue,” Moroney testified. “”I get thirty per cent, they get seventy per cent. On top of that, they have said we get the right to republish your intellectual property to any portable device.” The idea was that if a Kindle subscription to the Dallas Morning News cost ten dollars a month, seven dollars of that belonged to Amazon, the provider of the gadget on which the news was read, and just three dollars belonged to the newspaper, the provider of an expensive and ever-changing variety of editorial content. The people at Amazon valued the newspaper’s contribution so little, in fact, that they felt they ought then to be able to license it to anyone else they wanted. Another witness at the hearing, Arianna Huffington, of the Huffington Post, said that she thought the Kindle could provide a business model to save the beleaguered newspaper industry. Moroney disagreed. “I get thirty per cent and they get the right to license my content to any portable device—not just ones made by Amazon?” He was incredulous. “That, to me, is not a model.”

Had James Moroney read Chris Anderson’s new book, “Free: The Future of a Radical Price” (Hyperion; $26.99), Amazon’s offer might not have seemed quite so surprising. Anderson is the editor of Wired and the author of the 2006 best-seller “The Long Tail,” and “Free” is essentially an extended elaboration of Stewart Brand’s famous declaration that “information wants to be free.” The digital age, Anderson argues, is exerting an inexorable downward pressure on the prices of all things “made of ideas.” Anderson does not consider this a passing trend. Rather, he seems to think of it as an iron law: “In the digital realm you can try to keep Free at bay with laws and locks, but eventually the force of economic gravity will win.” To musicians who believe that their music is being pirated, Anderson is blunt. They should stop complaining, and capitalize on the added exposure that piracy provides by making money through touring, merchandise sales, and “yes, the sale of some of [their] music to people who still want CDs or prefer to buy their music online.” To the Dallas Morning News, he would say the same thing. Newspapers need to accept that content is never again going to be worth what they want it to be worth, and reinvent their business. “Out of the bloodbath will come a new role for professional journalists,” he predicts, and he goes on:

There may be more of them, not fewer, as the ability to participate in journalism extends beyond the credentialed halls of traditional media. But they may be paid far less, and for many it won’t be a full time job at all. Journalism as a profession will share the stage with journalism as an avocation. Meanwhile, others may use their skills to teach and organize amateurs to do a better job covering their own communities, becoming more editor/coach than writer. If so, leveraging the Free—paying people to get other people to write for non-monetary rewards—may not be the enemy of professional journalists. Instead, it may be their salvation.

Anderson is very good at paragraphs like this—with its reassuring arc from “bloodbath” to “salvation.” His advice is pithy, his tone uncompromising, and his subject matter perfectly timed for a moment when old-line content providers are desperate for answers. That said, it is not entirely clear what distinction is being marked between “paying people to get other people to write” and paying people to write. If you can afford to pay someone to get other people to write, why can’t you pay people to write? It would be nice to know, as well, just how a business goes about reorganizing itself around getting people to work for “non-monetary rewards.” Does he mean that the New York Times should be staffed by volunteers, like Meals on Wheels? Anderson’s reference to people who “prefer to buy their music online” carries the faint suggestion that refraining from theft should be considered a mere preference. And then there is his insistence that the relentless downward pressure on prices represents an iron law of the digital economy. Why is it a law? Free is just another price, and prices are set by individual actors, in accordance with the aggregated particulars of marketplace power. “Information wants to be free,” Anderson tells us, “in the same way that life wants to spread and water wants to run downhill.” But information can’t actually want anything, can it? Amazon wants the information in the Dallas paper to be free, because that way Amazon makes more money. Why are the self-interested motives of powerful companies being elevated to a philosophical principle? But we are getting ahead of ourselves.

2.

Anderson’s argument begins with a technological trend. The cost of the building blocks of all electronic activity—storage, processing, and bandwidth—has fallen so far that it is now approaching zero. In 1961, Anderson says, a single transistor was ten dollars. In 1963, it was five dollars. By 1968, it was one dollar. Today, Intel will sell you two billion transistors for eleven hundred dollars—meaning that the cost of a single transistor is now about .000055 cents.

Anderson’s second point is that when prices hit zero extraordinary things happen. Anderson describes an experiment conducted by the M.I.T. behavioral economist Dan Ariely, the author of “Predictably Irrational.” Ariely offered a group of subjects a choice between two kinds of chocolate—Hershey’s Kisses, for one cent, and Lindt truffles, for fifteen cents. Three-quarters of the subjects chose the truffles. Then he redid the experiment, reducing the price of both chocolates by one cent. The Kisses were now free. What happened? The order of preference was reversed. Sixty-nine per cent of the subjects chose the Kisses. The price difference between the two chocolates was exactly the same, but that magic word “free” has the power to create a consumer stampede. Amazon has had the same experience with its offer of free shipping for orders over twenty-five dollars. The idea is to induce you to buy a second book, if your first book comes in at less than the twenty-five-dollar threshold. And that’s exactly what it does. In France, however, the offer was mistakenly set at the equivalent of twenty cents—and consumers didn’t buy the second book. “From the consumer’s perspective, there is a huge difference between cheap and free,” Anderson writes. “Give a product away, and it can go viral. Charge a single cent for it and you’re in an entirely different business. . . . The truth is that zero is one market and any other price is another.”

Since the falling costs of digital technology let you make as much stuff as you want, Anderson argues, and the magic of the word “free” creates instant demand among consumers, then Free (Anderson honors it with a capital) represents an enormous business opportunity. Companies ought to be able to make huge amounts of money “around” the thing being given away—as Google gives away its search and e-mail and makes its money on advertising.

Anderson cautions that this philosophy of embracing the Free involves moving from a “scarcity” mind-set to an “abundance” mind-set. Giving something away means that a lot of it will be wasted. But because it costs almost nothing to make things, digitally, we can afford to be wasteful. The elaborate mechanisms we set up to monitor and judge the quality of content are, Anderson thinks, artifacts of an era of scarcity: we had to worry about how to allocate scarce resources like newsprint and shelf space and broadcast time. Not anymore. Look at YouTube, he says, the free video archive owned by Google. YouTube lets anyone post a video to its site free, and lets anyone watch a video on its site free, and it doesn’t have to pass judgment on the quality of the videos it archives. “Nobody is deciding whether a video is good enough to justify the scarce channel space it takes, because there is no scarce channel space,” he writes, and goes on:

Distribution is now close enough to free to round down. Today, it costs about $0.25 to stream one hour of video to one person. Next year, it will be $0.15. A year later it will be less than a dime. Which is why YouTube’s founders decided to give it away. . . . The result is both messy and runs counter to every instinct of a television professional, but this is what abundance both requires and demands.

There are four strands of argument here: a technological claim (digital infrastructure is effectively Free), a psychological claim (consumers love Free), a procedural claim (Free means never having to make a judgment), and a commercial claim (the market created by the technological Free and the psychological Free can make you a lot of money). The only problem is that in the middle of laying out what he sees as the new business model of the digital age Anderson is forced to admit that one of his main case studies, YouTube, “has so far failed to make any money for Google.”

Why is that? Because of the very principles of Free that Anderson so energetically celebrates. When you let people upload and download as many videos as they want, lots of them will take you up on the offer. That’s the magic of Free psychology: an estimated seventy-five billion videos will be served up by YouTube this year. Although the magic of Free technology means that the cost of serving up each video is “close enough to free to round down,” “close enough to free” multiplied by seventy-five billion is still a very large number. A recent report by Credit Suisse estimates that YouTube’s bandwidth costs in 2009 will be three hundred and sixty million dollars. In the case of YouTube, the effects of technological Free and psychological Free work against each other.

So how does YouTube bring in revenue? Well, it tries to sell advertisements alongside its videos. The problem is that the videos attracted by psychological Free—pirated material, cat videos, and other forms of user-generated content—are not the sort of thing that advertisers want to be associated with. In order to sell advertising, YouTube has had to buy the rights to professionally produced content, such as television shows and movies. Credit Suisse put the cost of those licenses in 2009 at roughly two hundred and sixty million dollars. For Anderson, YouTube illustrates the principle that Free removes the necessity of aesthetic judgment. (As he puts it, YouTube proves that “crap is in the eye of the beholder.”) But, in order to make money, YouTube has been obliged to pay for programs that aren’t crap. To recap: YouTube is a great example of Free, except that Free technology ends up not being Free because of the way consumers respond to Free, fatally compromising YouTube’s ability to make money around Free, and forcing it to retreat from the “abundance thinking” that lies at the heart of Free. Credit Suisse estimates that YouTube will lose close to half a billion dollars this year. If it were a bank, it would be eligible for TARP funds.

3.

Anderson begins the second part of his book by quoting Lewis Strauss, the former head of the Atomic Energy Commission, who famously predicted in the mid-nineteen-fifties that “our children will enjoy in their homes electrical energy too cheap to meter.”

“What if Strauss had been right?” Anderson wonders, and then diligently sorts through the implications: as much fresh water as you could want, no reliance on fossil fuels, no global warming, abundant agricultural production. Anderson wants to take “too cheap to meter” seriously, because he believes that we are on the cusp of our own “too cheap to meter” revolution with computer processing, storage, and bandwidth. But here is the second and broader problem with Anderson’s argument: he is asking the wrong question. It is pointless to wonder what would have happened if Strauss’s prediction had come true while rushing past the reasons that it could not have come true.

Strauss’s optimism was driven by the fuel cost of nuclear energy—which was so low compared with its fossil-fuel counterparts that he considered it (to borrow Anderson’s phrase) close enough to free to round down. Generating and distributing electricity, however, requires a vast and expensive infrastructure of transmission lines and power plants—and it is this infrastructure that accounts for most of the cost of electricity. Fuel prices are only a small part of that. As Gordon Dean, Strauss’s predecessor at the A.E.C., wrote, ” ” Even if coal were mined and distributed free to electric generating plants today, the reduction in your monthly electricity bill would amount to but twenty per cent, so great is the cost of the plant itself and the distribution system.”

This is the kind of error that technological utopians make. They assume that their particular scientific revolution will wipe away all traces of its predecessors—that if you change the fuel you change the whole system. Strauss went on to forecast “an age of peace,” jumping from atoms to human hearts. “As the world of chips and glass fibers and wireless waves goes, so goes the rest of the world,” Kevin Kelly, another Wired visionary, proclaimed at the start of his 1998 digital manifesto, “New Rules for the New Economy,” offering up the same non sequitur. And now comes Anderson. “The more products are made of ideas, rather than stuff, the faster they can get cheap,” he writes, and we know what’s coming next: “However, this is not limited to digital products.” Just look at the pharmaceutical industry, he says. Genetic engineering means that drug development is poised to follow the same learning curve of the digital world, to “accelerate in performance while it drops in price.”

But, like Strauss, he’s forgotten about the plants and the power lines. The expensive part of making drugs has never been what happens in the laboratory. It’s what happens after the laboratory, like the clinical testing, which can take years and cost hundreds of millions of dollars. In the pharmaceutical world, what’s more, companies have chosen to use the potential of new technology to do something very different from their counterparts in Silicon Valley. They’ve been trying to find a way to serve smaller and smaller markets—to create medicines tailored to very specific subpopulations and strains of diseases—and smaller markets often mean higher prices. The biotechnology company Genzyme spent five hundred million dollars developing the drug Myozyme, which is intended for a condition, Pompe disease, that afflicts fewer than ten thousand people worldwide. That’s the quintessential modern drug: a high-tech, targeted remedy that took a very long and costly path to market. Myozyme is priced at three hundred thousand dollars a year. Genzyme isn’t a mining company: its real assets are intellectual property—information, not stuff. But, in this case, information does not want to be free. It wants to be really, really expensive.

And there’s plenty of other information out there that has chosen to run in the opposite direction from Free. The Times gives away its content on its Web site. But the Wall Street Journal has found that more than a million subscribers are quite happy to pay for the privilege of reading online. Broadcast television—the original practitioner of Free—is struggling. But premium cable, with its stiff monthly charges for specialty content, is doing just fine. Apple may soon make more money selling iPhone downloads (ideas) than it does from the iPhone itself (stuff). The company could one day give away the iPhone to boost downloads; it could give away the downloads to boost iPhone sales; or it could continue to do what it does now, and charge for both. Who knows? The only iron law here is the one too obvious to write a book about, which is that the digital age has so transformed the ways in which things are made and sold that there are no iron laws.

© 2013 Malcolm Gladwell.

DAVID AND GOLIATH
 
WHAT THE DOG SAW
 
OUTLIERS
 
BLINK
 
THE TIPPING POINT
 
ARTICLES
 
ETC.
Clicks and Mortar

Posted December 6, 1999 by MALCOLM GLADWELL & filed under ANNALS OF RETAIL, THE NEW YORKER - ARCHIVE.

Don’t believe the Internet hype: the real E-commerce revolution happened off-line.

1.

At the turn of this century, a Missouri farmer named D.Ward King invented a device that came to be known, in his honor, as the King Road Drag. It consisted of two wooden rails that lay side by side about three feet apart, attached by a series of wooden braces. If you pulled the King Drag along a muddy road, it had the almost magical effect of smoothing out the ruts and molding the dirt into a slight crown, so that the next time it rained the water would drain off to the sides. In 1906, when King demonstrated his device to a group of farmers in Wellsville, Kansas, the locals went out and built a hundred King Drags of their own within the week, which makes sense, because if you had asked a farmer at the turn of the century what single invention could make his life easier he would probably have wanted something that improved the roads. They were, in the late nineteenth century, a disaster: of the country’s two million miles of roads, fewer than a hundred and fifty thousand had been upgraded with gravel or oil. The rest were dirt. They turned into rivers of mud when it was raining, and hardened into an impassable sea of ruts when it was not. A trip to church or to go shopping was an exhausting ordeal for many farmers. At one point in the early part of this century, economists estimated that it cost more to haul a bushel of wheat along ten miles of American dirt road than it did to ship it across the ocean from New York to Liverpool.

The King Road Drag was a simple invention that had the effect of reducing the isolation of the American farmer, and soon that simple invention led to all kinds of dramatic changes. Ever since the Post Office was established, for example, farmers had to make the difficult trek into town to pick up their mail. In the eighteen-nineties, Congress pledged that mail would be delivered free to every farmer’s home, but only so long as rural communities could demonstrate that their roads were good enough for a mailman to pass by every day–which was a Catch-22 neatly resolved by the King Road Drag. And once you had rural free delivery and good roads, something like parcel post became inevitable. Through the beginning of the century, all packages that weighed more than four pounds were carried by private-express services, which were unreliable and expensive and would, outside big cities, deliver only to a set of depots. But if the mail was being delivered every day to rural dwellers, why not have the mailman deliver packages, too? In 1912, Congress agreed, and with that the age of the mail-order house began: now a farmer could look through a catalogue that contained many thousands of products and have them delivered right to his door. Smaller companies, with limited resources, had a way to bypass the middleman and reach customers all over the country. You no longer needed to sell to the consumer through actual stores made of bricks and mortar. You could build a virtual store!

In the first fifteen years of this century, in other words, America underwent something of a revolution. Before rural free delivery, if you didn’t live in a town–and most Americans didn’t–it wasn’t really practical to get a daily newspaper. It was only after daily delivery that the country became “wired,” in the sense that if something happened in Washington or France or the Congo one evening, everyone would know about it by the next morning. In 1898, mailmen were delivering about eighteen thousand pieces of mail per rural route. Within five years, that number had more than doubled, and by 1929 it had topped a hundred thousand.

Here was the dawn of the modern consumer economy–an economy in which information moved freely around the country, in which retailers and consumers, buyers and sellers became truly connected for the first time. “You may go to an average store, spend valuable time and select from a limited stock at retail prices,” the fall 1915 Sears, Roebuck catalogue boasted, “or have our Big Store of World Wide Stocks at Economy Prices come to you in this catalog–the Modern Way.” By the turn of the century, the Sears catalogue had run to over a thousand pages, listing tens of thousands of items in twenty-four departments: music, buggies, stoves, carriage hardware, drugs, vehicles, shoes, notions, sewing machines, cloaks, sporting goods, dry goods, hardware, groceries, furniture and baby carriages, jewelry, optical goods, books, stereopticons, men’s clothing, men’s furnishings, bicycles, gramophones, and harnesses. Each page was a distinct site, offering a reader in- depth explanations and descriptions well beyond what he would expect if he went to a store, talked to a sales clerk, and personally examined a product. To find all those products, the company employed scores of human search engines–”missionaries” who, the historians Boris Emmet and John Jeuck write, were “said to travel constantly, inspecting the stocks of virtually all retail establishments in the country, conversing with the public at large to discover their needs and desires, and buying goods ‘of all kinds and descriptions’” in order to post them on the World Wide Stock.

The catalogue, as economists have argued, represented a radical transformation in the marketing and distribution of consumer goods. But, of course, that transformation would not have been possible unless you had parcel post, and you couldn’t have had parcel post unless you had rural free delivery, and you could not have had rural free delivery without good roads, and you would not have had good roads without D. Ward King. So what was the genuine revolution? Was it the World Wide Stock or was it the King Road Drag?

2.

We are now, it is said, in the midst of another business revolution. “This new economy represents a tectonic upheaval in our commonwealth, a far more turbulent reordering than mere digital hardware has produced,” Kevin Kelly, a former executive editor of Wired, writes in his book “New Rules for the New Economy.” In “Cyber Rules,” the software entrepreneurs Thomas M. Siebel and Pat House compare the advent of the Internet to the invention of writing, the appearance of a metal currency in the eastern Mediterranean several thousand years ago, and the adoption of the Arabic zero. “Business,” Bill Gates states flatly in the opening sentence of “Business @ the Speed of Thought,” “is going to change more in the next ten years than it has in the last fifty.”

The revolution of today, however, turns out to be as difficult to define as the revolution of a hundred years ago. Kelly, for example, writes that because of the Internet “the new economy is about communication, deep and wide.” Communication, he maintains, “is not just a sector of the economy. Communication is the economy.” But which is really key–how we communicate, or what we communicate? Gates, meanwhile, is preoccupied with the speed of interaction in the new economy. Going digital, he writes, will “shatter the old way of doing business” because it will permit almost instant communication. Yet why is the critical factor how quickly I communicate some decision or message to you–as opposed to how long it takes me to make that decision, or how long it takes you to act on it? Gates called his book “Business @ the Speed of Thought,” but thought is a slow and messy thing. Computers do nothing to speed up our thought process; they only make it a lot faster to communicate our thoughts once we’ve had them. Gates should have called his book “Business @ the Speed of Typing.” In “Growing Up Digital,” Don Tapscott even goes so far as to claim that the rise of the Internet has created an entirely new personality among the young. N-Geners, as Tapscott dubs the generation, have a different set of assumptions about work than their parents have. They thrive on collaboration, and many find the notion of a boss somewhat bizarre….They are driven to innovate and have a mindset of immediacy requiring fast results. They love hard work because working, learning, and playing are the same thing to them. They are creative in ways their parents could only imagine….Corporations who hire them should be prepared to have their windows and walls shaken.

Let’s leave aside the fact that the qualities Tapscott ascribes to the Net Generation–energy, a “mindset of immediacy,” creativity, a resistance to authority, and (of all things) sharp differences in outlook from their parents–could safely have been ascribed to every upcoming generation in history. What’s interesting here is the blithe assumption, which runs through so much of the thinking and talking about the Internet, that this new way of exchanging information must be at the root of all changes now sweeping through our economy and culture. In these last few weeks before Christmas, as the country’s magazines and airways become crowded with advertisements for the fledgling class of dot coms, we may be tempted to concur. But is it possible that, once again, we’ve been dazzled by the catalogues and forgotten the roads?

3.

The world’s largest on-line apparel retailer is Lands’ End, in Wisconsin. Lands’ End began in 1963 as a traditional mail-order company. It mailed you its catalogue, and you mailed back your order along with a check. Then, in the mid-nineteen-eighties, Lands’ End, like the rest of the industry, reinvented itself. It mailed you its catalogue, and you telephoned an 800 number with your order and paid with a credit card. Now Lands’ End has moved on line. In the first half of this year, E-commerce sales accounted for ten per cent of Lands’ End’s total business, up two hundred and fifty per cent from last year. What has this move to the Web meant?

Lands’ End has its headquarters in the tiny farming town of Dodgeville, about an hour’s drive west of Madison, through the rolling Midwestern countryside. The main Lands’ End campus is composed of half a dozen modern, low-slung buildings, clustered around a giant parking lot. In one of those buildings, there is a huge open room filled with hundreds of people sitting in front of computer terminals and wearing headsets. These are the people who take your orders. Since the bulk of Lands’ End’s business is still driven by the catalogue and the 800 number, most of those people are simply talking on the phone to telephone customers. But a growing percentage of the reps are now part of the company’s Internet team, serving people who use the Lands’ End Live feature on the company’s Web site. Lands’ End Live allows customers, with the click of a mouse, to start a live chat with a Lands’ End representative or get a rep to call them at home, immediately.

On a recent fall day, a Lands’ End Live user–let’s call her Betty–was talking to one of the company’s customer-service reps, a tall, red-haired woman named Darcia. Betty was on the Lands’ End Web site to buy a pair of sweatpants for her young daughter, and had phoned to ask a few questions.

“What size did I order last year?” Betty asked. “I think I need one size bigger.” Darcia looked up the record of Betty’s purchase. Last year, she told Betty, she bought the same pants in big- kid’s small.

“I’m thinking medium or large,” Betty said. She couldn’t decide.

“The medium is a ten or a twelve, really closer to a twelve,” Darcia told her. “I’m thinking if you go to a large, it will throw you up to a sixteen, which is really big.”

Betty agreed. She wanted the medium. But now she had a question about delivery. It was Thursday morning, and she needed the pants by Tuesday. Darcia told her that the order would go out on Friday morning, and with U.P.S. second-day air she would almost certainly get it by Tuesday. They briefly discussed spending an extra six dollars for the premium, next- day service, but Darcia talked Betty out of it. It was only an eighteen-dollar order, after all.

Betty hung up, her decision made, and completed her order on the Internet. Darcia started an on-line chat with a woman from the East Coast. Let’s call her Carol. Carol wanted to buy the forty-nine-dollar attaché case but couldn’t decide on a color. Darcia was partial to the dark olive, which she said was “a professional alternative to black.” Carol seemed convinced, but she wanted the case monogrammed and there were eleven monogramming styles on the Web-site page.

“Can I have a personal suggestion?” she wrote.

“Sure,” Darcia typed back. “Who is the case for?”

“A conservative psychiatrist,” Carol replied.

Darcia suggested block initials, in black. Carol agreed, and sent the order in herself on the Internet. “All right,” Darcia said, as she ended the chat. “She feels better.” The exchange had taken twenty-three minutes.

Notice that in each case the customer filled out the actual order herself and sent it in to the Lands’ End computer electronically–which is, of course, the great promise of E-commerce. But that didn’t make some human element irrelevant. The customers still needed Darcia for advice on colors, and styles, or for reassurance that their daughter was a medium and not a large. In each case, the sale was closed because that human interaction allayed the last-minute anxieties and doubts that so many of us have at the point of purchase. It’s a mistake, in other words, to think that E-commerce will entirely automate the retail process. It just turns reps from order-takers into sales advisers.

“One of the big fallacies when the Internet came along was that you could get these huge savings by eliminating customer- service costs,” Bill Bass, the head of E-commerce for Lands’ End, says. “People thought the Internet was self-service, like a gas station. But there are some things that you cannot program a computer to provide. People will still have questions, and what you get are much higher-level questions. Like, ‘Can you help me come up with a gift?’ And they take longer.”

Meanwhile, it turns out, Internet customers at Lands’ End aren’t much different from 800-number customers. Both groups average around a hundred dollars an order, and they have the same rate of returns. Call volume on the 800 numbers is highest on Mondays and Tuesdays, from ten in the morning until one in the afternoon. So is E-commerce volume. In the long term, of course, the hope is that the Web site will reduce dependence on the catalogue, and that would be a huge efficiency. Given that last year the company mailed two hundred and fifty million catalogues, costing about a dollar each, the potential savings could be enormous. And yet customers’ orders on the Internet spike just after a new catalogue arrives at people’s homes in exactly the same way that the 800-number business spikes just after the catalogue arrives. E-commerce users, it seems, need the same kind of visual, tangible prompting to use Lands’ End as traditional customers. If Lands’ End did all its business over the Internet, it would still have to send out something in the mail–a postcard or a bunch of fabric swatches or a slimmed-down catalogue. “We thought going into E-commerce it would be a different business,” Tracy Schmit, an Internet analyst at the company, says. “But it’s the same business, the same patterns, the same contacts. It’s an extension of what we already do.”

4.

Now consider what happens on what retailers call the “back end”–the customer-fulfillment side–of Lands’ End’s operations. Say you go to the company’s Web site one afternoon and order a blue 32-16 oxford-cloth button-down shirt and a pair of size-9 Top-Siders. At midnight, the computer at Lands’ End combines your order with all the other orders for the day: it lumps your shirt order with the hundred other orders, say, that came in for 32-16 blue oxford-cloth button-downs, and lumps your shoe order with the fifty other size-9 Top-Sider orders of the day. It then prints bar codes for every item, so each of those hundred shirts is assigned a sticker listing the location of blue oxford 32-16 shirts in the warehouse, the order that it belongs to, shipping information, and instructions for things like monogramming.

The next morning, someone known as a “picker” finds the hundred oxford- cloth shirts in that size, yours among them, and puts a sticker on each one, as does another picker in the shoe area with the fifty size-9 Top-Siders. Each piece of merchandise is placed on a yellow plastic tray along an extensive conveyor belt, and as the belt passes underneath a bar-code scanner the computer reads the label and assembles your order. The tray with your shirt on it circles the room until it is directly above a bin that has been temporarily assigned to you, and then tilts, sending the package sliding downward. Later, when your shoes come gliding along on the belt, the computer reads the bar code on the box and sends the shoe box tumbling into the same bin. Then the merchandise is packed and placed on another conveyor belt, and a bar-code scanner sorts the packages once again, sending the New York-bound packages to the New York-bound U.P.S. truck, the Detroit packages to the Detroit truck, and so on.

It’s an extraordinary operation. When you stand in the middle of the Lands’ End warehouse–while shirts and pants and sweaters and ties roll by at a rate that, at Christmas, can reach twenty-five thousand items an hour–you feel as if you’re in Willy Wonka’s chocolate factory. The warehouses are enormous buildings–as big, in all, as sixteen football fields–and the conveyor belts hang from the ceiling like giant pieces of industrial sculpture. Every so often, a belt lurches to a halt, and a little black scanner box reads the bar code and sends the package off again, directing it left or right or up or down, onto any number of separate sidings and overpasses. In the middle of one of the buildings, there is another huge room where thousands of pants, dangling from a jumbo-sized railing like a dry cleaner’s rack, are sorted by color (so sewers don’t have to change thread as often) and by style, then hemmed, pressed, bagged, and returned to the order-fulfillment chain–all within a day.

This system isn’t unique to Lands’ End. If you went to L. L. Bean or J.Crew or, for that matter, a housewares-catalogue company like Pottery Barn, you’d find the same kind of system. It’s what all modern, automated warehouses look like, and it is as much a part of E-commerce as a Web site. In fact, it is the more difficult part of E-commerce. Consider the problem of the Christmas rush. Lands’ End records something like thirty per cent of its sales during November and December. A well- supported Web site can easily handle those extra hits, but for the rest of the operation that surge in business represents a considerable strain. Lands’ End, for example, aims to respond to every phone call or Lands’ End Live query within twenty seconds, and to ship out every order within twenty-four hours of its receipt. In August, those goals are easily met. But, to maintain that level of service in November and December, Lands’ End must hire an extra twenty-six hundred people, increasing its normal payroll by more than fifty per cent. Since unemployment in the Madison area is hovering around one per cent, this requires elaborate planning: the company charters buses to bring in students from a nearby college, and has made a deal in the past with a local cheese factory to borrow its workforce for the rush. Employees from other parts of the company are conscripted to help out as pickers, while others act as “runners” in the customer-service department, walking up and down the aisles and jumping into any seat made vacant by someone taking a break. Even the structure of the warehouse is driven, in large part, by the demands of the holiday season. Before the popularization of the bar code, in the early nineteen- eighties, Lands’ End used what is called an “order picking” method. That meant that the picker got your ticket, then went to the shirt room and got your shirt, and the shoe room and got your shoes, then put your order together. If another shoe-and- shirt order came over next, she would have to go back to the shirts and back to the shoes all over again. A good picker under the old system could pick between a hundred and fifty and a hundred and seventy-five pieces an hour. The new technique, known as “batch picking,” is so much more efficient that a good picker can now retrieve between six hundred and seven hundred pieces an hour. Without bar codes, if you placed an order in mid-December, you’d be hard pressed to get it by Christmas.

None of this is to minimize the significance of the Internet. Lands’ End has a feature on its Web site which allows you to try clothes on a virtual image of yourself–a feature that is obviously not possible with a catalogue. The Web site can list all the company’s merchandise, whereas a catalogue has space to list only a portion of the inventory. But how big a role does the Internet ultimately play in E-commerce? It doesn’t much affect the cost of running a customer-service department. It reduces catalogue costs, but it doesn’t eliminate traditional marketing, because you still have to remind people of your Web site. You still need to master batch picking. You still need the Willy Wonka warehouse. You still need dozens of sewers in the inseaming department, and deals with the local cheese factory, and buses to ship in students every November and December. The head of operations for Lands’ End is a genial man in his fifties named Phil Schaecher, who works out of a panelled office decorated with paintings of ducks which overlooks the warehouse floor. When asked what he would do if he had to choose between the two great innovations of the past twenty years–the bar code, which has transformed the back end of his business, and the Internet, which is transforming the front end–Schaecher paused, for what seemed a long time. “I’d take the Internet,” he said finally, toeing the line that all retailers follow these days. Then he smiled. “But of course if we lost bar codes I’d retire the next day.”

5.

On a recent fall morning, a young woman named Charlene got a call from a shipping agent at a firm in Oak Creek, Wisconsin. Charlene is a dispatcher with a trucking company in Akron, Ohio, called Roberts Express. She sits in front of a computer with a telephone headset on, in a large crowded room filled with people in front of computers wearing headsets, not unlike the large crowded room at Lands’ End. The shipping agent told Charlene that she had to get seven drums of paint to Muskegon, Michigan, as soon as possible. It was 11:25 a.m. Charlene told the agent she would call her back, and immediately typed those details into her computer, which relayed the message to the two-way-communications satellite that serves as the backbone for the Roberts transportation network. The Roberts satellite, in turn, “pinged” the fifteen hundred independent truckers that Roberts works with, and calculated how far each available vehicle was from the customer in Oak Creek. Those data were then analyzed by proprietary software, which sorted out the cost of the job and the distance between Muskegon and Oak Creek, and sifted through more than fifteen variables governing the optimal distribution of the fleet.

This much–the satellite relay and the probability calculation–took a matter of seconds. The trip, Charlene’s screen told her, was two hundred and seventy-four miles and would cost seven hundred and twenty-six dollars. The computer also gave her twenty-three candidates for the run, ranked in order of preference. The first, Charlene realized, was ineligible, because federal regulations limit the number of hours drivers can spend on the road. The second, she found out, was being held for another job. The third, according to the satellite, was fifty miles away, which was too far. But the fourth, a husband- and-wife team named Jerry and Ann Love, seemed ideal. They were just nineteen miles from OakCreek. “I’ve worked with them before,” Charlene said. “They’re really nice people.” At eleven-twenty-seven, Charlene sent the Loves an E-mail message, via satellite, that would show up instantly on the computer screens Roberts installs in the cabs of all its contractors. According to Roberts’ rules, they had ten minutes to respond. “I’m going to give them a minute or two,” Charlene said. There was no answer, so she called the Loves on their cell phone. Ann Love answered. “We’ll do that,” she said. Charlene chatted with her for a moment and then, as an afterthought, E-mailed the Loves again: “Thank you!” It was eleven-thirty.

Trucking companies didn’t work this way twenty years ago. But Roberts uses its state-of-the-art communications and computer deployment to give the shipping business a new level of precision. If your pickup location is within twenty-five miles of one of the company’s express centers–and Roberts has express centers in most major North American cities–Roberts will pick up a package of almost any size within ninety minutes, and it will do so twenty-four hours a day, seven days a week. If the cargo is located between twenty-six and fifty miles of an express center, it will be picked up within two hours. More than half of those deliveries will be made by midnight of the same day. Another twenty-five per cent will be made by eight o’clock the next morning. Ninety-six per cent of all Roberts deliveries are made within fifteen minutes of the delivery time promised when the order is placed. Because of its satellite system, the company knows precisely, within yards, where your order is at all times. The minute the computer tells her your truck is running fifteen minutes behind, Charlene or one of her colleagues will call you to work out some kind of solution. Roberts has been known to charter planes or send in Huey helicopters to rescue time-sensitive cargo stranded in traffic or in a truck that has broken down. The result is a truck-based system so efficient that Roberts estimates it can outperform air freight at distances of up to seven hundred or eight hundred miles.

Roberts, of course, isn’t the only company to reinvent the delivery business over the past twenty years. In the same period, Federal Express has put together, from scratch, a network of six hundred and forty-three planes, forty-three thousand five hundred vehicles, fourteen hundred service centers, thirty-four thousand drop boxes, and a hundred and forty-eight thousand employees–all coordinated by satellite links and organized around a series of huge, automated, bar- code-driven Willy Wonka warehouses. Federal Express was even a pioneer in the development of aircraft antifog navigational equipment: if it absolutely, positively has to get there overnight, the weather can’t be allowed to get in the way.

E-commerce would be impossible without this extraordinary infrastructure. Would you care that you could order a new wardrobe with a few clicks of a mouse if the package took a couple of weeks to get to you? Lands’ End has undergone three major changes over the past couple of decades. The first was the introduction of an 800 number, in 1978; the second was express delivery, in 1994; and the third was the introduction of a Web site, in 1995. The first two innovations cut the average transaction time–the time between the moment of ordering and the moment the goods are received–from three weeks to four days. The third innovation has cut the transaction time from four days to, well, four days.

It isn’t just that E-commerce depends on express mail; there’s a sense in which E-commerce is express mail. Right now, billions of dollars are being spent around the country on so-called “last-mile delivery systems.” Companies such as Webvan, in San Francisco, or Kozmo.com, in New York, are putting together networks of trucks and delivery personnel which can reach almost any home in their area within an hour. What if Webvan or Kozmo were somehow integrated into a huge, national, Roberts-style network of connected trucks? And what if that network were in turn integrated into the operations of a direct merchant like Lands’ End? There may soon come a time when a customer from Northampton could order some shirts on LandsEnd.com at the height of the Christmas rush, knowing that the retailer’s computer could survey its stock, assess its warehouse capabilities, “ping” a network of thousands of trucks it has at its disposal, look up how many other orders are going to his neck of the woods, check in with his local Kozmo or Webvan, and tell him, right then and there, precisely what time it could deliver those shirts to him that evening or the next morning. It’s not hard to imagine, under such a system, that Lands’ End’s sales would soar; the gap between the instant gratification of a real store and the delayed gratification of a virtual store would narrow even further. It would be a revolution of sorts, a revolution of satellites, probability models, people in headsets, cell phones, truckers, logistics experts, bar codes, deals with the local cheese factory, and–oh yes, the Internet.

The interesting question, of course, is why we persist in identifying the E-commerce boom as an Internet revolution. Part of the reason, perhaps, is simply the convenience of the word “Internet” as a shorthand for all the technological wizardry of the last few decades. But surely whom and what we choose to celebrate in any period of radical change says something about the things we value. This fall, for example, the Goodyear Tire & Rubber Company–a firm with sales of more than thirteen billion dollars–was dropped from the Dow Jones industrial average. After all, Goodyear runs factories, not Web sites. It is based in Akron, not in Silicon Valley. It is part of the highway highway, not the information highway. The manufacturing economy of the early twentieth century, from which Goodyear emerged, belonged to trade unions and blue-collar men. But ours is the first economic revolution in history that the educated classes have sought to claim as wholly their own, a revolution of Kevin Kelly’s “communication” and Bill Gates’s “thought”–the two activities for which the Net-Geners believe themselves to be uniquely qualified. Today’s talkers and thinkers value the conception of ideas, not their fulfillment. They give credit to the catalogue, but not to the postman who delivered it, or to the road he travelled on. The new economy was supposed to erase all hierarchies. Instead, it has devised another one. On the front end, there are visionaries. On the back end, there are drones.

6.

One of the very first packages ever delivered by parcel post, in 1913, was an eight-pound crate of apples sent from New Jersey to President Wilson at the White House. The symbolism of that early delivery was deliberate. When the parcel post was established, the assumption was that it would be used by farmers as a way of sending their goods cheaply and directly to customers in the city. “Let us imagine that the Gotham family,” one journalist wrote at the time, immured in the city by the demands of Father Gotham’s business, knew that twice a week during the summer they could get from Farmer Ruralis, forty miles out in the country, a hamper of fresh-killed poultry, green peas, string beans, asparagus, strawberries, lettuce, cherries, summer squash, and what not; that the “sass” would be only a day from garden to table; that prices would be lower than market prices; that the cost of transportation would be only thirty-five cents in and, say, eleven cents for the empty hamper back again. Would the Gotham family be interested?

The Post Office told rural mailmen to gather the names and addresses of all those farmers along their routes who wanted to sell their produce by mail. Those lists were given to city mailmen, who delivered them along their routes, so interested customers could get in contact with interested farmers directly. Because customers wanted to know what kind of produce each farmer had to sell, local postmasters began including merchandise information on their lists, essentially creating a farm-produce mail-order catalogue. A California merchant named David Lubin proposed a scheme whereby a farmer would pick up colored cards from the post office–white for eggs, pink for chickens, yellow for butter–mark each card with his prices, and mail the cards back. If he had three chickens that week for a dollar each, he would mail three pink cards to the post office. There they would be put in a pigeonhole with all the other pink cards. Customers could come by and comparison shop, pick out the cards they liked, write their address on these cards, and have the postal clerk mail them back to the farmer. It was a pre-digital eBay. The scheme was adopted in and around Sacramento, and Congress appropriated ten thousand dollars to try a similar version of it on a large scale.

At about the same time, an assistant Postmaster General, James Blakslee, had the bright idea of putting together a fleet of parcel-post trucks, which would pick up farm produce from designated spots along the main roads and ship it directly to town. Blakslee laid out four thousand miles of produce routes around the country, to be covered by fifteen hundred parcel- post trucks. In 1918, in the system’s inaugural run, four thousand day-old chicks, two hundred pounds of honey, five hundred pounds of smoked sausage, five hundred pounds of butter, and eighteen thousand eggs were carried from Lancaster, Pennsylvania, to New York City, all for $31.60 in postage. New York’s Secretary of State called it “an epoch in the history of the United States and the world.”

Only, it wasn’t. The Post Office had devised a wonderful way of communicating between farmer and customer. But there is more to a revolution than communication, and within a few years the farm-to-table movement, which started out with such high hopes, was dead. The problem was that Blakslee’s trucks began to break down, which meant that the food on board spoiled. Eggs proved hard to package, and so they often arrived damaged. Butter went rancid. In the winter of 1919-20, Blakslee collected a huge number of orders for potatoes, but, as Wayne Fuller writes in his wonderful history of the era, “RFD:The Changing Face of Rural America,” the potatoes that year were scarce, and good ones even scarcer, and when Blakslee’s men were able to buy them and attempted delivery, nothing but trouble followed. Some of the potatoes were spoiled to begin with; some froze in transit; prices varied, deliveries went astray, and customers complained loudly enough for Congress to hear. One harried official wrote Blakslee that he could “fill the mails with complaints from people who have ordered potatoes from October to December.”… Some people had been waiting over four months, either to have the potatoes delivered or their money refunded.

Parcel post, in the end, turned out to be something entirely different from what was originally envisioned–a means not to move farm goods from country to town but to move consumer goods from town to country. That is the first lesson from the revolution of a hundred years ago, and it’s one that should give pause to all those eager to pronounce on the significance of the Internet age: the nature of revolutions is such that you never really know what they mean until they are over. The other lesson, of course, is that coming up with a new way of connecting buyers and sellers is a very fine thing, but what we care about most of all is getting our potatoes.

© 2013 Malcolm Gladwell.

DAVID AND GOLIATH
 
WHAT THE DOG SAW
 
OUTLIERS
 
BLINK
 
THE TIPPING POINT
 
ARTICLES
 
ETC.
Smaller

Posted November 26, 2001 by MALCOLM GLADWELL & filed under ANNALS OF TECHNOLOGY, THE NEW YORKER - ARCHIVE.

The disposable diaper and the meaning of progress.

1.

The best way to explore the mystery of the Huggies Ultratrim disposable diaper is to unfold it and then cut it in half, widthwise, across what is known as the diaper’s chassis. At Kimberly-Clark’s Lakeview plant, in Neenah, Wisconsin, where virtually all the Huggies in the Midwest are made, there is a quality-control specialist who does this all day long, culling diapers from the production line, pinning them up against a lightboard, and carefully dismembering them with a pair of scissors. There is someone else who does a “visual cull,” randomly picking out Huggies and turning them over to check for flaws. But a surface examination tells you little. A diaper is not like a computer that makes satisfying burbling noises from time to time, hinting at great inner complexity. It feels like papery underwear wrapped around a thin roll of Cottonelle. But peel away the soft fabric on the top side of the diaper, the liner, which receives what those in the trade delicately refer to as the “insult.” You’ll find a layer of what’s called polyfilm, which is thinner than a strip of Scotch tape. This layer is one of the reasons the garment stays dry: it has pores that are large enough to let air flow in, so the diaper can breathe, but small enough to keep water from flowing out, so the diaper doesn’t leak.

Or run your hands along that liner. It feels like cloth. In fact, the people at Kimberly-Clark make the liner out of a special form of plastic, a polyresin. But they don’t melt the plastic into a sheet, as one would for a plastic bag. They spin the resin into individual fibres, and then use the fibres to create a kind of microscopic funnel, channelling the insult toward the long, thick rectangular pad that runs down the center of the chassis, known as the absorbent core. A typical insult arrives at a rate of seven millilitres a second, and might total seventy millilitres of fluid. The liner can clear that insult in less than twenty seconds. The core can hold three or more of those insults, with a chance of leakage in the single digits. The baby’s skin will remain almost perfectly dry, and that is critical, because prolonged contact between the baby and the insult (in particular, ammonium hydroxide, a breakdown product of urine) is what causes diaper rash. And all this will be accomplished by a throwaway garment measuring, in the newborn size, just seven by thirteen inches. This is the mystery of the modern disposable diaper: how does something so small do so much?

2.

Thirty-seven years ago, the Silicon Valley pioneer Gordon Moore made a famous prediction. The number of transistors that engineers could fit onto a microchip, he said, would double every two years. It seemed like a foolhardy claim: it was not clear that you could keep making transistors smaller and smaller indefinitely. It also wasn’t clear that it would make sense to do so. Most of the time when we make things smaller, after all, we pay a price. A smaller car is cheaper and more fuel-efficient, and easier to park and maneuver, but it will never be as safe as a larger car. In the nineteen-fifties and sixties, the transistor radio was all the rage; it could fit inside your pocket and run on a handful of batteries. But, because it was so small, the sound was terrible, and virtually all the other mini-electronics turn out to be similarly imperfect. Tiny cell phones are hard to dial. Tiny televisions are hard to watch. In making an object smaller, we typically compromise its performance. The remarkable thing about chips, though, was that there was no drawback: if you could fit more and more transistors onto a microchip, then instead of using ten or twenty or a hundred microchips for a task you could use just one. This meant, in turn, that you could fit microchips in all kinds of places (such as cellular phones and laptops) that you couldn’t before, and, because you were using one chip and not a hundred, computer power could be had at a fraction of the price, and because chips were now everywhere and in such demand they became even cheaper to make–and so on and so on. Moore’s Law, as it came to be called, describes that rare case in which there is no trade-off between size and performance. Microchips are what might be termed a perfect innovation.

In the past twenty years, diapers have got smaller and smaller, too. In the early eighties, they were three times bulkier than they are now, thicker and substantially wider in the crotch. But in the mid-eighties Huggies and Procter & Gamble’s Pampers were reduced in bulk by fifty per cent; in the mid-nineties they shrank by a third or so; and in the next few years they may shrink still more. It seems reasonable that there should have been a downside to this, just as there was to the shrinking of cars and radios: how could you reduce the amount of padding in a diaper and not, in some way, compromise its ability to handle an insult? Yet, as diapers got smaller, they got better, and that fact elevates the diaper above nearly all the thousands of other products on the supermarket shelf.

Kimberly-Clark’s Lakeview plant is a huge facility, just down the freeway from Green Bay. Inside, it is as immaculate as a hospital operating room. The walls and floors have been scrubbed white. The stainless-steel machinery gleams. The employees are dressed in dark-blue pants, starched light-blue button-down shirts, and tissue-paper caps. There are rows of machines in the plant, each costing more than fifteen million dollars–a dizzying combination of conveyor belts and whirling gears and chutes stretching as long as a city block and creating such a din that everyone on the factory floor wears headsets and communicates by radio. Computers monitor a million data points along the way, insuring that each of those components is precisely cut and attached according to principles and processes and materials protected, on the Huggies Ultratrim alone, by hundreds of patents. At the end of the line, the Huggies come gliding out of the machine, stacked upright, one after another in an endless row, looking like exquisitely formed slices of white bread in a toast rack. For years, because of Moore’s Law, we have considered the microchip the embodiment of the technological age. But if the diaper is also a perfect innovation, doesn’t it deserve a place beside the chip?

3.

The modern disposable diaper was invented twice, first by Victor Mills and then by Carlyle Harmon and Billy Gene Harper. Mills worked for Procter & Gamble, and he was a legend. Ivory soap used to be made in an expensive and time-consuming batch-by-batch method. Mills figured out a simpler, continuous process. Duncan Hines cake mixes used to have a problem blending flour, sugar, and shortening in a consistent mixture. Mills introduced the machines used for milling soap, which ground the ingredients much more finely than before, and the result was New, Improved Duncan Hines cake mix. Ever wonder why Pringles, unlike other potato chips, are all exactly the same shape? Because they are made like soap: the potato is ground into a slurry, then pressed, baked, and wrapped–and that was Victor Mills’s idea, too.

In 1957, Procter & Gamble bought the Charmin Paper Company, of Green Bay, Wisconsin, and Mills was told to think of new products for the paper business. Since he was a grandfather–and had always hated washing diapers–he thought of a disposable diaper. “One of the early researchers told me that among the first things they did was go out to a toy store and buy one of those Betsy Wetsy-type dolls, where you put water in the mouth and it comes out the other end,” Ed Rider, the head of the archives department at Procter & Gamble, says. “They brought it back to the lab, hooked up its legs on a treadmill to make it walk, and tested diapers on it.” The end result was Pampers, which were launched in Peoria, in 1961. The diaper had a simple rectangular shape. Its liner, which lay against the baby’s skin, was made of rayon. The outside material was plastic. In between were multiple layers of crêped tissue. The diaper was attached with pins and featured what was known as a Z fold, meaning that the edges of the inner side were pleated, to provide a better fit around the legs.

In 1968, Kimberly-Clark brought out Kimbies, which took the rectangular diaper and shaped it to more closely fit a baby’s body. In 1976, Procter & Gamble brought out Luvs, which elasticized the leg openings to prevent leakage. But diapers still adhered to the basic Millsian notion of an absorbent core made out of paper–and that was a problem. When paper gets wet, the fluid soaks right through, which makes diaper rash worse. And if you put any kind of pressure on paper–if you squeeze it, or sit on it–it will surrender some of the water it has absorbed, which creates further difficulties, because a baby, in the usual course of squirming and crawling and walking, might place as much as five kilopascals of pressure on the absorbent core of a diaper. Diaper-makers tried to address this shortcoming by moving from crêped tissue to what they called fluff, which was basically finely shredded cellulose. Then they began to compensate for paper’s failing by adding more and more of it, until diapers became huge. But they now had Moore’s Law in reverse: in order to get better, they had to get bigger–and bigger still wasn’t very good.

Carlyle Harmon worked for Johnson & Johnson and Billy Gene Harper worked for Dow Chemical, and they had a solution. In 1966, each filed separate but virtually identical patent applications, proposing that the best way to solve the diaper puzzle was with a peculiar polymer that came in the form of little pepperlike flakes and had the remarkable ability to absorb up to three hundred times its weight in water.

In the Dow patent, Harper and his team described how they sprinkled two grams of the superabsorbent polymer between two twenty-inch-square sheets of nylon broadcloth, and then quilted the nylon layers together. The makeshift diaper was “thereafter put into use in personal management of a baby of approximately 6 months age.” After four hours, the diaper was removed. It now weighed a hundred and twenty grams, meaning the flakes had soaked up sixty times their weight in urine.

Harper and Harmon argued that it was quite unnecessary to solve the paper problem by stuffing the core of the diaper with thicker and thicker rolls of shredded pulp. Just a handful of superabsorbent polymer would do the job. Thus was the modern diaper born. Since the mid-eighties, Kimberly-Clark and Procter & Gamble have made diapers the Harper and Harmon way, pulling out paper and replacing it with superabsorbent polymer. The old, paper-filled diaper could hold, at most, two hundred and seventy-five millilitres of fluid, or a little more than a cup. Today, a diaper full of superabsorbent polymer can handle as much as five hundred millilitres, almost twice that. The chief characteristic of the Mills diaper was its simplicity: the insult fell directly into the core. But the presence of the polymer has made the diaper far more complex. It takes longer for the polymer than it does paper to fully absorb an insult, for instance. So another component was added, the acquisition layer, between the liner and the core. The acquisition layer acts like blotting paper, holding the insult while the core slowly does its work, and distributing the fluid over its full length.

Diaper researchers sometimes perform what is called a re-wet test, where they pour a hundred millilitres of fluid onto the surface of a diaper and then apply a piece of filter paper to the diaper liner with five kilopascals of pressure–the average load a baby would apply to a diaper during ordinary use. In a contemporary superabsorbent diaper, like a Huggies or a Pampers, the filter paper will come away untouched after one insult. After two insults, there might be 0.1 millilitres of fluid on the paper. After three insults, the diaper will surrender, at most, only two millilitres of moisture–which is to say that, with the aid of superabsorbents, a pair of Huggies or Pampers can effortlessly hold, even under pressure, a baby’s entire night’s work.

The heir to the legacy of Billy Gene Harper at Dow Chemical is Fredric Buchholz, who works in Midland, Michigan, a small town two hours northwest of Detroit, where Dow has its headquarters. His laboratory is in the middle of the sprawling chemical works, a mile or two away from corporate headquarters, in a low, unassuming brick building. “We still don’t understand perfectly how these polymers work,” Buchholz said on a recent fall afternoon. What we do know, he said, is that superabsorbent polymers appear, on a microscopic level, to be like a tightly bundled fisherman’s net. In the presence of water, that net doesn’t break apart into thousands of pieces and dissolve, like sugar. Rather, it just unravels, the way a net would open up if you shook it out, and as it does the water gets stuck in the webbing. That ability to hold huge amounts of water, he said, could make superabsorbent polymers useful in fire fighting or irrigation, because slightly gelled water is more likely to stay where it’s needed. There are superabsorbents mixed in with the sealant on the walls of the Chunnel between England and France, so if water leaks in the polymer will absorb the water and plug the hole.

Right now, one of the major challenges facing diaper technology, Buchholz said, is that urine is salty, and salt impairs the unravelling of the netting: superabsorbents can handle only a tenth as much salt water as fresh water. “One idea is to remove the salt from urine. Maybe you could have a purifying screen,” he said. If the molecular structure of the superabsorbent were optimized, he went on, its absorptive capacity could increase by another five hundred per cent. “Superabsorbents could go from absorbing three hundred times their weight to absorbing fifteen hundred times their weight. We could have just one perfect particle of super-absorbent in a diaper. If you are going to dream, why not make the diaper as thin as a pair of underwear?”

Buchholz was in his laboratory, and he held up a small plastic cup filled with a few tablespoons of superabsorbent flakes, each not much larger than a grain of salt. “It’s just a granular material, totally nontoxic,” he said. “This is about two grams.” He walked over to the sink and filled a large beaker with tap water, and poured the contents of the beaker into the jar of superabsorbent. At first, nothing happened. The amounts were so disproportionate that it looked as if the water would simply engulf the flakes. But, slowly and steadily, the water began to thicken. “Look,” Buchholz said. “It’s becoming soupy.” Sure enough, little beads of gel were forming. Nothing else was happening: there was no gas given off, no burbling or sizzling as the chemical process took place. The superabsorbent polymer was simply swallowing up the water, and within minutes the contents of the cup had thickened into what looked like slightly lumpy, spongy pudding. Buchholz picked up the jar and tilted it, to show that nothing at all was coming out. He pushed and prodded the mass with his finger. The water had disappeared. To soak up that much liquid, the Victor Mills diaper would have needed a thick bundle of paper towelling. Buchholz had used a few tablespoons of superabsorbent flakes. Superabsorbent was not merely better; it was smaller.

4.

Why does it matter that the diaper got so small? It seems a trivial thing, chiefly a matter of convenience to the parent taking a bag of diapers home from the supermarket. But it turns out that size matters a great deal. There’s a reason that there are now “new, improved concentrated” versions of laundry detergent, and that some cereals now come in smaller boxes. Smallness is one of those changes that send ripples through the whole economy. The old disposable diapers, for example, created a transportation problem. Tractor-trailers are prohibited by law from weighing more than eighty thousand pounds when loaded. That’s why a truck carrying something heavy and compact like bottled water or Campbell’s soup is “full,” when the truck itself is still half empty. But the diaper of the eighties was what is known as a “high cube” item. It was bulky and not very heavy, meaning that a diaper truck was full before it reached its weight limit. By cutting the size of a diaper in half, companies could fit twice as many diapers on a truck, and cut transportation expenses in half. They could also cut the amount of warehouse space and labor they needed in half. And companies could begin to rethink their manufacturing operations. “Distribution costs used to force you to have plants in lots of places,” Dudley Lehman, who heads the Kimberly-Clark diaper business, says. “As that becomes less and less of an issue, you say, ‘Do I really need all my plants?’ In the United States, it used to take eight. Now it takes five.” (Kimberly-Clark didn’t close any plants. But other manufacturers did, and here, perhaps, is a partial explanation for the great wave of corporate restructuring that swept across America in the late eighties and early nineties: firms could downsize their workforce because they had downsized their products.) And, because using five plants to make diapers is more efficient than using eight, it became possible to improve diapers without raising diaper prices–which is important, because the sheer number of diapers parents have to buy makes it a price-sensitive product. Until recently, diapers were fastened with little pieces of tape, and if the person changing the diapers got lotion or powder on her fingers the tape wouldn’t work. A hook-and-loop, Velcro-like fastener doesn’t have this problem. But it was years before the hook-and-loop fastener was incorporated into the diaper chassis: until over-all manufacturing costs were reduced, it was just too expensive.

Most important, though, is how size affects the way diapers are sold. The shelves along the aisles of a supermarket are divided into increments of four feet, and the space devoted to a given product category is almost always a multiple of that. Diapers, for example, might be presented as a twenty-foot set. But when diapers were at their bulkiest the space reserved for them was never enough. “You could only get a limited number on the shelf,” says Sue Klug, the president of Catalina Marketing Solutions and a former executive for Albertson’s and Safeway. “Say you only had six bags. Someone comes in and buys a few, and then someone else comes in and buys a few more. Now you’re out of stock until someone reworks the shelf, which in some supermarkets might be a day or two.” Out-of-stock rates are already a huge problem in the retail business. At any given time, only about ninety-two per cent of the products that a store is supposed to be carrying are actually on the shelf–which, if you consider that the average supermarket has thirty-five thousand items, works out to twenty-eight hundred products that are simply not there. (For a highly efficient retailer like Wal-Mart, in-stock rates might be as high as ninety-nine per cent; for a struggling firm, they might be in the low eighties.) But, for a fast-moving, bulky item like diapers, the problem of restocking was much worse. Supermarkets could have allocated more shelf space to diapers, of course, but diapers aren’t a particularly profitable category for retailers–profit margins are about half what they are for the grocery department. So retailers would much rather give more shelf space to a growing and lucrative category like bottled water. “It’s all a trade-off,” Klug says. “If you expand diapers four feet, you’ve got to give up four feet of something else.” The only way diaper-makers could insure that their products would actually be on the shelves was to make the products smaller, so they could fit twelve bags into the space of six. And if you can fit twelve bags on a shelf, you can introduce different kinds of diapers. You can add pull-ups and premium diapers and low-cost private-label diapers, all of which give parents more options.

“We cut the cost of trucking in half,” says Ralph Drayer, who was in charge of logistics for Procter & Gamble for many years and now runs his own supply-chain consultancy in Cincinnati. “We cut the cost of storage in half. We cut handling in half, and we cut the cost of the store shelf in half, which is probably the most expensive space in the whole chain.” Everything in the diaper world, from plant closings and trucking routes to product improvements and consumer choice and convenience, turns, in the end, on the fact that Harmon and Harper’s absorbent core was smaller than Victor Mills’s.

The shame of it, though, is that Harmon and Harper have never been properly celebrated for their accomplishment. Victor Mills is the famous one. When he died, he was given a Times obituary, in which he was called “the father of disposable diapers.” When Carlyle Harmon died, seven months earlier, he got four hundred words in Utah’s Deseret News, stressing his contributions to the Mormon Church. We tend to credit those who create an idea, not those who perfect it, forgetting that it is often only in the perfection of an idea that true progress occurs. Putting sixty-four transistors on a chip allowed people to dream of the future. Putting four million transistors on a chip actually gave them the future. The diaper is no different. The paper diaper changed parenting. But a diaper that could hold four insults without leakage, keep a baby’s skin dry, clear an insult in twenty seconds flat, and would nearly always be in stock, even if you arrived at the supermarket at eight o’clock in the evening–and that would keep getting better at all those things, year in and year out–was another thing altogether. This was more than a good idea. This was something like perfection.

© 2013 Malcolm Gladwell.

DAVID AND GOLIATH
 
WHAT THE DOG SAW
 
OUTLIERS
 
BLINK
 
THE TIPPING POINT
 
ARTICLES
 
ETC.
Java Man

Posted July 30, 2001 by MALCOLM GLADWELL & filed under A CRITIC AT LARGE, THE NEW YORKER - ARCHIVE.

How caffeine created the modern world.

1.

The original Coca-Cola was a late-nineteenth-century concoction known as Pemberton’s French Wine Coca, a mixture of alcohol, the caffeine-rich kola nut, and coca, the raw ingredient of cocaine. In the face of social pressure, first the wine and then the coca were removed, leaving the more banal modern beverage in its place: carbonated, caffeinated sugar water with less kick to it than a cup of coffee. But is that the way we think of Coke? Not at all. In the nineteen-thirties, a commercial artist named Haddon Sundblom had the bright idea of posing a portly retired friend of his in a red Santa Claus suit with a Coke in his hand, and plastering the image on billboards and advertisements across the country. Coke, magically, was reborn as caffeine for children, caffeine without any of the weighty adult connotations of coffee and tea. It was–as the ads with Sundblom’s Santa put it–”the pause that refreshes.” It added life. It could teach the world to sing.

One of the things that have always made drugs so powerful is their cultural adaptability, their way of acquiring meanings beyond their pharmacology. We think of marijuana, for example, as a drug of lethargy, of disaffection. But in Colombia, the historian David T. Courtwright points out in “Forces of Habit” (Harvard; $24.95), “peasants boast that cannabis helps them to quita el cansancio or reduce fatigue; increase their fuerza and ánimo, force and spirit; and become incansable, tireless.” In Germany right after the Second World War, cigarettes briefly and suddenly became the equivalent of crack cocaine. “Up to a point, the majority of the habitual smokers preferred to do without food even under extreme conditions of nutrition rather than to forgo tobacco,” according to one account of the period. “Many housewives… bartered fat and sugar for cigarettes.” Even a drug as demonized as opium has been seen in a more favorable light. In the eighteen-thirties, Franklin Delano Roosevelt’s grandfather Warren Delano II made the family fortune exporting the drug to China, and Delano was able to sugarcoat his activities so plausibly that no one ever accused his grandson of being the scion of a drug lord. And yet, as Bennett Alan Weinberg and Bonnie K. Bealer remind us in their marvellous new book “The World of Caffeine” (Routledge; $27.50), there is no drug quite as effortlessly adaptable as caffeine, the Zelig of chemical stimulants.

At one moment, in one form, it is the drug of choice of café intellectuals and artists; in another, of housewives; in another, of Zen monks; and, in yet another, of children enthralled by a fat man who slides down chimneys. King Gustav III, who ruled Sweden in the latter half of the eighteenth century, was so convinced of the particular perils of coffee over all other forms of caffeine that he devised an elaborate experiment. A convicted murderer was sentenced to drink cup after cup of coffee until he died, with another murderer sentenced to a lifetime of tea drinking, as a control. (Unfortunately, the two doctors in charge of the study died before anyone else did; then Gustav was murdered; and finally the tea drinker died, at eighty-three, of old age–leaving the original murderer alone with his espresso, and leaving coffee’s supposed toxicity in some doubt.) Later, the various forms of caffeine began to be divided up along sociological lines. Wolfgang Schivelbusch, in his book “Tastes of Paradise,” argues that, in the eighteenth century, coffee symbolized the rising middle classes, whereas its great caffeinated rival in those years–cocoa, or, as it was known at the time, chocolate–was the drink of the aristocracy. “Goethe, who used art as a means to lift himself out of his middle class background into the aristocracy, and who as a member of a courtly society maintained a sense of aristocratic calm even in the midst of immense productivity, made a cult of chocolate, and avoided coffee,” Schivelbusch writes. “Balzac, who despite his sentimental allegiance to the monarchy, lived and labored for the literary marketplace and for it alone, became one of the most excessive coffee-drinkers in history. Here we see two fundamentally different working styles and means of stimulation–fundamentally different psychologies and physiologies.” Today, of course, the chief cultural distinction is between coffee and tea, which, according to a list drawn up by Weinberg and Bealer, have come to represent almost entirely opposite sensibilities:

Coffee Aspect
Male
Boisterous
Indulgence
Hardheaded
Topology
Heidegger
Beethoven
Libertarian
Promiscuous

Tea Aspect
Female
Decorous
Temperance
Romantic
Geometry
Carnap
Mozart
Statist
Pure

That the American Revolution began with the symbolic rejection of tea in Boston Harbor, in other words, makes perfect sense. Real revolutionaries would naturally prefer coffee. By contrast, the freedom fighters of Canada, a hundred years later, were most definitely tea drinkers. And where was Canada’s autonomy won? Not on the blood-soaked fields of Lexington and Concord but in the genteel drawing rooms of Westminster, over a nice cup of Darjeeling and small, triangular cucumber sandwiches.

2.

All this is a bit puzzling. We don’t fetishize the difference between salmon eaters and tuna eaters, or people who like their eggs sunny-side up and those who like them scrambled. So why invest so much importance in the way people prefer their caffeine? A cup of coffee has somewhere between a hundred and two hundred and fifty milligrams; black tea brewed for four minutes has between forty and a hundred milligrams. But the disparity disappears if you consider that many tea drinkers drink from a pot, and have more than one cup. Caffeine is caffeine. “The more it is pondered,” Weinberg and Bealer write, “the more paradoxical this duality within the culture of caffeine appears. After all, both coffee and tea are aromatic infusions of vegetable matter, served hot or cold in similar quantities; both are often mixed with cream or sugar; both are universally available in virtually any grocery or restaurant in civilized society; and both contain the identical psychoactive alkaloid stimulant, caffeine.”

It would seem to make more sense to draw distinctions based on the way caffeine is metabolized rather than on the way it is served. Caffeine, whether it is in coffee or tea or a soft drink, moves easily from the stomach and intestines into the bloodstream, and from there to the organs, and before long has penetrated almost every cell of the body. This is the reason that caffeine is such a wonderful stimulant. Most substances can’t cross the blood-brain barrier, which is the body’s defensive mechanism, preventing viruses or toxins from entering the central nervous system. Caffeine does so easily. Within an hour or so, it reaches its peak concentration in the brain, and there it does a number of things–principally, blocking the action of adenosine, the neuromodulator that makes you sleepy, lowers your blood pressure, and slows down your heartbeat. Then, as quickly as it builds up in your brain and tissues, caffeine is gone–which is why it’s so safe. (Caffeine in ordinary quantities has never been conclusively linked to serious illness.)

But how quickly it washes away differs dramatically from person to person. A two-hundred-pound man who drinks a cup of coffee with a hundred milligrams of caffeine will have a maximum caffeine concentration of one milligram per kilogram of body weight. A hundred-pound woman having the same cup of coffee will reach a caffeine concentration of two milligrams per kilogram of body weight, or twice as high. In addition, when women are on the Pill, the rate at which they clear caffeine from their bodies slows considerably. (Some of the side effects experienced by women on the Pill may in fact be caffeine jitters caused by their sudden inability to tolerate as much coffee as they could before.) Pregnancy reduces a woman’s ability to process caffeine still further. The half-life of caffeine in an adult is roughly three and a half hours. In a pregnant woman, it’s eighteen hours. (Even a four-month-old child processes caffeine more efficiently.) An average man and woman sitting down for a cup of coffee are thus not pharmaceutical equals: in effect, the woman is under the influence of a vastly more powerful drug. Given these differences, you’d think that, instead of contrasting the caffeine cultures of tea and coffee, we’d contrast the caffeine cultures of men and women.

3.

But we don’t, and with good reason. To parse caffeine along gender lines does not do justice to its capacity to insinuate itself into every aspect of our lives, not merely to influence culture but even to create it. Take coffee’s reputation as the “thinker’s” drink. This dates from eighteenth-century Europe, where coffeehouses played a major role in the egalitarian, inclusionary spirit that was then sweeping the continent. They sprang up first in London, so alarming Charles II that in 1676 he tried to ban them. It didn’t work. By 1700, there were hundreds of coffeehouses in London, their subversive spirit best captured by a couplet from a comedy of the period: “In a coffeehouse just now among the rabble / I bluntly asked, which is the treason table.” The movement then spread to Paris, and by the end of the eighteenth century coffeehouses numbered in the hundreds–most famously, the Café de la Régence, near the Palais Royal, which counted among its customers Robespierre, Napoleon, Voltaire, Victor Hugo, Théophile Gautier, Rousseau, and the Duke of Richelieu. Previously, when men had gathered together to talk in public places, they had done so in bars, which drew from specific socioeconomic niches and, because of the alcohol they served, created a specific kind of talk. The new coffeehouses, by contrast, drew from many different classes and trades, and they served a stimulant, not a depressant. “It is not extravagant to claim that it was in these gathering spots that the art of conversation became the basis of a new literary style and that a new ideal of general education in letters was born,” Weinberg and Bealer write.

It is worth noting, as well, that in the original coffeehouses nearly everyone smoked, and nicotine also has a distinctive physiological effect. It moderates mood and extends attention, and, more important, it doubles the rate of caffeine metabolism: it allows you to drink twice as much coffee as you could otherwise. In other words, the original coffeehouse was a place where men of all types could sit all day; the tobacco they smoked made it possible to drink coffee all day; and the coffee they drank inspired them to talk all day. Out of this came the Enlightenment. (The next time we so perfectly married pharmacology and place, we got Joan Baez.)

In time, caffeine moved from the café to the home. In America, coffee triumphed because of the country’s proximity to the new Caribbean and Latin American coffee plantations, and the fact that throughout the nineteenth century duties were negligible. Beginning in the eighteen-twenties, Courtwright tells us, Brazil “unleashed a flood of slave-produced coffee. American per capita consumption, three pounds per year in 1830, rose to eight pounds by 1859.”

What this flood of caffeine did, according to Weinberg and Bealer, was to abet the process of industrialization–to help “large numbers of people to coordinate their work schedules by giving them the energy to start work at a given time and continue it as long as necessary.” Until the eighteenth century, it must be remembered, many Westerners drank beer almost continuously, even beginning their day with something called “beer soup.” (Bealer and Weinberg helpfully provide the following eighteenth-century German recipe: “Heat the beer in a saucepan; in a separate small pot beat a couple of eggs. Add a chunk of butter to the hot beer. Stir in some cool beer to cool it, then pour over the eggs. Add a bit of salt, and finally mix all the ingredients together, whisking it well to keep it from curdling.”) Now they began each day with a strong cup of coffee. One way to explain the industrial revolution is as the inevitable consequence of a world where people suddenly preferred being jittery to being drunk. In the modern world, there was no other way to keep up. That’s what Edison meant when he said that genius was ninety-nine per cent perspiration and one per cent inspiration. In the old paradigm, working with your mind had been associated with leisure. It was only the poor who worked hard. (The quintessential pre-industrial narrative of inspiration belonged to Archimedes, who made his discovery, let’s not forget, while taking a bath.) But Edison was saying that the old class distinctions no longer held true–that in the industrialized world there was as much toil associated with the life of the mind as there had once been with the travails of the body.

In the twentieth century, the professions transformed themselves accordingly: medicine turned the residency process into an ordeal of sleeplessness, the legal profession borrowed a page from the manufacturing floor and made its practitioners fill out time cards like union men. Intellectual heroics became a matter of endurance. “The pace of computation was hectic,” James Gleick writes of the Manhattan Project in “Genius,” his biography of the physicist Richard Feynman. “Feynman’s day began at 8:30 and ended fifteen hours later. Sometimes he could not leave the computing center at all. He worked through for thirty-one hours once and the next day found that an error minutes after he went to bed had stalled the whole team. The routine allowed just a few breaks.” Did Feynman’s achievements reflect a greater natural talent than his less productive forebears had? Or did he just drink a lot more coffee? Paul Hoffman, in “The Man Who Loved Only Numbers,” writes of the legendary twentieth-century mathematician Paul Erdös that “he put in nineteen-hour days, keeping himself fortified with 10 to 20 milligrams of Benzedrine or Ritalin, strong espresso and caffeine tablets. ‘A mathematician,’ Erdös was fond of saying, ‘is a machine for turning coffee into theorems.’” Once, a friend bet Erdös five hundred dollars that he could not quit amphetamines for a month. Erdös took the bet and won, but, during his time of abstinence, he found himself incapable of doing any serious work. “You’ve set mathematics back a month,” he told his friend when he collected, and immediately returned to his pills.

Erdös’s unadulterated self was less real and less familiar to him than his adulterated self, and that is a condition that holds, more or less, for the rest of society as well. Part of what it means to be human in the modern age is that we have come to construct our emotional and cognitive states not merely from the inside out–with thought and intention–but from the outside in, with chemical additives. The modern personality is, in this sense, a synthetic creation: skillfully regulated and medicated and dosed with caffeine so that we can always be awake and alert and focussed when we need to be. On a bet, no doubt, we could walk away from caffeine if we had to. But what would be the point? The lawyers wouldn’t make their billable hours. The young doctors would fall behind in their training. The physicists might still be stuck out in the New Mexico desert. We’d set the world back a month.

4.

That the modern personality is synthetic is, of course, a disquieting notion. When we talk of synthetic personality–or of constructing new selves through chemical means–we think of hard drugs, not caffeine. Timothy Leary used to make such claims about LSD, and the reason his revolution never took flight was that most of us found the concept of tuning in, turning on, and dropping out to be a bit creepy. Here was this shaman, this visionary–and yet, if his consciousness was so great, why was he so intent on altering it? More important, what exactly were we supposed to be tuning in to? We were given hints, with psychedelic colors and deep readings of “Lucy in the Sky with Diamonds,” but that was never enough. If we are to re-create ourselves, we would like to know what we will become.

Caffeine is the best and most useful of our drugs because in every one of its forms it can answer that question precisely. It is a stimulant that blocks the action of adenosine, and comes in a multitude of guises, each with a ready-made story attached, a mixture of history and superstition and whimsy which infuses the daily ritual of adenosine blocking with meaning and purpose. Put caffeine in a red can and it becomes refreshing fun. Brew it in a teapot and it becomes romantic and decorous. Extract it from little brown beans and, magically, it is hardheaded and potent. “There was a little known Russian émigré, Trotsky by name, who during World War I was in the habit of playing chess in Vienna’s Café Central every evening,” Bealer and Weinberg write, in one of the book’s many fascinating café yarns:

A typical Russian refugee, who talked too much but seemed utterly harmless, indeed, a pathetic figure in the eyes of the Viennese. One day in 1917 an official of the Austrian Foreign Ministry rushed into the minister’s room, panting and excited, and told his chief, “Your excellency . . . Your excellency . . . Revolution has broken out in Russia.” The minister, less excitable and less credulous than his official, rejected such a wild claim and retorted calmly, “Go away . . . Russia is not a land where revolutions break out. Besides, who on earth would make a revolution in Russia? Perhaps Herr Trotsky from the Café Central?”

The minister should have known better. Give a man enough coffee and he’s capable of anything.

© 2013 Malcolm Gladwell.

DAVID AND GOLIATH
 
WHAT THE DOG SAW
 
OUTLIERS
 
BLINK
 
THE TIPPING POINT
 
ARTICLES
 
ETC.
The Ketchup Conundrum

Posted September 6, 2004 by MALCOLM GLADWELL & filed under TASTE TECHNOLOGIES, THE NEW YORKER - ARCHIVE.

Mustard now comes in dozens of varieties. Why has ketchup stayed the same?

1.

Many years ago, one mustard dominated the supermarket shelves: French’s.  It came in a plastic bottle.  People used it on hot dogs and bologna.  It was a yellow mustard, made from ground white mustard seed with turmeric and vinegar, which gave it a mild, slightly metallic taste.  If you looked hard in the grocery store, you might find something in the specialty-foods section called Grey Poupon, which was Dijon mustard, made from the more pungent brown mustard seed.  In the early seventies, Grey Poupon was no more than a hundred-thousand-dollar-a-year business.  Few people knew what it was or how it tasted, or had any particular desire for an alternative to French’s or the runner-up, Gulden’s.  Then one day the Heublein Company, which owned Grey Poupon, discovered something remarkable: if you gave people a mustard taste test, a significant number had only to try Grey Poupon once to switch from yellow mustard.  In the food world that almost never happens; even among the most successful food brands, only about one in a hundred have that kind of conversion rate.  Grey Poupon was magic.

So Heublein put Grey Poupon in a bigger glass jar, with an enamelled label and enough of a whiff of Frenchness to make it seem as if it were still being made in Europe (it was made in Hartford, Connecticut, from Canadian mustard seed and white wine).  The company ran tasteful print ads in upscale food magazines.  They put the mustard in little foil packets and distributed them with airplane meals—which was a brand-new idea at the time.  Then they hired the Manhattan ad agency Lowe Marschalk to do something, on a modest budget, for television.  The agency came back with an idea: A Rolls-Royce is driving down a country road.  There’s a man in the back seat in a suit with a plate of beef on a silver tray.  He nods to the chauffeur, who opens the glove compartment.  Then comes what is known in the business as the “reveal.”  The chauffeur hands back a jar of Grey Poupon.  Another Rolls-Royce pulls up alongside.  A man leans his head out the window.  ”Pardon me.  Would you have any Grey Poupon?”

In the cities where the ads ran, sales of Grey Poupon leaped forty to fifty per cent, and whenever Heublein bought airtime in new cities sales jumped by forty to fifty per cent again.  Grocery stores put Grey Poupon next to French’s and Gulden’s.  By the end of the nineteen-eighties Grey Poupon was the most powerful brand in mustard.  ”The tagline in the commercial was that this was one of life’s finer pleasures,” Larry Elegant, who wrote the original Grey Poupon spot, says, “and that, along with the Rolls-Royce, seemed to impart to people’s minds that this was something truly different and superior.”

The rise of Grey Poupon proved that the American supermarket shopper was willing to pay more—in this case, $3.99 instead of $1.49 for eight ounces—as long as what they were buying carried with it an air of sophistication and complex aromatics.  Its success showed, furthermore, that the boundaries of taste and custom were not fixed: that just because mustard had always been yellow didn’t mean that consumers would use only yellow mustard.  It is because of Grey Poupon that the standard American supermarket today has an entire mustard section.  And it is because of Grey Poupon that a man named Jim Wigon decided, four years ago, to enter the ketchup business.  Isn’t the ketchup business today exactly where mustard was thirty years ago? There is Heinz and, far behind, Hunt’s and Del Monte and a handful of private-label brands.  Jim Wigon wanted to create the Grey Poupon of ketchup.

Wigon is from Boston.  He’s a thickset man in his early fifties, with a full salt-and-pepper beard.  He runs his ketchup business—under the brand World’s Best Ketchup—out of the catering business of his partner, Nick Schiarizzi, in Norwood, Massachusetts, just off Route 1, in a low-slung building behind an industrial-equipment-rental shop.  He starts with red peppers, Spanish onions, garlic, and a high-end tomato paste.  Basil is chopped by hand, because the buffalo chopper bruises the leaves.  He uses maple syrup, not corn syrup, which gives him a quarter of the sugar of Heinz.  He pours his ketchup into a clear glass ten-ounce jar, and sells it for three times the price of Heinz, and for the past few years he has crisscrossed the country, peddling World’s Best in six flavors—regular, sweet, dill, garlic, caramelized onion, and basil—to specialty grocery stores and supermarkets.  If you were in Zabar’s on Manhattan’s Upper West Side a few months ago, you would have seen him at the front of the store, in a spot between the sushi and the gefilte fish.  He was wearing a World’s Best baseball cap, a white shirt, and a red-stained apron.  In front of him, on a small table, was a silver tureen filled with miniature chicken and beef meatballs, a box of toothpicks, and a dozen or so open jars of his ketchup.  ”Try my ketchup!” Wigon said, over and over, to anyone who passed.  ”If you don’t try it, you’re doomed to eat Heinz the rest of your life.”

In the same aisle at Zabar’s that day two other demonstrations were going on, so that people were starting at one end with free chicken sausage, sampling a slice of prosciutto, and then pausing at the World’s Best stand before heading for the cash register.  They would look down at the array of open jars, and Wigon would impale a meatball on a toothpick, dip it in one of his ketchups, and hand it to them with a flourish.  The ratio of tomato solids to liquid in World’s Best is much higher than in Heinz, and the maple syrup gives it an unmistakable sweet kick.  Invariably, people would close their eyes, just for a moment, and do a subtle double take.  Some of them would look slightly perplexed and walk away, and others would nod and pick up a jar.  ”You know why you like it so much?” he would say, in his broad Boston accent, to the customers who seemed most impressed.  ”Because you’ve been eating bad ketchup all ” Jim Wigon had a simple vision: build a better ketchup—the way Grey Poupon built a better mustard—and the world will beat a path to your door.  If only it were that easy.

2.

The story of World’s Best Ketchup cannot properly be told without a man from White Plains, New York, named Howard Moskowitz.  Moskowitz is sixty, short and round, with graying hair and huge gold-rimmed glasses.  When he talks, he favors the Socratic monologue—a series of questions that he poses to himself, then answers, punctuated by “ahhh” and much vigorous nodding.  He is a lineal descendant of the legendary eighteenth-century Hasidic rabbi known as the Seer of Lublin.  He keeps a parrot.  At Harvard, he wrote his doctoral dissertation on psychophysics, and all the rooms on the ground floor of his food-testing and market-research business are named after famous psychophysicists.  (“Have you ever heard of the name Rose Marie Pangborn? Ahhh.  She was a professor at Davis.  Very famous.  This is the Pangborn kitchen.”) Moskowitz is a man of uncommon exuberance and persuasiveness: if he had been your freshman statistics professor, you would today be a statistician.  ”My favorite writer? Gibbon,” he burst out, when we met not long ago.  He had just been holding forth on the subject of sodium solutions.  ”Right now I’m working my way through the Hales history of the Byzantine Empire.  Holy shit! Everything is easy until you get to the Byzantine Empire.  It’s impossible.  One emperor is always killing the others, and everyone has five wives or three husbands.  It’s very Byzantine.”

Moskowitz set up shop in the seventies, and one of his first clients was Pepsi.  The artificial sweetener aspartame had just become available, and Pepsi wanted Moskowitz to figure out the perfect amount of sweetener for a can of Diet Pepsi.  Pepsi knew that anything below eight per cent sweetness was not sweet enough and anything over twelve per cent was too sweet.  So Moskowitz did the logical thing.  He made up experimental batches of Diet Pepsi with every conceivable degree of sweetness—8 per cent, 8.25 per cent, 8.5, and on and on up to 12—gave them to hundreds of people, and looked for the concentration that people liked the most.  But the data were a mess—there wasn’t a pattern—and one day, sitting in a diner, Moskowitz realized why.  They had been asking the wrong question.  There was no such thing as the perfect Diet Pepsi.  They should have been looking for the perfect Diet Pepsis.

It took a long time for the food world to catch up with Howard Moskowitz.  He knocked on doors and tried to explain his idea about the plural nature of perfection, and no one answered.  He spoke at food-industry conferences, and audiences shrugged.  But he could think of nothing else.  ”It’s like that Yiddish expression,” he says.  ”Do you know it? To a worm in horseradish, the world is horseradish!” Then, in 1986, he got a call from the Campbell’s Soup Company.  They were in the spaghetti-sauce business, going up against Ragú with their Prego brand.  Prego was a little thicker than Ragú, with diced tomatoes as opposed to Ragú’s purée, and, Campbell’s thought, had better pasta adherence.  But, for all that, Prego was in a slump, and Campbell’s was desperate for new ideas.

Standard practice in the food industry would have been to convene a focus group and ask spaghetti eaters what they wanted.  But Moskowitz does not believe that consumers—even spaghetti lovers—know what they desire if what they desire does not yet exist.  ”The mind,” as Moskowitz is fond of saying, “knows not what the tongue wants.”  Instead, working with the Campbell’s kitchens, he came up with forty-five varieties of spaghetti sauce.  These were designed to differ in every conceivable way: spiciness, sweetness, tartness, saltiness, thickness, aroma, mouth feel, cost of ingredients, and so forth.  He had a trained panel of food tasters analyze each of those varieties in depth.  Then he took the prototypes on the road—to New York, Chicago, Los Angeles, and Jacksonville—and asked people in groups of twenty-five to eat between eight and ten small bowls of different spaghetti sauces over two hours and rate them on a scale of one to a hundred.  When Moskowitz charted the results, he saw that everyone had a slightly different definition of what a perfect spaghetti sauce tasted like.  If you sifted carefully through the data, though, you could find patterns, and Moskowitz learned that most people’s preferences fell into one of three broad groups: plain, spicy, and extra-chunky, and of those three the last was the most important.  Why? Because at the time there was no extra-chunky spaghetti sauce in the supermarket.  Over the next decade, that new category proved to be worth hundreds of millions of dollars to Prego.  ”We all said, ‘Wow!’ ” Monica Wood, who was then the head of market research for Campbell’s, recalls.  ”Here there was this third segment—people who liked their spaghetti sauce with lots of stuff in it—and it was completely untapped.  So in about 1989-90 we launched Prego extra-chunky.  It was extraordinarily successful.”

It may be hard today, fifteen years later—when every brand seems to come in multiple varieties—to appreciate how much of a breakthrough this was.  In those years, people in the food industry carried around in their heads the notion of a platonic dish—the version of a dish that looked and tasted absolutely right.  At Ragú and Prego, they had been striving for the platonic spaghetti sauce, and the platonic spaghetti sauce was thin and blended because that’s the way they thought it was done in Italy.  Cooking, on the industrial level, was consumed with the search for human universals.  Once you start looking for the sources of human variability, though, the old orthodoxy goes out the window.  Howard Moskowitz stood up to the Platonists and said there are no universals.

Moskowitz still has a version of the computer model he used for Prego fifteen years ago.  It has all the coded results from the consumer taste tests and the expert tastings, split into the three categories (plain, spicy, and extra-chunky) and linked up with the actual ingredients list on a spreadsheet.  ”You know how they have a computer model for building an aircraft,” Moskowitz said as he pulled up the program on his computer.  ”This is a model for building spaghetti sauce.  Look, every variable is here.”  He pointed at column after column of ratings.  ”So here are the ingredients.  I’m a brand manager for Prego.  I want to optimize one of the segments.  Let’s start with Segment 1.”  In Moskowitz’s program, the three spaghetti-sauce groups were labelled Segment 1, Segment 2, and Segment 3.  He typed in a few commands, instructing the computer to give him the formulation that would score the highest with those people in Segment 1.  The answer appeared almost immediately: a specific recipe that, according to Moskowitz’s data, produced a score of 78 from the people in Segment 1.  But that same formulation didn’t do nearly as well with those in Segment 2 and Segment 3.  They scored it 67 and 57, respectively.  Moskowitz started again, this time asking the computer to optimize for Segment 2.  This time the ratings came in at 82, but now Segment 1 had fallen ten points, to 68.  ”See what happens?” he said.  ”If I make one group happier, I piss off another group.  We did this for coffee with General Foods, and we found that if you create only one product the best you can get across all the segments is a 60—if you’re lucky.  That’s if you were to treat everybody as one big happy family.  But if I do the sensory segmentation, I can get 70, 71, 72.  Is that big? Ahhh.  It’s a very big difference.  In coffee, a 71 is something you’ll die for.”

When Jim Wigon set up shop that day in Zabar’s, then, his operating assumption was that there ought to be some segment of the population that preferred a ketchup made with Stanislaus tomato paste and hand-chopped basil and maple syrup.  That’s the Moskowitz theory.  But there is theory and there is practice.  By the end of that long day, Wigon had sold ninety jars.  But he’d also got two parking tickets and had to pay for a hotel room, so he wasn’t going home with money in his pocket.  For the year, Wigon estimates, he’ll sell fifty thousand jars—which, in the universe of condiments, is no more than a blip.  ”I haven’t drawn a paycheck in five years,” Wigon said as he impaled another meatball on a toothpick.  ”My wife is killing me.”  And it isn’t just World’s Best that is struggling.  In the gourmet-ketchup world, there is River Run and Uncle Dave’s, from Vermont, and Muir Glen Organic and Mrs.  Tomato Head Roasted Garlic Peppercorn Catsup, in California, and dozens of others—and every year Heinz’s overwhelming share of the ketchup market just grows.

It is possible, of course, that ketchup is waiting for its own version of that Rolls-Royce commercial, or the discovery of the ketchup equivalent of extra-chunky—the magic formula that will satisfy an unmet need.  It is also possible, however, that the rules of Howard Moskowitz, which apply to Grey Poupon and Prego spaghetti sauce and to olive oil and salad dressing and virtually everything else in the supermarket, don’t apply to ketchup.

3.

Tomato ketchup is a nineteenth-century creation—the union of the English tradition of fruit and vegetable sauces and the growing American infatuation with the tomato.  But what we know today as ketchup emerged out of a debate that raged in the first years of the last century over benzoate, a preservative widely used in late-nineteenth-century condiments.  Harvey Washington Wiley, the chief of the Bureau of Chemistry in the Department of Agriculture from 1883 to 1912, came to believe that benzoates were not safe, and the result was an argument that split the ketchup world in half.  On one side was the ketchup establishment, which believed that it was impossible to make ketchup without benzoate and that benzoate was not harmful in the amounts used.  On the other side was a renegade band of ketchup manufacturers, who believed that the preservative puzzle could be solved with the application of culinary science.  The dominant nineteenth-century ketchups were thin and watery, in part because they were made from unripe tomatoes, which are low in the complex carbohydrates known as pectin, which add body to a sauce.  But what if you made ketchup from ripe tomatoes, giving it the density it needed to resist degradation? Nineteenth-century ketchups had a strong tomato taste, with just a light vinegar touch.  The renegades argued that by greatly increasing the amount of vinegar, in effect protecting the tomatoes by pickling them, they were making a superior ketchup: safer, purer, and better tasting.  They offered a money-back guarantee in the event of spoilage.  They charged more for their product, convinced that the public would pay more for a better ketchup, and they were right.  The benzoate ketchups disappeared.  The leader of the renegade band was an entrepreneur out of Pittsburgh named Henry J.  Heinz.

The world’s leading expert on ketchup’s early years is Andrew F.  Smith, a substantial man, well over six feet, with a graying mustache and short wavy black hair.  Smith is a scholar, trained as a political scientist, intent on bringing rigor to the world of food.  When we met for lunch not long ago at the restaurant Savoy in SoHo (chosen because of the excellence of its hamburger and French fries, and because Savoy makes its own ketchup—a dark, peppery, and viscous variety served in a white porcelain saucer), Smith was in the throes of examining the origins of the croissant for the upcoming “Oxford Encyclopedia of Food and Drink in America,” of which he is the editor-in-chief.  Was the croissant invented in 1683, by the Viennese, in celebration of their defeat of the invading Turks? Or in 1686, by the residents of Budapest, to celebrate their defeat of the Turks? Both explanations would explain its distinctive crescent shape—since it would make a certain cultural sense (particularly for the Viennese) to consecrate their battlefield triumphs in the form of pastry.  But the only reference Smith could find to either story was in the Larousse Gastronomique of 1938.  ”It just doesn’t check out,” he said, shaking his head wearily.

Smith’s specialty is the tomato, however, and over the course of many scholarly articles and books—”The History of Home-Made Anglo-American Tomato Ketchup,” for Petits Propos Culinaires, for example, and “The Great Tomato Pill War of the 1830's,” for The Connecticut Historical Society Bulletin—Smith has argued that some critical portion of the history of culinary civilization could be told through this fruit.  Cortez brought tomatoes to Europe from the New World, and they inexorably insinuated themselves into the world’s cuisines.  The Italians substituted the tomato for eggplant.  In northern India, it went into curries and chutneys.  ”The biggest tomato producer in the world today?” Smith paused, for dramatic effect.  ”China.  You don’t think of tomato being a part of Chinese cuisine, and it wasn’t ten years ago.  But it is now.”  Smith dipped one of my French fries into the homemade sauce.  ”It has that raw taste,” he said, with a look of intense concentration.  ”It’s fresh ketchup.  You can taste the tomato.”  Ketchup was, to his mind, the most nearly perfect of all the tomato’s manifestations.  It was inexpensive, which meant that it had a firm lock on the mass market, and it was a condiment, not an ingredient, which meant that it could be applied at the discretion of the food eater, not the food preparer.  ”There’s a quote from Elizabeth Rozin I’ve always loved,” he said.  Rozin is the food theorist who wrote the essay “Ketchup and the Collective Unconscious,” and Smith used her conclusion as the epigraph of his ketchup book: ketchup may well be “the only true culinary expression of the melting pot, and .  .  .  its special and unprecedented ability to provide something for everyone makes it the Esperanto of cuisine.”  Here is where Henry Heinz and the benzoate battle were so important: in defeating the condiment Old Guard, he was the one who changed the flavor of ketchup in a way that made it universal.

4.

There are five known fundamental tastes in the human palate: salty, sweet, sour, bitter, and umami.  Umami is the proteiny, full-bodied taste of chicken soup, or cured meat, or fish stock, or aged cheese, or mother’s milk, or soy sauce, or mushrooms, or seaweed, or cooked tomato.  ”Umami adds body,” Gary Beauchamp, who heads the Monell Chemical Senses Center, in Philadelphia, says.  ”If you add it to a soup, it makes the soup seem like it’s thicker—it gives it sensory heft.  It turns a soup from salt water into a food.”  When Heinz moved to ripe tomatoes and increased the percentage of tomato solids, he made ketchup, first and foremost, a potent source of umami.  Then he dramatically increased the concentration of vinegar, so that his ketchup had twice the acidity of most other ketchups; now ketchup was sour, another of the fundamental tastes.  The post-benzoate ketchups also doubled the concentration of sugar—so now ketchup was also sweet—and all along ketchup had been salty and bitter.  These are not trivial issues.  Give a baby soup, and then soup with MSG (an amino-acid salt that is pure umami), and the baby will go back for the MSG soup every time, the same way a baby will always prefer water with sugar to water alone.  Salt and sugar and umami are primal signals about the food we are eating—about how dense it is in calories, for example, or, in the case of umami, about the presence of proteins and amino acids.  What Heinz had done was come up with a condiment that pushed all five of these primal buttons.  The taste of Heinz’s ketchup began at the tip of the tongue, where our receptors for sweet and salty first appear, moved along the sides, where sour notes seem the strongest, then hit the back of the tongue, for umami and bitter, in one long crescendo.  How many things in the supermarket run the sensory spectrum like this?

A number of years ago, the H.  J.  Heinz Company did an extensive market-research project in which researchers went into people’s homes and watched the way they used ketchup.  ”I remember sitting in one of those households,” Casey Keller, who was until recently the chief growth officer for Heinz, says.  ”There was a three-year-old and a six-year-old, and what happened was that the kids asked for ketchup and Mom brought it out.  It was a forty-ounce bottle.  And the three-year-old went to grab it himself, and Mom intercepted the bottle and said, ‘No, you’re not going to do that.’ She physically took the bottle away and doled out a little dollop.  You could see that the whole thing was a bummer.”  For Heinz, Keller says, that moment was an epiphany.  A typical five-year-old consumes about sixty per cent more ketchup than a typical forty-year-old, and the company realized that it needed to put ketchup in a bottle that a toddler could control.  ”If you are four—and I have a four-year-old—he doesn’t get to choose what he eats for dinner, in most cases,” Keller says.  ”But the one thing he can control is ketchup.  It’s the one part of the food experience that he can customize and personalize.”  As a result, Heinz came out with the so-called EZ Squirt bottle, made out of soft plastic with a conical nozzle.  In homes where the EZ Squirt is used, ketchup consumption has grown by as much as twelve per cent.

There is another lesson in that household scene, though.  Small children tend to be neophobic: once they hit two or three, they shrink from new tastes.  That makes sense, evolutionarily, because through much of human history that is the age at which children would have first begun to gather and forage for themselves, and those who strayed from what was known and trusted would never have survived.  There the three-year-old was, confronted with something strange on his plate—tuna fish, perhaps, or Brussels sprouts—and he wanted to alter his food in some way that made the unfamiliar familiar.  He wanted to subdue the contents of his plate.  And so he turned to ketchup, because, alone among the condiments on the table, ketchup could deliver sweet and sour and salty and bitter and umami, all at once.

5.

Last February, Edgar Chambers IV, who runs the sensory-analysis center at Kansas State University, conducted a joint assessment of World’s Best and Heinz.  He has seventeen trained tasters on his staff, and they work for academia and industry, answering the often difficult question of what a given substance tastes like.  It is demanding work.  Immediately after conducting the ketchup study, Chambers dispatched a team to Bangkok to do an analysis of fruit—bananas, mangoes, rose apples, and sweet tamarind.  Others were detailed to soy and kimchi in South Korea, and Chambers’s wife led a delegation to Italy to analyze ice cream.

The ketchup tasting took place over four hours, on two consecutive mornings.  Six tasters sat around a large, round table with a lazy Susan in the middle.  In front of each panelist were two one-ounce cups, one filled with Heinz ketchup and one filled with World’s Best.  They would work along fourteen dimensions of flavor and texture, in accordance with the standard fifteen-point scale used by the food world.  The flavor components would be divided two ways: elements picked up by the tongue and elements picked up by the nose.  A very ripe peach, for example, tastes sweet but it also smells sweet—which is a very different aspect of sweetness.  Vinegar has a sour taste but also a pungency, a vapor that rises up the back of the nose and fills the mouth when you breathe out.  To aid in the rating process, the tasters surrounded themselves with little bowls of sweet and sour and salty solutions, and portions of Contadina tomato paste, Hunt’s tomato sauce, and Campbell’s tomato juice, all of which represent different concentrations of tomato-ness.

After breaking the ketchup down into its component parts, the testers assessed the critical dimension of “amplitude,” the word sensory experts use to describe flavors that are well blended and balanced, that “bloom” in the mouth.  ”The difference between high and low amplitude is the difference between my son and a great pianist playing ‘Ode to Joy’ on the piano,” Chambers says.  ”They are playing the same notes, but they blend better with the great pianist.”  Pepperidge Farm shortbread cookies are considered to have high amplitude.  So are Hellman’s mayonnaise and Sara Lee poundcake.  When something is high in amplitude, all its constituent elements converge into a single gestalt.  You can’t isolate the elements of an iconic, high-amplitude flavor like Coca-Cola or Pepsi.  But you can with one of those private-label colas that you get in the supermarket.  ”The thing about Coke and Pepsi is that they are absolutely gorgeous,” Judy Heylmun, a vice-president of Sensory Spectrum, Inc., in Chatham, New Jersey, says.  ”They have beautiful notes—all flavors are in balance.  It’s very hard to do that well.  Usually, when you taste a store cola it’s”— and here she made a series of pik! pik! pik! sounds—”all the notes are kind of spiky, and usually the citrus is the first thing to spike out.  And then the cinnamon.  Citrus and brown spice notes are top notes and very volatile, as opposed to vanilla, which is very dark and deep.  A really cheap store brand will have a big, fat cinnamon note sitting on top of everything.”

Some of the cheaper ketchups are the same way.  Ketchup aficionados say that there’s a disquieting unevenness to the tomato notes in Del Monte ketchup: Tomatoes vary, in acidity and sweetness and the ratio of solids to liquid, according to the seed variety used, the time of year they are harvested, the soil in which they are grown, and the weather during the growing season.  Unless all those variables are tightly controlled, one batch of ketchup can end up too watery and another can be too strong.  Or try one of the numerous private-label brands that make up the bottom of the ketchup market and pay attention to the spice mix; you may well find yourself conscious of the clove note or overwhelmed by a hit of garlic.  Generic colas and ketchups have what Moskowitz calls a hook—a sensory attribute that you can single out, and ultimately tire of.

The tasting began with a plastic spoon.  Upon consideration, it was decided that the analysis would be helped if the ketchups were tasted on French fries, so a batch of fries were cooked up, and distributed around the table.  Each tester, according to protocol, took the fries one by one, dipped them into the cup—all the way, right to the bottom—bit off the portion covered in ketchup, and then contemplated the evidence of their senses.  For Heinz, the critical flavor components—vinegar, salt, tomato I.D.  (over-all tomato-ness), sweet, and bitter—were judged to be present in roughly equal concentrations, and those elements, in turn, were judged to be well blended.  The World’s Best, though, “had a completely different view, a different profile, from the Heinz,” Chambers said.  It had a much stronger hit of sweet aromatics—4.0 to 2.5—and outstripped Heinz on tomato I.D.  by a resounding 9 to 5.5.  But there was less salt, and no discernible vinegar.  ”The other comment from the panel was that these elements were really not blended at all,” Chambers went on.  ”The World’s Best product had really low amplitude.”  According to Joyce Buchholz, one of the panelists, when the group judged aftertaste, “it seemed like a certain flavor would hang over longer in the case of World’s Best—that cooked-tomatoey flavor.”

But what was Jim Wigon to do? To compete against Heinz, he had to try something dramatic, like substituting maple syrup for corn syrup, ramping up the tomato solids.  That made for an unusual and daring flavor.  World’s Best Dill ketchup on fried catfish, for instance, is a marvellous thing.  But it also meant that his ketchup wasn’t as sensorily complete as Heinz, and he was paying a heavy price in amplitude.  ”Our conclusion was mainly this,” Buchholz said.  ”We felt that World’s Best seemed to be more like a sauce.”  She was trying to be helpful.

There is an exception, then, to the Moskowitz rule.  Today there are thirty-six varieties of Ragú spaghetti sauce, under six rubrics—Old World Style, Chunky Garden Style, Robusto, Light, Cheese Creations, and Rich & Meaty—which means that there is very nearly an optimal spaghetti sauce for every man, woman, and child in America.  Measured against the monotony that confronted Howard Moskowitz twenty years ago, this is progress.  Happiness, in one sense, is a function of how closely our world conforms to the infinite variety of human preference.  But that makes it easy to forget that sometimes happiness can be found in having what we’ve always had and everyone else is having.  ”Back in the seventies, someone else—I think it was Ragú—tried to do an ‘Italian’-style ketchup,” Moskowitz said.  ”They failed miserably.”  It was a conundrum: what was true about a yellow condiment that went on hot dogs was not true about a tomato condiment that went on hamburgers, and what was true about tomato sauce when you added visible solids and put it in a jar was somehow not true about tomato sauce when you added vinegar and sugar and put it in a bottle.  Moskowitz shrugged.  ”I guess ketchup is ketchup.”

© 2013 Malcolm Gladwell.

DAVID AND GOLIATH
 
WHAT THE DOG SAW
 
OUTLIERS
 
BLINK
 
THE TIPPING POINT
 
ARTICLES
 
ETC.
The Trouble with Fries

Posted March 5, 2001 by MALCOLM GLADWELL & filed under ANNALS OF EATING, THE NEW YORKER - ARCHIVE.

Fast food is killing us. Can it be fixed?

1.

In 1954, a man named Ray Kroc, who made his living selling the five-spindle Multimixer milkshake machine, began hearing about a hamburger stand in San Bernardino, California. This particular restaurant, he was told, had no fewer than eight of his machines in operation, meaning that it could make forty shakes simultaneously. Kroc was astounded. He flew from Chicago to Los Angeles, and drove to San Bernardino, sixty miles away, where he found a small octagonal building on a corner lot. He sat in his car and watched as the workers showed up for the morning shift. They were in starched white shirts and paper hats, and moved with a purposeful discipline. As lunchtime approached, customers began streaming into the parking lot, lining up for bags of hamburgers. Kroc approached a strawberry blonde in a yellow convertible.

“How often do you come here?” he asked.

“Anytime I am in the neighborhood,” she replied, and, Kroc would say later, “it was not her sex appeal but the obvious relish with which she devoured the hamburger that made my pulse begin to hammer with excitement.” He came back the next morning, and this time set up inside the kitchen, watching the griddle man, the food preparers, and, above all, the French-fry operation, because it was the French fries that truly captured his imagination. They were made from top-quality oblong Idaho russets, eight ounces apiece, deep-fried to a golden brown, and salted with a shaker that, as he put it, kept going like a Salvation Army girl’s tambourine. They were crispy on the outside and buttery soft on the inside, and that day Kroc had a vision of a chain of restaurants, just like the one in San Bernardino, selling golden fries from one end of the country to the other. He asked the two brothers who owned the hamburger stand if he could buy their franchise rights. They said yes. Their names were Mac and Dick McDonald.

Ray Kroc was the great visionary of American fast food, the one who brought the lessons of the manufacturing world to the restaurant business. Before the fifties, it was impossible, in most American towns, to buy fries of consistent quality. Ray Kroc was the man who changed that. “The french fry,” he once wrote, “would become almost sacrosanct for me, its preparation a ritual to be followed religiously.” A potato that has too great a percentage of water–and potatoes, even the standard Idaho russet burbank, vary widely in their water content–will come out soggy at the end of the frying process. It was Kroc, back in the fifties, who sent out field men, armed with hydrometers, to make sure that all his suppliers were producing potatoes in the optimal solids range of twenty to twenty-three per cent. Freshly harvested potatoes, furthermore, are rich in sugars, and if you slice them up and deep-fry them the sugars will caramelize and brown the outside of the fry long before the inside is cooked. To make a crisp French fry, a potato has to be stored at a warm temperature for several weeks in order to convert those sugars to starch. Here Kroc led the way as well, mastering the art of “curing” potatoes by storing them under a giant fan in the basement of his first restaurant, outside Chicago.

Perhaps his most enduring achievement, though, was the so-called potato computer–developed for McDonald’s by a former electrical engineer for Motorola named Louis Martino–which precisely calibrated the optimal cooking time for a batch of fries. (The key: when a batch of cold raw potatoes is dumped into a vat of cooking oil, the temperature of the fat will drop and then slowly rise. Once the oil has risen three degrees, the fries are ready.) Previously, making high-quality French fries had been an art. The potato computer, the hydrometer, and the curing bins made it a science. By the time Kroc was finished, he had figured out how to turn potatoes into an inexpensive snack that would always be hot, salty, flavorful, and crisp, no matter where or when you bought it.

This was the first fast-food revolution–the mass production of food that had reliable mass appeal. But today, as the McDonald’s franchise approaches its fiftieth anniversary, it is clear that fast food needs a second revolution. As many Americans now die every year from obesity-related illnesses–heart disease and complications of diabetes–as from smoking, and the fast-food toll grows heavier every year. In the fine new book “Fast Food Nation,” the journalist Eric Schlosser writes of McDonald’s and Burger King in the tone usually reserved for chemical companies, sweatshops, and arms dealers, and, as shocking as that seems at first, it is perfectly appropriate. Ray Kroc’s French fries are killing us. Can fast food be fixed?

2.

Fast-food French fries are made from a baking potato like an Idaho russet, or any other variety that is mealy, or starchy, rather than waxy. The potatoes are harvested, cured, washed, peeled, sliced, and then blanched–cooked enough so that the insides have a fluffy texture but not so much that the fry gets soft and breaks. Blanching is followed by drying, and drying by a thirty-second deep fry, to give the potatoes a crisp shell. Then the fries are frozen until the moment of service, when they are deep-fried again, this time for somewhere around three minutes. Depending on the fast-food chain involved, there are other steps interspersed in this process. McDonald’s fries, for example, are briefly dipped in a sugar solution, which gives them their golden-brown color; Burger King fries are dipped in a starch batter, which is what gives those fries their distinctive hard shell and audible crunch. But the result is similar. The potato that is first harvested in the field is roughly eighty per cent water. The process of creating a French fry consists, essentially, of removing as much of that water as possible–through blanching, drying, and deep-frying–and replacing it with fat.

Elisabeth Rozin, in her book “The Primal Cheeseburger,” points out that the idea of enriching carbohydrates with fat is nothing new. It’s a standard part of the cuisine of almost every culture. Bread is buttered; macaroni comes with cheese; dumplings are fried; potatoes are scalloped, baked with milk and cheese, cooked in the dripping of roasting meat, mixed with mayonnaise in a salad, or pan-fried in butterfat as latkes. But, as Rozin argues, deep-frying is in many ways the ideal method of adding fat to carbohydrates. If you put butter on a mashed potato, for instance, the result is texturally unexciting: it simply creates a mush. Pan-frying results in uneven browning and crispness. But when a potato is deep-fried the heat of the oil turns the water inside the potato into steam, which causes the hard granules of starch inside the potato to swell and soften: that’s why the inside of the fry is fluffy and light. At the same time, the outward migration of the steam limits the amount of oil that seeps into the interior, preventing the fry from getting greasy and concentrating the oil on the surface, where it turns the outer layer of the potato brown and crisp. “What we have with the french fry,” Rozin writes, “is a near perfect enactment of the enriching of a starch food with oil or fat.”

This is the trouble with the French fry. The fact that it is cooked in fat makes it unhealthy. But the contrast that deep-frying creates between its interior and its exterior–between the golden shell and the pillowy whiteness beneath–is what makes it so irresistible. The average American now eats a staggering thirty pounds of French fries a year, up from four pounds when Ray Kroc was first figuring out how to mass-produce a crisp fry. Meanwhile, fries themselves have become less healthful. Ray Kroc, in the early days of McDonald’s, was a fan of a hot-dog stand on the North Side of Chicago called Sam’s, which used what was then called the Chicago method of cooking fries. Sam’s cooked its fries in animal fat, and Kroc followed suit, prescribing for his franchises a specially formulated beef tallow called Formula 47 (in reference to the forty-seven-cent McDonald’s “All-American meal” of the era: fifteen-cent hamburger, twelve-cent fries, twenty-cent shake). Among aficionados, there is general agreement that those early McDonald’s fries were the finest mass-market fries ever made: the beef tallow gave them an unsurpassed rich, buttery taste. But in 1990, in the face of public concern about the health risks of cholesterol in animal-based cooking oil, McDonald’s and the other major fast-food houses switched to vegetable oil. That wasn’t an improvement, however. In the course of making vegetable oil suitable for deep frying, it is subjected to a chemical process called hydrogenation, which creates a new substance called a trans unsaturated fat. In the hierarchy of fats, polyunsaturated fats–the kind found in regular vegetable oils–are the good kind; they lower your cholesterol. Saturated fats are the bad kind. But trans fats are worse: they wreak havoc with the body’s ability to regulate cholesterol.

According to a recent study involving some eighty thousand women, for every five-per-cent increase in the amount of saturated fats that a woman consumes, her risk of heart disease increases by seventeen per cent. But only a two-per-cent increase in trans fats will increase her heart-disease risk by ninety-three per cent. Walter Willett, an epidemiologist at Harvard–who helped design the study–estimates that the consumption of trans fats in the United States probably causes about thirty thousand premature deaths a year.

McDonald’s and the other fast-food houses aren’t the only purveyors of trans fats, of course; trans fats are in crackers and potato chips and cookies and any number of other processed foods. Still, a lot of us get a great deal of our trans fats from French fries, and to read the medical evidence on trans fats is to wonder at the odd selectivity of the outrage that consumers and the legal profession direct at corporate behavior. McDonald’s and Burger King and Wendy’s have switched to a product, without disclosing its risks, that may cost human lives. What is the difference between this and the kind of thing over which consumers sue companies every day?

3.

The French-fry problem ought to have a simple solution: cook fries in oil that isn’t so dangerous. Oils that are rich in monounsaturated fats, like canola oil, aren’t nearly as bad for you as saturated fats, and are generally stable enough for deep-frying. It’s also possible to “fix” animal fats so that they aren’ t so problematic. For example, K. C. Hayes, a nutritionist at Brandeis University, has helped develop an oil called Appetize. It’s largely beef tallow, which gives it a big taste advantage over vegetable shortening, and makes it stable enough for deep-frying. But it has been processed to remove the cholesterol, and has been blended with pure corn oil, in a combination that Hayes says removes much of the heart-disease risk.

Perhaps the most elegant solution would be for McDonald’s and the other chains to cook their fries in something like Olestra, a fat substitute developed by Procter & Gamble. Ordinary fats are built out of a molecular structure known as a triglyceride: it’s a microscopic tree, with a trunk made of glycerol and three branches made of fatty acids. Our bodies can’t absorb triglycerides, so in the digestive process each of the branches is broken off by enzymes and absorbed separately. In the production of Olestra, the glycerol trunk of a fat is replaced with a sugar, which has room for not three but eight fatty acids. And our enzymes are unable to break down a fat tree with eight branches–so the Olestra molecule can’t be absorbed by the body at all. “Olestra” is as much a process as a compound: you can create an “Olestra” version of any given fat. Potato chips, for instance, tend to be fried in cottonseed oil, because of its distinctively clean taste. Frito-Lay’s no-fat Wow! chips are made with an Olestra version of cottonseed oil, which behaves just like regular cottonseed oil except that it’s never digested. A regular serving of potato chips has a hundred and fifty calories, ninety of which are fat calories from the cooking oil. A serving of Wow! chips has seventy-five calories and no fat. If Procter & Gamble were to seek F.D.A. approval for the use of Olestra in commercial deep-frying (which it has not yet done), it could make an Olestra version of the old McDonald’s Formula 47, which would deliver every nuance of the old buttery, meaty tallow at a fraction of the calories.

Olestra, it must be said, does have some drawbacks–in particular, a reputation for what is delicately called “gastrointestinal distress.” The F.D.A. has required all Olestra products to carry a somewhat daunting label saying that they may cause “cramping and loose stools.” Not surprisingly, sales have been disappointing, and Olestra has never won the full acceptance of the nutrition community. Most of this concern, however, appears to be overstated. Procter & Gamble has done randomized, double-blind studies–one of which involved more than three thousand people over six weeks–and found that people eating typical amounts of Olestra-based chips don’t have significantly more gastrointestinal problems than people eating normal chips. Diarrhea is such a common problem in America–nearly a third of adults have at least one episode each month–that even F.D.A. regulators now appear to be convinced that in many of the complaints they received Olestra was unfairly blamed for a problem that was probably caused by something else. The agency has promised Procter & Gamble that the warning label will be reviewed.

Perhaps the best way to put the Olestra controversy into perspective is to compare it to fibre. Fibre is vegetable matter that goes right through you: it’s not absorbed by the gastrointestinal tract. Nutritionists tell us to eat it because it helps us lose weight and it lowers cholesterol–even though if you eat too many baked beans or too many bowls of oat bran you will suffer the consequences. Do we put warning labels on boxes of oat bran? No, because the benefits of fibre clearly outweigh its drawbacks. Research has suggested that Olestra, like fibre, helps people lose weight and lowers cholesterol; too much Olestra, like too much fibre, may cause problems. (Actually, too much Olestra may not be as troublesome as too much bran. According to Procter &amp; Gamble, eating a large amount of Olestra–forty grams–causes no more problems than eating a small bowl–twenty grams–of wheat bran.) If we had Olestra fries, then, they shouldn’t be eaten for breakfast, lunch, and dinner. In fact, fast-food houses probably shouldn’t use hundred-per-cent Olestra; they should cook their fries in a blend, using the Olestra to displace the most dangerous trans and saturated fats. But these are minor details. The point is that it is entirely possible, right now, to make a delicious French fry that does not carry with it a death sentence. A French fry can be much more than a delivery vehicle for fat.

4.

Is it really that simple, though? Consider the cautionary tale of the efforts of a group of food scientists at Auburn University, in Alabama, more than a decade ago to come up with a better hamburger. The Auburn team wanted to create a leaner beef that tasted as good as regular ground beef. They couldn’t just remove the fat, because that would leave the meat dry and mealy. They wanted to replace the fat. “If you look at ground beef, it contains moisture, fat, and protein,” says Dale Huffman, one of the scientists who spearheaded the Auburn project. “Protein is relatively constant in all beef, at about twenty per cent. The traditional McDonald’s ground beef is around twenty per cent fat. The remainder is water. So you have an inverse ratio of water and fat. If you reduce fat, you need to increase water.” The goal of the Auburn scientists was to cut about two-thirds of the fat from normal ground beef, which meant that they needed to find something to add to the beef that would hold an equivalent amount of water–and continue to retain that water even as the beef was being grilled. Their choice? Seaweed, or, more precisely, carrageenan. “It’s been in use for centuries,” Huffman explains. “It’s the stuff that keeps the suspension in chocolate milk–otherwise the chocolate would settle at the bottom. It has tremendous water-holding ability. There’s a loose bond between the carrageenan and the moisture.” They also selected some basic flavor enhancers, designed to make up for the lost fat “taste.” The result was a beef patty that was roughly three-quarters water, twenty per cent protein, five per cent or so fat, and a quarter of a per cent seaweed. They called it AU Lean.

It didn’t take the Auburn scientists long to realize that they had created something special. They installed a test kitchen in their laboratory, got hold of a McDonald’s grill, and began doing blind taste comparisons of AU Lean burgers and traditional twenty- per-cent-fat burgers. Time after time, the AU Lean burgers won. Next, they took their invention into the field. They recruited a hundred families and supplied them with three kinds of ground beef for home cooking over consecutive three-week intervals–regular “market” ground beef with twenty per cent fat, ground beef with five percent fat, and AU Lean. The families were asked to rate the different kinds of beef, without knowing which was which. Again, the AU Lean won hands down–trumping the other two on “likability,” “tenderness,” “flavorfulness,” and “juiciness.”

What the Auburn team showed was that, even though people love the taste and feel of fat–and naturally gravitate toward high-fat food–they can be fooled into thinking that there is a lot of fat in something when there isn’t. Adam Drewnowski, a nutritionist at the University of Washington, has found a similar effect with cookies. He did blind taste tests of normal and reduced-calorie brownies, biscotti, and chocolate-chip, oatmeal, and peanut-butter cookies. If you cut the sugar content of any of those cookies by twenty-five per cent, he found, people like the cookies much less. But if you cut the fat by twenty-five per cent they barely notice. “People are very finely attuned to how much sugar there is in a liquid or a solid,” Drewnowski says. “For fat, there’s no sensory break point. Fat comes in so many guises and so many textures it is very difficult to perceive how much is there.” This doesn’t mean we are oblivious of fat levels, of course. Huffman says that when his group tried to lower the fat in AU Lean below five per cent, people didn’t like it anymore. But, within the relatively broad range of between five and twenty-five per cent, you can add water and some flavoring and most people can’t tell the difference.

What’s more, people appear to be more sensitive to the volume of food they consume than to its calorie content. Barbara Rolls, a nutritionist at Penn State, has demonstrated this principle with satiety studies. She feeds one group of people a high-volume snack and another group a low-volume snack. Even though the two snacks have the same calorie count, she finds that people who eat the high-volume snack feel more satisfied. “People tend to eat a constant weight or volume of food in a given day, not a constant portion of calories,” she says. Eating AU Lean, in short, isn’t going to leave you with a craving for more calories; you’ll feel just as full.

For anyone looking to improve the quality of fast food, all this is heartening news. It means that you should be able to put low-fat cheese and low-fat mayonnaise in a Big Mac without anyone’s complaining. It also means that there’s no particular reason to use twenty-per-cent-fat ground beef in a fast-food burger. In 1990, using just this argument, the Auburn team suggested to McDonald’s that it make a Big Mac out of AU Lean. Shortly thereafter, McDonald’s came out with the McLean Deluxe. Other fast-food houses scrambled to follow suit. Nutritionists were delighted. And fast food appeared on the verge of a revolution.

Only, it wasn’t. The McLean was a flop, and four years later it was off the market. What happened? Part of the problem appears to have been that McDonald’s rushed the burger to market before many of the production kinks had been worked out. More important, though, was the psychological handicap the burger faced. People liked AU Lean in blind taste tests because they didn’t know it was AU Lean; they were fooled into thinking it was regular ground beef. But nobody was fooled when it came to the McLean Deluxe. It was sold as the healthy choice–and who goes to McDonald’s for health food?

Leann Birch, a developmental psychologist at Penn State, has looked at the impact of these sorts of expectations on children. In one experiment, she took a large group of kids and fed them a big lunch. Then she turned them loose in a room with lots of junk food. “What we see is that some kids eat almost nothing,” she says. “But other kids really chow down, and one of the things that predicts how much they eat is the extent to which parents have restricted their access to high-fat, high-sugar food in the past: the more the kids have been restricted, the more they eat.” Birch explains the results two ways. First, restricting food makes kids think not in terms of their own hunger but in terms of the presence and absence of food. As she puts it, “The kid is essentially saying, ‘If the food’s here I better get it while I can, whether or not I’m hungry.’ We see these five-year-old kids eating as much as four hundred calories.” Birch’s second finding, though, is more important. Because the children on restricted diets had been told that junk food was bad for them, they clearly thought that it had to taste good. When it comes to junk food, we seem to follow an implicit script that powerfully biases the way we feel about food. We like fries not in spite of the fact that they’re unhealthy but because of it.

That is sobering news for those interested in improving the American diet. For years, the nutrition movement in this country has made transparency one of its principal goals: it has assumed that the best way to help people improve their diets is to tell them precisely what’s in their food, to label certain foods good and certain foods bad. But transparency can backfire, because sometimes nothing is more deadly for our taste buds than the knowledge that what we are eating is good for us. McDonald’s should never have called its new offering the McLean Deluxe, in other words. They should have called it the Burger Supreme or the Monster Burger, and then buried the news about reduced calories and fat in the tiniest type on the remotest corner of their Web site. And if we were to cook fries in some high-tech, healthful cooking oil–whether Olestrized beef tallow or something else with a minimum of trans and saturated fats–the worst thing we could do would be to market them as healthy fries. They will not taste nearly as good if we do. They have to be marketed as better fries, as Classic Fries, as fries that bring back the rich tallowy taste of the original McDonald’s.

What, after all, was Ray Kroc’s biggest triumph? A case could be made for the field men with their hydrometers, or the potato-curing techniques, or the potato computer, which turned the making of French fries from an art into a science. But we should not forget Ronald McDonald, the clown who made the McDonald’s name irresistible to legions of small children. Kroc understood that taste comprises not merely the food on our plate but also the associations and assumptions and prejudices we bring to the table–that half the battle in making kids happy with their meal was calling what they were eating a Happy Meal. The marketing of healthful fast food will require the same degree of subtlety and sophistication. The nutrition movement keeps looking for a crusader–someone who will bring about better public education and tougher government regulations. But we need much more than that. We need another Ray Kroc.

© 2013 Malcolm Gladwell.

DAVID AND GOLIATH
 
WHAT THE DOG SAW
 
OUTLIERS
 
BLINK
 
THE TIPPING POINT
 
ARTICLES
 
ETC.
The Bakeoff

Posted September 5, 2005 by MALCOLM GLADWELL & filed under ANNALS OF TECHNOLOGY, THE NEW YORKER - ARCHIVE.

Project Delta aims to create the perfect cookie.

1.

Steve Gundrum launched Project Delta at a small dinner last fall at Il Fornaio, in Burlingame, just down the road from the San Francisco Airport. It wasn’t the first time he’d been to Il Fornaio, and he made his selection quickly, with just a glance at the menu; he is the sort of person who might have thought about his choice in advance — maybe even that morning, while shaving. He would have posed it to himself as a question — Ravioli alla Lucana?—and turned it over in his mind, assembling and disassembling the dish, ingredient by ingredient, as if it were a model airplane. Did the Pecorino pepato really belong? What if you dropped the basil? What would the ravioli taste like if you froze it, along with the ricotta and the Parmesan, and tried to sell it in the supermarket? And then what would you do about the fennel?

Gundrum is short and round. He has dark hair and a mustache and speaks with the flattened vowels of the upper Midwest. He is voluble and excitable and doggedly unpretentious, to the point that your best chance of seeing him in a suit is probably Halloween. He runs Mattson, one of the country’s foremost food research-and-development firms, which is situated in a low-slung concrete-and-glass building in a nondescript office park in Silicon Valley. Gundrum’s office is a spare, windowless room near the rear, and all day long white-coated technicians come to him with prototypes in little bowls, or on skewers, or in Tupperware containers. His job is to taste and advise, and the most common words out of his mouth are “I have an idea.” Just that afternoon, Gundrum had ruled on the reformulation of a popular spinach dip (which had an unfortunate tendency to smell like lawn clippings) and examined the latest iteration of a low-carb kettle corn for evidence of rhythmic munching (the metronomic hand-to-mouth cycle that lies at the heart of any successful snack experience). Mattson created the shelf-stable Mrs. Fields Chocolate Chip Cookie, the new Boca Burger products for Kraft Foods, Orville Redenbacher’s Butter Toffee Popcorn Clusters, and so many other products that it is impossible to walk down the aisle of a supermarket and not be surrounded by evidence of the company’s handiwork.

That evening, Gundrum had invited two of his senior colleagues at Mattson — Samson Hsia and Carol Borba — to dinner, along with Steven Addis, who runs a prominent branding firm in the Bay Area. They sat around an oblong table off to one side of the dining room, with the sun streaming in the window, and Gundrum informed them that he intended to reinvent the cookie, to make something both nutritious and as “indulgent” as the premium cookies on the supermarket shelf. “We want to delight people,” he said. “We don’t want some ultra-high-nutrition power bar, where you have to rationalize your consumption.” He said it again: “We want to delight people.”

As everyone at the table knew, a healthful, good-tasting cookie is something of a contradiction. A cookie represents the combination of three unhealthful ingredients—sugar, white flour, and shortening. The sugar adds sweetness, bulk, and texture: along with baking powder, it produces the tiny cell structures that make baked goods light and fluffy. The fat helps carry the flavor. If you want a big hit of vanilla, or that chocolate taste that really blooms in the nasal cavities, you need fat. It also keeps the strands of gluten in the flour from getting too tightly bound together, so that the cookie stays chewable. The ¦our, of course, gives the batter its structure, and, with the sugar, provides the base for the browning reaction that occurs during baking. You could replace the standard white flour with wheat flour, which is higher in fibre, but fibre adds grittiness. Over the years, there have been many attempts to resolve these contradictions — from Snackwells and diet Oreos to the dry, grainy hockey pucks that pass for cookies in health-food stores — but in every case ¦flavor or fluffiness or tenderness has been compromised. Steve Gundrum was undeterred. He told his colleagues that he wanted Project Delta to create the world’s great-est cookie. He wanted to do it in six months. He wanted to enlist the biggest players in the American food industry. And how would he come up with this wonder cookie? The old-fashioned way. He wanted to hold a bakeoff.

2.

The standard protocol for inventing something in the food industry is called the matrix model. There is a department for product development, which comes up with a new idea, and a department for process development, which figures out how to realize it, and then, down the line, departments for packing, quality assurance, regulatory affairs, chemistry, microbiology, and so on. In a conventional bakeoff, Gundrum would have pitted three identical matrixes against one another and compared the results. But he wasn’t satisfied with the unexamined assumption behind the conventional bakeoff — that there was just one way of inventing something new.

Gundrum had a particular interest, as it happened, in software. He had read widely about it, and once, when he ran into Steve Jobs at an Apple store in the Valley, chatted with him for forty-five minutes on technical matters relating to the Apple operating system. He saw little difference between what he did for a living and what the soft-ware engineers in the surrounding hills of Silicon Valley did. “Lines of code are no different from a recipe,” he explains. “It’s the same thing. You add a little salt, and it tastes better. You write a little piece of code, and it makes the software work faster.” But in the software world, Gundrum knew, there were ongoing debates about the best way to come up with new code.

On the one hand, there was the “open source” movement. Its patron saint was Linus Torvald, the Norwegian hacker who decided to build a free version of Unix, the hugely complicated operating system that runs many of the world’s large computers. Torvald created the basic implementation of his version, which he called Linux, posted it online, and invited people to contribute to its development. Over the years, thousands of programmers had helped, and Linux was now considered as good as proprietary versions of Unix. “Given enough eyeballs all bugs are shallow” was the Linux mantra: a thousand people working for an hour each can do a better job writing and fixing code than a single person working for a thou-sand hours, because the chances are that among those thousand people you can find precisely the right expert for every problem that comes up.

On the other hand, there was the “extreme programming” movement, known as XP, which was led by a legendary programmer named Kent Beck. He called for breaking a problem into the smallest possible increments, and proceeding as simply and modestly as possible. He thought that programmers should work in pairs, two to a computer, passing the keyboard back and forth. Between Beck and Torvald were countless other people, arguing for slightly different variations. But everyone in the software world agreed that trying to get people to be as creative as possible was, as often as not, a social problem: it depended not just on who was on the team but on how the team was organized.

“I remember once I was working with a printing company in Chicago,” Beck says. “The people there were having a terrible problem with their technology. I got there, and I saw that the senior people had these corner offices, and they were working separately and doing things separately that they had trouble integrating later on. So I said, ‘Find a space where you can work together.’ So they found a corner of the machine room. It was a raised floor, ice cold. They just loved it. They would go there five hours a day, making lots of progress. I flew home. They hired me for my technical expertise. And I told them to rearrange the office furniture, and that was the most valuable thing I could offer them.”

It seemed to Gundrum that people in the food world had a great deal to learn from all this. They had become adept at solving what he called “science projects” — problems that required straightforward, linear applications of expensive German machinery and armies of white-coated people with advanced degrees in engineering. Cool Whip was a good example: a product processed so exquisitely — with air bubbles of such fantastic uniformity and stability — that it remains structurally sound for months, at high elevation and at low elevation, frozen and thawed and then refrozen. But coming up with a healthy cookie, which required finessing the inherent contradictions posed by sugar, flour, and shortening, was the kind of problem that the food industry had more trouble with. Gundrum recalled one brainstorming session that a client of his, a major food company, had convened. “This is no joke,” he said. “They played a tape where it sounded like the wind was blowing and the birds were chirping. And they posed us out on a dance floor, and we had to hold our arms out like we were trees and close our eyes, and the ideas were supposed to grow like fruits off the limbs of the trees. Next to me was the head of R. & D., and he looked at me and said: ‘What the hell are we doing here?’”

For Project Delta, Gundrum decreed that there would be three teams, each representing a different methodology of invention. He had read Kent Beck’s writings, and decided that the first would be the XP team. He enlisted two of Mattson’s brightest young associates — Peter Dea and Dan Howell. Dea is a food scientist, who worked as a confectionist before coming to Mattson. He is tall and spare, with short dark hair. “Peter is really good at hitting the high note,” Gundrum said. “If a product needs to have a particular flavor profile, he’s really good at getting that one dimension and getting it right.” Howell is a culinarian-goateed and talkative, a man of enthusiasms who uses high-end Mattson equipment to make an exceptional cup of espresso every afternoon. He started his career as a barista at Starbucks, and then realized that his vocation lay elsewhere. “A customer said to me, ‘What do you want to be doing? Because you clearly don’t want to be here,’” Howell said. “I told him, ‘I want to be sitting in a room working on a better non-fat pudding.’ ”

The second team was headed by Barb Stuckey, an executive vice-president of marketing at Mattson and one of the firm’s stars. She is slender and sleek, with short blond hair. She tends to think out loud, and, because she thinks quickly, she ends up talking quickly, too-in nervous brilliant bursts. Stuckey, Gundrum decided, would represent “managed” research and development—a traditional hierarchical team, as opposed to a partnership like Dea and Howell’s. She would work with Doug Berg, who runs one of Mattson’s product-development teams. Stuckey would draw the big picture. Berg would serve as sounding board and project director. His team would execute their conceptions.

Then Gundrum was at a technology conference in California and heard the software pioneer Mitch Kapor talking about the open-source revolution. Afterward, Gundrum approached Kapor. “I said to Mitch, ‘What do you think? Can I apply this—some of the same principles—outside of software and bring it to the food industry?’” Gundrum recounted. “He stopped and said, ‘Why the hell not!’” So Gundrum invited an élite group of food-industry bakers and scientists to collaborate online. They would be the third team. He signed up a senior person from Mars, Inc., someone from R. & D. at Kraft, the marketing manager for Nestlé Toll House refrigerated/frozen cookie dough, a senior director of R. & D. at Birds Eye Foods, the head of the innovation program for Kellogg’s Morning Foods, the director of seasoning at McCormick, a cookie maven formerly at Keebler, and six more high-level specialists. Mattson’s innovation manager, Carol Borba, who began her career as a line cook at Bouley, in Manhattan, was given the role of project manager. Two Mattson staffers were assigned to carry out the group’s recommendations. This was the Dream Team. It is quite possible that this was the most talented group of people ever to work together in the history of the food industry.

Soon after the launch of Project Delta, Steve Gundrum and his colleague Samson Hsia were standing around, talking about the current products in the supermarket which they particularly admire. “I like the Uncrustable line from Smuckers,” Hsia said. “It’s a frozen sandwich without any crust. It eats very well. You can put it in a lunchbox frozen, and it will be unfrozen by lunchtime.” Hsia is a trim, silver-haired man who is said to know as much about emulsions as anyone in the business. “There’s something else,” he said, suddenly. “We just saw it last week. It’s made by Jennie-O. It’s turkey in a bag.” This was a turkey that was seasoned, plumped with brine, and sold in a heat-resistant plastic bag: the customer simply has to place it in the oven. Hsia began to stride toward the Mattson kitchens, because he realized they actually had a Jennie-O turkey in the back. Gundrum followed, the two men weaving their way through the maze of corridors that make up the Mattson offices. They came to a large freezer. Gundrum pulled out a bright-colored bag. Inside was a second, clear bag, and inside that bag was a twelve-pound turkey. “This is one of my favorite innovations of the last year,” Gundrum said, as Hsia nodded happily. “There is material science involved. There is food science involved. There is positioning involved. You can take this thing, throw it in your oven, and people will be blown away. It’s that good. If I was Butterball, I’d be terrified.”

Jennie-O had taken something old and made it new. But where had that idea come from? Was it a team? A committee? A lone turkey genius? Those of us whose only interaction with such innovations is at the point of sale have a naïve faith in human creativity; we suppose that a world capable of coming up with turkey in a bag is capable of coming up with the next big thing as well—a healthy cookie, a faster computer chip, an automobile engine that gets a hundred miles to the gallon. But if you’re the one responsible for those bright new ideas there is no such certainty. You come up with one great idea, and the process is so miraculous that all you do is puzzle over how on earth you ever did it, and worry whether you’ll ever be able to do it again.

3.

The Mattson kitchens are a series of large, connecting rooms, running along the back of the building. There is a pilot plant in one corner — containing a mini version of the equipment that, say, Heinz would use to make canned soup, a soft-serve ice-cream machine, an industrial-strength pasta-maker, a colloid mill for making oil-and-water emulsions, a flash pasteurizer, and an eighty-five-thousand-dollar Japanese-made coextruder for, among other things, pastry-and-filling combinations. At any given time, the firm may have as many as fifty or sixty projects under way, so the kitchens are a hive of activity, with pressure cookers filled with baked beans bubbling in one corner, and someone rushing from one room to another carrying a tray of pizza slices with experimental toppings.

Dea and Howell, the XP team, took over part of one of the kitchens, setting up at a long stainless-steel lab bench. The countertop was crowded with tins of flour, a big white plastic container of wheat dextrin, a dozen bottles of liquid sweeteners, two plastic bottles of Kirkland olive oil, and, somewhat puzzlingly, three varieties of single-malt Scotch. The Project Delta brief was simple. All cookies had to have fewer than a hundred and thirty calories per serving. Carbohydrates had to be under 17.5 grams, saturated fat under two grams, fibre more than one gram, protein more than two grams, and so on; in other words, the cookie was to be at least fifteen per cent superior to the supermarket average in the major nutritional categories. To Dea and Howell, that suggested oatmeal, and crispy, as opposed to soft. “I’ve tried lots of cookies that are sold as soft and I never like them, because they’re trying to be something that they’re not,” Dea explained. “A soft cookie is a fresh cookie, and what you are trying to do with soft is be a fresh cookie that’s a month old. And that means you need to fake the freshness, to engineer the cookie.”

The two decided to focus on a kind of oatmeal-chocolate-chip hybrid, with liberal applications of roasted soy nuts, toffee, and caramel. A straight oatmeal-raisin cookie or a straight low-cal chocolate-chip cookie was out of the question. This was a reflection of what might be called the Hidden Valley Ranch principle, in honor of a story that Samson Hsia often told about his years working on salad dressing when he was at Clorox. The couple who owned Hidden Valley Ranch, near Santa Barbara, had come up with a seasoning blend of salt, pepper, onion, garlic, and parsley flakes that was mixed with equal parts mayonnaise and buttermilk to make what was, by all accounts, an extraordinary dressing. Clorox tried to bottle it, but found that the buttermilk could not coexist, over any period of time, with the mayonnaise. The way to fix the problem, and preserve the texture, was to make the combination more acidic. But when you increased the acidity you ruined the flavor. Clorox’s food engineers worked on Hidden Valley Ranch dressing for close to a decade. They tried different kinds of processing and stability control and endless cycles of consumer testing before they gave up and simply came out with a high-acid Hidden Valley Ranch dressing — which promptly became a runaway best-seller. Why? Because consumers had never tasted real Hidden Valley Ranch dressing, and as a result had no way of knowing that what they were eating was inferior to the original. For those in the food business, the lesson was unforgettable: if something was new, it didn’t have to be perfect. And, since healthful, indulgent cookies couldn’t be perfect, they had to be new: hence oatmeal, chocolate chips, toffee, and caramel.

Cookie development, at the Mattson level, is a matter of endless iteration, and Dea and Howell began by baking version after version in quick succession — establishing the cookie size, the optimal baking time, the desired variety of chocolate chips, the cut of oats (bulk oats? rolled oats? groats?), the varieties of flour, and the toffee dosage, while testing a variety of high-tech supplements, notably inulin, a fibre source derived from chicory root. As they worked, they made notes on tablet P.C.s, which gave them a running electronic record of each version. “With food, there’s a large circle of pretty good, and we’re solidly in pretty good,” Dea announced, after several intensive days of baking. A tray of cookies was cooling in front of him on the counter. “Typically, that’s when you take it to the customers.”

In this case, the customer was Gundrum, and the next week Howell marched over to Gundrum’s office with two Ziploc bags of cookies in his hand. There was a package of Chips Ahoy! on the table, and Howell took one out. “We’ve been eating these versus Chips Ahoy!,” he said.

The two cookies looked remarkably alike. Gundrum tried one of each. “The Chips Ahoy!, it’s tasty,” he said. “When you eat it, the starch hydrates in your mouth. The XP doesn’t have that same granulated-sugar kind of mouth feel.”

“It’s got more fat than us, though, and subsequently it’s shorter in texture,” Howell said. “And so, when you break it, it breaks more nicely. Ours is a little harder to break.”

By “shorter in texture,” he meant that the cookie “popped” when you bit into it. Saturated fats are solid fats, and give a cookie crispness. Parmesan cheese is short-textured. Brie is long. A shortbread like a Lorna Doone is a classic short-textured cookie. But the XP cookie had, for health reasons, substituted unsaturated fats for saturated fats, and unsaturated fats are liquid. They make the dough stickier, and inevitably compromise a little of that satisfying pop.

“The whole-wheat flour makes us a little grittier, too,” Howell went on. “It has larger particulates.” He broke open one of the Chips Ahoy!. “See how fine the grain is? Now look at one of our cookies. The particulates are larger. It is part of what we lose by going with a healthy profile. If it was just sugar and ¦our, for instance, the carbohydrate chains are going to be shorter, and so they will dissolve more quickly in your mouth. Whereas with more fibre you get longer carbohydrate chains and they don’t dissolve as quickly, and you get that slightly tooth-packing feel.”

“It looks very wholesome, like something you would want to feed your kids,” Gundrum said, finally. They were still only in the realm of pretty good.

4.

Team Stuckey, meanwhile, was having problems of its own. Barb Stuckey’s first thought had been a tea cookie, or, more specifically, a chai cookie — something with cardamom and cinnamon and vanilla and cloves and a soft dairy note. Doug Berg was dispatched to run the experiment. He and his team did three or four rounds of prototypes. The result was a cookie that tasted, astonishingly, like a cup of chai, which was, of course, its problem. Who wanted a cookie that tasted like a cup of chai? Stuckey called a meeting in the Mattson trophy room, where samples of every Mattson product that has made it to market are displayed. After everyone was done tasting the cookies, a bag of them sat in the middle of the table for forty-five minutes—and no one reached to take a second bite. It was a bad sign.

“You know, before the election Good Housekeeping had this cookie bakeoff,” Stuckey said, as the meeting ended. “Laura Bush’s entry was full of chocolate chips and had familiar ingredients. And Teresa Heinz went with pumpkin-spice cookies. I remember thinking, That’s just like the Democrats! So not mainstream! I wanted her to win. But she’s chosen this cookie that’s funky and weird and out of the box. And I kind of feel the same way about the tea cookie. It’s too far out, and will lose to something that’s more comfortable for consumers.”

Stuckey’s next thought involved strawberries and a shortbread base. But shortbread was virtually impossible under the nutritional guidelines: there was no way to get that smooth butter-flour-sugar combination. So Team Stuckey switched to something closer to a strawberry-cobbler cookie, which had the Hidden Valley Ranch advantage that no one knew what a strawberry-cobbler cookie was supposed to taste like. Getting the carbohydrates down to the required 17.5 grams, though, was a struggle, because of how much flour and fruit cobbler requires. The obvious choice to replace the flour was almonds. But nuts have high levels of both saturated and unsaturated fat. “It became a balancing act,” Anne Cristofano, who was doing the bench work for Team Stuckey, said. She baked batch after batch, playing the carbohydrates (first the flour, and then granulated sugar, and finally various kinds of what are called sugar alcohols, low-calorie sweeteners derived from hydrogenizing starch) against the almonds. Cristofano took a version to Stuckey. It didn’t go well.

“We’re not getting enough strawberry impact from the fruit alone,” Stuckey said. “We have to find some way to boost the strawberry.” She nibbled some more. “And, because of the low fat and all that stuff, I don’t feel like we’re getting that pop.”

The Dream Team, by any measure, was the overwhelming Project Delta favorite. This was, after all, the Dream Team, and if any idea is ingrained in our thinking it is that the best way to solve a difficult problem is to bring the maximum amount of expertise to bear on it. Sure enough, in the early going the Dream Team was on fire. The members of the Dream Team did not doggedly fix on a single idea, like Dea and Howell, or move in fits and starts from chai sugar cookies to strawberry shortbread to strawberry cobbler, like Team Stuckey. It came up with thirty-four ideas, representing an astonishing range of cookie philosophies: a chocolate cookie with gourmet cocoa, high-end chocolate chips, pecans, raisins, Irish steel-cut oats, and the new Ultragrain White Whole Wheat flour; a bite-size oatmeal cookie with a Ceylon cinnamon filling, or chili and tamarind, or pieces of dried peaches with a cinnamon-and-ginger dusting; the classic seven-layer bar with oatmeal instead of graham crackers, coated in chocolate with a choice of coffee flavors; a “wellness” cookie, with an oatmeal base, soy and whey proteins, inulin and oat beta glucan and a combination of erythritol and sugar and sterol esters—and so on.

In the course of spewing out all those new ideas, however, the Dream Team took a difficult turn. A man named J. Hugh McEvoy (a.k.a. Chef J.), out of Chicago, tried to take control of the discussion. He wanted something exotic — not a health-food version of something already out there. But in the e-mail discussions with others on the team his sense of what constituted exotic began to get really exotic — “Chinese star anise plus fennel plus Pastis plus dark chocolate.” Others, emboldened by his example, began talking about a possible role for zucchini or wasabi peas. Meanwhile, a more conservative faction, mindful of the Project Delta mandate to appeal to the whole family, started talking up peanut butter. Within a few days, the tensions were obvious:

From: Chef J.

Subject: <no subject>

Please keep in mind that less than 10 years ago, espresso, latte and dulce de leche were EXOTIC flavors / products that were considered unsuitable for the mainstream. And let’s not even mention CHIPOTLE.

From: Andy Smith

Subject: Bought any Ben and Jerry’s recently?

While we may not want to invent another Oreo or Chips Ahoy!, last I looked, World’s Best Vanilla was B&J’s # 2 selling flavor and Haagen Dazs’ Vanilla (their top seller) outsold Dulce 3 to 1.

From: Chef J.

Subject: <no subject>Yes. Gourmet Vanilla does outsell any new flavor. But we must remember that DIET vanilla does not and never has. It is the high end, gourmet segment of ice cream that is growing. Diet Oreos were vastly outsold by new entries like Snackwells. Diet Snickers were vastly outsold by new entries like balance bars. New Coke failed miserably, while Red Bull is still growing.

What flavor IS Red Bull, anyway?

Eventually, Carol Borba, the Dream Team project leader, asked Gundrum whether she should try to calm things down. He told her no; the group had to find its “own kind of natural rhythm.” He wanted to know what fifteen high-powered bakers thrown together on a project felt like, and the answer was that they felt like chaos. They took twice as long as the XP team. They created ten times the headache.

Worse, no one in the open-source group seemed to be having any fun. “Quite honestly, I was expecting a bit more involvement in this,” Howard Plein, of Edlong Dairy Flavors, confessed afterward. “They said, expect to spend half an hour a day. But without doing actual bench work — all we were asked to do was to come up with ideas.” He wanted to bake: he didn’t enjoy being one of fifteen cogs in a machine. To Dan Fletcher, of Kellogg’s, “the whole thing spun in place for a long time. I got frustrated with that. The number of people involved seemed unwieldy. You want some diversity of youth and experience, but you want to keep it close-knit as well. You get some depth in the process versus breadth. We were a mile wide and an inch deep.” Chef J., meanwhile, felt thwarted by Carol Borba; he felt that she was pushing her favorite, a caramel turtle, to the detriment of better ideas. “We had the best people in the country involved,” he says. “We were irrelevant. That’s the weakness of it. Fifteen is too many. How much true input can any one person have when you are lost in the crowd?” In the end, the Dream Team whittled down its thirty-four possibilities to one: a chewy oatmeal cookie, with a pecan “thumbprint” in the middle, and ribbons of caramel-and-chocolate glaze. When Gundrum tasted it, he had nothing but praise for its “cookie hedonics.” But a number of the team members were plainly unhappy with the choice. “It is not bad,” Chef J. said. “But not bad doesn’t win in the food business. There was nothing there that you couldn’t walk into a supermarket and see on the shelf. Any Pepperidge Farm product is better than that. Any one.”

It may have been a fine cookie. But, since no single person played a central role in its creation, it didn’t seem to anyone to be a fine cookie.

The strength of the Dream Team — the fact that it had so many smart people on it — was also its weakness: it had too many smart people on it. Size provides expertise. But it also creates friction, and one of the truths Project Delta exposed is that we tend to overestimate the importance of expertise and underestimate the problem of friction. Gary Klein, a decision-making consultant, once examined this issue in depth at a nuclear power plant in North Carolina. In the nineteen-nineties, the power supply used to keep the reactor cool malfunctioned. The plant had to shut down in a hurry, and the shutdown went badly. So the managers brought in Klein’s consulting group to observe as they ran through one of the crisis rehearsals mandated by federal regulators. “The drill lasted four hours,” David Klinger, the lead consultant on the project, recalled. “It was in this big operations room, and there were between eighty and eighty-five people involved. We roamed around, and we set up a video camera, because we wanted to make sense of what was happening.”

When the consultants asked people what was going on, though, they couldn’t get any satisfactory answers. “Each person only knew a little piece of the puzzle, like the radiation person knew where the radiation was, or the maintenance person would say, ‘I’m trying to get this valve closed,’ ” Klinger said. “No one had the big picture. We started to ask questions. We said, ‘What is your mission?’ And if the person didn’t have one, we said, ‘Get out.’ There were just too many people. We ended up getting that team down from eighty-five to thirty-five people, and the first thing that happened was that the noise in the room was dramatically reduced.” The room was quiet and calm enough so that people could easily find those they needed to talk to. “At the very end, they had a big drill that the N.R.C. was going to regulate. The regulators said it was one of their hardest drills. And you know what? They aced it.” Was the plant’s management team smarter with thirty-five people on it than it was with eighty-five? Of course not, but the expertise of those additional fifty people was more than cancelled out by the extra confusion and noise they created.

The open-source movement has had the same problem. The number of people involved can result in enormous friction. The software theorist Joel Spolsky points out that open-source software tends to have user interfaces that are difficult for ordinary people to use: “With Microsoft Windows, you right-click on a folder, and you’re given the option to share that folder over the Web. To do the same thing with Apache, the open-source Web server, you’ve got to track down a file that has a different name and is stored in a different place on every system. Then you have to edit it, and it has its own syntax and its own little programming language, and there are lots of different comments, and you edit it the first time and it doesn’t work and then you edit it the second time and it doesn’t work.”

Because there are so many individual voices involved in an open-source project, no one can agree on the right way to do things. And, because no one can agree, every possible option is built into the software, thereby frustrating the central goal of good design, which is, after all, to understand what to leave out. Spolsky notes that almost all the successful open-source products have been attempts to clone some preexisting software program, like Microsoft’s Internet Explorer, or Unix. “One of the reasons open source works well for Linux is that there isn’t any real design work to be undertaken,” he says. “They were doing what we would call chasing tail-lights.” Open source was great for a science project, in which the goals were clearly defined and the technical hurdles easily identifiable. Had Project Delta been a Cool Whip bakeoff, an exercise in chasing tail-lights, the Dream Team would easily win. But if you want to design a truly innovative software program — or a truly innovative cookie — the costs of bigness can become overwhelming.

In the frantic final weeks before the bakeoff, while the Dream Team was trying to fix a problem with crumbling, and hit on the idea of glazing the pecan on the face of the cookie, Dea and Howell continued to make steady, incremental improvements.

“These cookies were baked five days ago,” Howell told Gundrum, as he handed him a Ziploc bag. Dea was off somewhere in the Midwest, meeting with clients, and Howell looked apprehensive, stroking his goatee nervously as he stood by Gundrum’s desk. “We used wheat dextrin, which I think gives us some crispiness advantages and some shelf-stability advantages. We have a little more vanilla in this round, which gives you that brown, rounding background note.”

Gundrum nodded. “The vanilla is almost like a surrogate for sugar,” he said. “It potentiates the sweetness.”

“Last time, the leavening system was baking soda and baking powder,” Howell went on. “I switched that to baking soda and monocalcium phosphate. That helps them rise a little bit better. And we baked them at a slightly higher temperature for slightly longer, so that we drove off a little bit more moisture.”

“How close are you?” Gundrum asked.

“Very close,” Howell replied.

Gundrum was lost in thought for a moment. “It looks very wholesome. It looks like something you’d want to feed your kids. It has very good aroma. I really like the texture. My guess is that it eats very well with milk.” He turned back to Howell, suddenly solicitous. “Do you want some milk?”

Meanwhile, Barb Stuckey had a revelation. She was working on a tortilla-chip project, and had bags of tortilla chips all over her desk. “You have no idea how much engineering goes into those things,” she said, holding up a tortilla chip. “It’s greater than what it takes to build a bridge. It’s crazy.” And one of the clever things about cheese tortilla chips—particularly the low-fat versions—is how they go about distracting the palate. “You know how you put a chip in your mouth and the minute it hits your tongue it explodes with flavor?” Stuckey said. “It’s because it’s got this topical seasoning. It’s got dried cheese powders and sugar and probably M.S.G. and all that other stuff on the outside of the chip.”

Her idea was to apply that technique to strawberry cobbler—to take large crystals of sugar, plate them with citric acid, and dust the cookies with them. “The minute they reach your tongue, you get this sweet-and-sour hit, and then you crunch into the cookie and get the rest—the strawberry and the oats,” she said. The crystals threw off your taste buds. You weren’t focussed on the fact that there was half as much fat in the cookie as there should be. Plus, the citric acid brought a tangy flavor to the dried strawberries: suddenly they felt fresh.

Batches of the new strawberry-cobbler prototype were ordered up, with different formulations of the citric acid and the crystals. A meeting was called in the trophy room. Anne Cristofano brought two plastic bags filled with cookies. Stuckey was there, as was a senior Mattson food technologist named Karen Smithson, an outsider brought to the meeting in an advisory role. Smithson, a former pastry chef, was a little older than Stuckey and Cristofano, with an air of self-possession. She broke the seal on the first bag, and took a bite with her eyes half closed. The other two watched intently.

“Umm,” Smithson said, after the briefest of pauses. “That is pretty darn good. And this is one of the healthy cookies? I would not say, ‘This is healthy.’ I can’t taste the trade-off.” She looked up at Stuckey. “How old are they?”

“Today,” Stuckey replied.

“O.K. . . .” This was a complicating fact. Any cookie tastes good on the day it’s baked. The question was how it tasted after baking and packaging and shipping and sitting in a warehouse and on a supermarket shelf and finally in someone’s cupboard.

“What we’re trying to do here is a shelf-stable cookie that will last six months,” Stuckey said. “I think we’re better off if we can make it crispy.”

Smithson thought for a moment. “You can have either a crispy, low-moisture cookie or a soft and chewy cookie,” she said. “But you can’t get the outside crisp and the inside chewy. We know that. The moisture will migrate. It will equilibrate over time, so you end up with a cookie that’s consistent all the way through. Remember we did all that work on Mrs. Fields? That’s what we learned.”

They talked for a bit, in technical terms, about various kinds of sugars and starches. Smithson didn’t think that the stability issue was going to be a problem.

“Isn’t it compelling, visually?” Stuckey blurted out, after a lull in the conversation. And it was: the dried-strawberry chunks broke though the surface of the cookie, and the tiny citric-sugar crystals glinted in the light. “I just think you get so much more bang for the buck when you put the seasoning on the outside.”

“Yet it’s not weird,” Smithson said, nodding. She picked up another cookie. “The mouth feel is a combination of chewy and crunchy. With the flavors, you have the caramelized sugar, the brown-sugar notes. You have a little bit of a chew from the oats. You have a flavor from the strawberry, and it helps to have a combination of the sugar alcohol and the brown sugar. You know, sugars have different deliveries, and sometimes you get some of the sweetness right off and some of it continues on. You notice that a lot with the artificial sweeteners. You get the sweetness that doesn’t go away, long after the other flavors are gone. With this one, the sweetness is nice. The flavors come together at the same time and fade at the same time, and then you have the little bright after-hits from the fruit and the citric crunchies, which are” — she paused, looking for the right word — “brilliant.”

5.

The bakeoff took place in April. Mattson selected a representative sample of nearly three hundred households from around the country. Each was mailed bubble-wrapped packages containing all three entrants. The vote was close but unequivocal. Fourteen per cent of the households voted for the XP oatmeal-chocolate-chip cookie. Forty-one per cent voted for the Dream Team’s oatmeal-caramel cookie. Forty-four per cent voted for Team Stuckey’s strawberry cobbler.

The Project Delta postmortem was held at Chaya Brasserie, a French-Asian fusion restaurant on the Embarcadero, in San Francisco. It was just Gundrum and Steven Addis, from the first Project Delta dinner, and their wives. Dan Howell was immersed in a confidential project for a big food conglomerate back East. Peter Dea was working with Cargill on a wellness product. Carol Borba was in Chicago, at a meeting of the Food Marketing Institute. Barb Stuckey was helping Ringling Brothers rethink the food at its concessions. “We’ve learned a lot about the circus,” Gundrum said. Meanwhile, Addis’s firm had created a logo and a brand name for Project Delta. Mattson has offered to license the winning cookie at no cost, as long as a percentage of its sales goes to a charitable foundation that Mattson has set up to feed the hungry. Someday soon, you should be able to go into a supermarket and buy Team Stuckey’s strawberry-cobbler cookie.

“Which one would you have voted for?” Addis asked Gundrum.

“I have to say, they were all good in their own way,” Gundrum replied. It was like asking a mother which of her children she liked best. “I thought Barb’s cookie was a little too sweet, and I wish the open-source cookie was a little tighter, less crumbly. With XP, I think we would have done better, but we had a wardrobe malfunction. They used too much batter, overbaked it, and the cookie came out too hard and thick.

“In the end, it was not so much which cookie won that interested him. It was who won—and why. Three people from his own shop had beaten a Dream Team, and the decisive edge had come not from the collective wisdom of a large group but from one person’s ability to make a lateral connection between two previously unconnected objects — a tortilla chip and a cookie. Was that just Barb being Barb? In large part, yes. But it was hard to believe that one of the Dream Team members would not have made the same kind of leap had they been in an environment quiet enough to allow them to think.

“Do you know what else we learned?” Gundrum said. He was talking about a questionnaire given to the voters. “We were looking at the open-ended questions — where all the families who voted could tell us what they were thinking. They all said the same thing — all of them.” His eyes grew wide. “They wanted better granola bars and breakfast bars. I would not have expected that.” He fell silent for a moment, turning a granola bar over and around in his mind, assembling and disassembling it piece by piece, as if it were a model airplane. “I thought that they were pretty good,” he said. “I mean, there are so many of them out there. But apparently people want them better.”

© 2013 Malcolm Gladwell.

DAVID AND GOLIATH
 
WHAT THE DOG SAW
 
OUTLIERS
 
BLINK
 
THE TIPPING POINT
 
ARTICLES
 
ETC.
The Pima Paradox

Posted February 2, 1998 by MALCOLM GLADWELL & filed under ANNALS OF MEDICINE, THE NEW YORKER - ARCHIVE.

Can we learn how to lose weight from one of the most obese people in the world?

1.

Sacton lies in the center of Arizona, just off interstate 10, on the Gila River reservation of the Pima Indian tribe. It is a small town, dusty and unremarkable, which looks as if it had been blown there by a gust of desert wind. Shacks and plywood bungalows are scattered along a dirt-and-asphalt grid. Dogs crisscross the streets. Back yards are filled with rusted trucks and junk. The desert in these parts is scruffy and barren, drained of water by the rapid growth of Phoenix, just half an hour’s drive to the north. The nearby Gila River is dry, and the fields of wheat and cushaw squash and tepary beans which the Pima used to cultivate are long gone. The only prepossessing building in Sacaton is a gleaming low-slung modern structure on the outskirts of town–the Hu Hu Kam Memorial Hospital. There is nothing bigger or more impressive for miles, and that is appropriate, since medicine is what has brought Sacaton any wisp of renown it has.

Thirty-five years ago, a team of National Institutes of Health researchers arrived in Sacaton to study rheumatoid arthritis. They wanted to see whether the Pima had higher or lower rates of the disease than the Blackfoot of Montana. A third of the way through their survey, however, they realized that they had stumbled on something altogether strange–a population in the grip of a plague. Two years later, the N.I.H. returned to the Gila River Indian Reservation in force. An exhaustive epidemiological expedition was launched, in which thousands of Pima were examined every two years by government scientists, their weight and height and blood pressure checked, their blood sugar monitored, and their eyes and kidneys scrutinized. In Phoenix, a modern medical center devoted to Native Americans was built; on its top floor, the N.I.H. installed a state-of-the-art research lab, including the first metabolic chamber in North America–a sealed room in which to measure the precise energy intake and expenditure of Pima research subjects. Genetic samples were taken; family histories were mapped; patterns of illness and death were traced from relative to relative and generation to generation. Today, the original study group has grown from four thousand people to seven thousand five hundred, and so many new studies have been added to the old that the total number of research papers arising from the Gila River reservation takes up almost forty feet of shelf space in the N.I.H. library in Phoenix.

The Pima are famous now–famous for being fatter than any other group in the world, with the exception only of the Nauru islanders of the West Pacific. Among those over thirty- five on the reservation, the rate of diabetes, the disease most closely associated with obesity, is fifty per cent, eight times the national average and a figure unmatched in medical history. It is not unheard of in Sacaton for adults to weigh five hundred pounds, for teen-agers to be suffering from diabetes, or for relatively young men and women to be already disabled by the disease–to be blind, to have lost a limb, to be confined to a wheelchair, or to be dependent on kidney dialysis.

When I visited the town, on a monotonously bright desert day not long ago, I watched a group of children on a playing field behind the middle school moving at what seemed to be half speed, their generous shirts and baggy jeans barely concealing their bulk. At the hospital, one of the tribe’s public-health workers told me that when she began an education program on nutrition several years ago she wanted to start with second graders, to catch the children before it was too late. “We were under the delusion that kids didn’t gain weight until the second grade,” she said, shaking her head. “But then we realized we’d have to go younger. Those kids couldn’t run around the block.”

From the beginning, the N.I.H. researchers have hoped that if they can understand why the Pima are so obese they can better understand obesity in the rest of us; the assumption is that obesity in the Pima is different only in degree, not in kind. One hypothesis for the Pima’s plight, favored by Eric Ravussin, of the N.I.H.’s Phoenix team, is that after generations of living in the desert the only Pima who survived famine and drought were those highly adept at storing fat in times of plenty. Under normal circumstances, this disposition was kept in check by the Pima’s traditional diet: cholla-cactus buds, honey mesquite, povertyweed, and prickly pears from the desert floor; mule deer, white-winged dove, and black-tailed jackrabbit; squawfish from the Gila River; and wheat, squash, and beans grown in irrigated desert fields. By the end of the Second World War, however, the Pima had almost entirely left the land, and they began to eat like other Americans. Their traditional diet had been fifteen to twenty per cent fat. Their new diet was closer to forty per cent fat. Famine, which had long been a recurrent condition, gave way to permanent plenty, and so the Pima’s “thrifty” genes, once an advantage, were now a liability. N.I.H. researchers are trying to find these genes, on the theory that they may be the same genes that contribute to obesity in the rest of us. Their studies at Sacaton have also uncovered valuable clues to how diabetes works, how obesity in pregnant women affects their children, and how human metabolism is altered by weight gain. All told, the collaboration between the N.I.H. and the Pima is one of the most fruitful relationships in modern medical science–with one fateful exception. After thirty-five years, no one has had any success helping the Pima lose weight. For all the prodding and poking, the hundreds of research papers describing their bodily processes, and the determined efforts of health workers, year after year the tribe grows fatter.

“I used to be a nurse, I used to work in the clinic, I used to be all gung ho about going out and teaching people about diabetics and obesity,” Teresa Wall, who heads the tribe’s public-health department, told me. “I thought that was all people needed–information. But they weren’t interested. They had other issues.” Wall is a Pima, short and stocky, who has long, straight black hair, worn halfway down her back. She spoke softly. “There’s something missing. It’s one thing to say to people, ‘This is what you should do.’ It’s another to actually get them to take it in.”

The Pima have built a new wellness center in downtown Sacaton, with a weight room and a gymnasium. They now have an education program on nutrition aimed at preschoolers and first graders, and at all tribal functions signs identify healthful food choices–a tray of vegetables or of fruit, say. They are doing, in other words, what public-health professionals are supposed to be doing. But results are hard to see.

“We’ve had kids who were diabetic, whose mothers had diabetes and were on dialysis and had died of kidney failure,” one of the tribe’s nutritionists told me. “You’d think that that would make a difference–that it would motivate them to keep their diet under control. It doesn’t.” She got up from her desk, walked to a bookshelf, and pulled out two bottles of Coca-Cola. One was an old glass bottle. The other was a modern plastic bottle, which towered over it. “The original Coke bottle, in the nineteen-thirties, was six and a half ounces.” She held up the plastic bottle. “Now they are marketing one litre as a single serving. That’s five times the original serving size. The McDonald’s regular hamburger is two hundred and sixty calories, but now you’ve got the double cheeseburger, which is four hundred and forty-five calories. Portion sizes are getting way out of whack. Eating is not about hunger anymore. The fact that people are hungry is way down on the list of why they eat.” I told her that I had come to Sacaton, the front lines of the weight battle, in order to find out what really works in fighting obesity. She looked at me and shrugged. “We’re the last people who could tell you that,” she said.

In the early nineteen-sixties, at about the time the N.I.H. team stumbled on the Pima, seventeen per cent of middle-aged Americans met the clinical definition of obesity. Today, that figure is 32.3 per cent. Between the early nineteen-seventies and the early nineteen-nineties, the percentage of preschool girls who were overweight went from 5.8 per cent to ten per cent. The number of Americans who fall into what epidemiologists call Class Three Obesity–that is, people too grossly overweight, say, to fit into an airline seat–has risen three hundred and fifty per cent in the past thirty years. “We’ve looked at trends by educational level, race, and ethnic group, we’ve compared smokers and non-smokers, and it’s very hard to say that there is any group that is not experiencing this kind of weight gain,” Katherine Flegal, a senior research epidemiologist at the National Center for Health Statistics, says. “It’s all over the world. In China, the prevalence of obesity is vanishingly low, yet they are showing an increase. In Western Samoa, it is very high, and they are showing an increase.” In the same period, science has unlocked many of obesity’s secrets, the American public has been given a thorough education in the principles of good nutrition, health clubs have sprung up from one end of the country to another, dieting has become a religion, and health food a marketing phenomenon. None of it has mattered. It is the Pima paradox: in the fight against obesity all the things that worked in curbing behaviors like drunk driving and smoking and in encouraging things like safe sex and the use of seat belts–education, awareness, motivation–don’t seem to work. For one reason or another, we cannot stop eating. “Since many people cannot lose much weight no matter how hard they try, and promptly regain whatever they do lose,” the editors of The New England Journal of Medicine wearily concluded last month, “the vast amount of money spent on diet clubs, special foods and over-the-counter remedies, estimated to be on the order of $30 billion to $50 billion yearly, is wasted.” Who could argue? If the Pima–who are surrounded by the immediate and tangible consequences of obesity, who have every conceivable motivation–can’t stop themselves from eating their way to illness, what hope is there for the rest of us?

In the scientific literature, there is something called Gourmand Syndrome–a neurological condition caused by anterior brain lesions and characterized by an unusual passion for eating. The syndrome was described in a recent issue of the journal Neurology, and the irrational, seemingly uncontrollable obsession with food evinced by its victims seems a perfect metaphor for the irrational, apparently uncontrollable obsession with food which seems to have overtaken American society as a whole. Here is a diary entry from a Gourmand Syndrome patient, a fifty-five-year-old stroke victim who had previously displayed no more than a perfunctory interest in food.

After I could stand on my feet again, I dreamt to go downtown and sit down in this well-known restaurant. There I would get a beer, sausage, and potatoes. Slowly my diet improved again and thus did quality of life. The day after discharge, my first trip brought me to this restaurant, and here I order potato salad, sausage, and a beer. I feel wonderful. My spouse anxiously registers everything I eat and nibble. It irritates me. A few steps down the street, we enter a coffee-house. My hand is reaching for a pastry, my wife’s hand reaches between. Through the window I see my bank. If I choose, I could buy all the pastry I wanted, including the whole store. The creamy pastry slips from the foil like a mermaid. I take a bite.

2.

Is there an easy way out of this problem? Every year, millions of Americans buy books outlining new approaches to nutrition and diet, nearly all of which are based on the idea that overcoming our obsession with food is really just a matter of technique: that the right foods eaten in the right combination can succeed where more traditional approaches to nutrition have failed. A cynic would say, of course, that the seemingly endless supply of these books proves their lack of efficacy, since if one of these diets actually worked there would be no need for another. But that’s not quite fair. After all, the medical establishment, too, has been giving Americans nutritional advice without visible effect. We have been told that we must not take in more calories than we burn, that we cannot lose weight if we don’t exercise consistently, that an excess of eggs, red meat, cheese, and fried food clogs arteries, that fresh vegetables and fruits help to ward off cancer, that fibre is good and sugar is bad and whole-wheat bread is better than white bread. That few of us are able to actually follow this advice is either our fault or the fault of the advice. Medical orthodoxy, naturally, tends toward the former position. Diet books tend toward the latter. Given how often the medical orthodoxy has been wrong in the past, that position is not, on its face, irrational. It’s worth finding out whether it is true.

Arguably the most popular diet of the moment, for example, is one invented by the biotechnology entrepreneur Barry Sears. Sears’s first book, “The Zone,” written with Bill Lawren, sold a million and a half copies and has been translated into fourteen languages. His second book, “Mastering the Zone,” was on the best-seller lists for eleven weeks. Madonna is rumored to be on the Zone diet, and so are Howard Stern and President Clinton, and if you walk into almost any major bookstore in the country right now Sears’s two best-sellers–plus a new book, “Zone Perfect Meals in Minutes”–will quite likely be featured on a display table near the front. They are ambitious books, filled with technical discussions of food chemistry, metabolism, evolutionary theory, and obscure scientific studies, all apparently serving as proof of the idea that through careful management of”the most powerful and ubiquitous drug we have: food” we can enter a kind of high-efficiency, optimal metabolic state–the Zone.

The key to entering the Zone, according to Sears, is limiting your carbohydrates. When you eat carbohydrates, he writes, you stimulate the production of insulin, and insulin is a hormone that evolved to put aside excess carbohydrate calories in the form of fat in case of future famine. So the insulin that’s stimulated by excess carbohydrates aggressively promotes the accumulation of body fat. In other words, when we eat too much carbohydrate, we’re essentially sending a hormonal message, via insulin, to the body (actually to the adipose cells). The message: “Store fat.”

His solution is a diet in which carbohydrates make up no more than forty per cent of all calories consumed (as opposed to the fifty per cent or more consumed by most Americans), with fat and protein coming to thirty per cent each. Maintaining that precise four-to-three ratio between carbohydrates and protein is, in Sears’s opinion, critical for keeping insulin in check. “The Zone” includes all kinds of complicated instructions to help readers figure out how to do things like calculate their precise protein requirements in restaurants. (“Start with the protein, using the palm of your hand as a guide. The amount of protein that can fit into your palm is usually four protein blocks. That’s about one chicken breast or 4 ounces sliced turkey.”)

It should be said that the kind of diet Sears suggests is perfectly nutritious. Following the Zone diet, you’ll eat lots of fibre, fresh fruit, fresh vegetables, and fish, and very little red meat. Good nutrition, though, isn’t really the point. Sears’s argument is that being in the Zone can induce permanent weight loss–that by controlling carbohydrates and the production of insulin you can break your obsession with food and fundamentally alter the way your body works. “Weight loss . . . can be an ongoing and usually frustrating struggle for most people,” he writes. “In the Zone it is painless, almost automatic.”

Does the Zone exist? Yes and no. Certainly, if people start eating a more healthful diet they’ll feel better about themselves. But the idea that there is something magical about keeping insulin within a specific range is a little strange. Insulin is simply a hormone that regulates the storage of energy. Precisely how much insulin you need to store carbohydrates is dependent on all kinds of things, including how fit you are and whether, like many diabetics, you have a genetic predisposition toward insulin resistance. Generally speaking, the heavier and more out of shape you are, the more insulin your body needs to do its job. The Pima have a problem with obesity and that makes their problem with diabetes worse–not the other way around. High levels of insulin are the result of obesity. They aren’t the cause of obesity. When I read the insulin section of “The Zone” to Gerald Reaven, an emeritus professor of medicine at Stanford University, who is acknowledged to be the country’s leading insulin expert, I could hear him grinding his teeth. “I had the experience ofbeing on a panel discussion with Sears, and I couldn’t believe the stuff that comes out of this guy’s mouth,” he said. “I think he’s full of it.”

What Sears would have us believe is that when it comes to weight loss your body treats some kinds of calories differently from others–that the combination of the food we eat is more critical than the amount. To this end, he cites what he calls an “amazing” and “landmark” study published in 1956 in the British medical journal Lancet. (It should be a tipoff that the best corroborating research he can come up with here is more than forty years old.) In the study, a couple of researchers compared the effects of two different thousand-calorie diets–the first high in fat and protein and low in carbohydrates, and the second low in fat and protein and high in carbohydrates–on two groups of obese men. After eight to ten days, the men on the low-carbohydrate diet had lost more weight than the men on the high-carbohydrate diet. Sears concludes from the study that if you want to lose weight you should eat protein and shun carbohydrates. Actually, it shows nothing of the sort. Carbohydrates promote water retention; protein acts like a diuretic. Over a week or so, someone on a high-protein diet will always look better than someone on a high-carbohydrate diet, simply because of dehydration. When a similar study was conducted several years later, researchers found that after about three weeks–when the effects of dehydration had evened out–the weight loss on the two diets was virtually identical. The key isn’t how you eat, in other words; it’s how much you eat. Calories, not carbohydrates, are still what matters. The dirty little secret of the Zone system is that, despite Sears’s expostulations about insulin, all he has done is come up with another low-calorie diet. He doesn’t do the math for his readers, but some nutritionists have calculated that if you follow Sears’s prescriptions religiously you’ll take in at most seventeen hundred calories a day, and at seventeen hundred calories a day virtually anyone can lose weight. The problem with low-calorie diets, of course, is that no one can stay on them for very long. Just ask Sears. “Diets based on choice restriction and calorie limits usually fail,” he writes in the second chapter of”The Zone,” just as he is about to present his own choice-restricted and calorie-limited diet. “People on restrictive diets get tired of feeling hungry and deprived. They go off their diets, put the weight back on (primarily, as increased body fat) and then feel bad about themselves for not having enough will power, discipline, or motivation.”

These are not, however, the kinds of contradiction that seem to bother Sears. His first book’s dust jacket claims that in the Zone you can “reset your genetic code” and “burn more fat watching TV than by exercising.” By the time he’s finished, Sears has held up his diet as the answer to virtually every medical ill facing Western society, from heart disease to cancer and on to alcoholism and PMS. He writes, “Dr. Paul Kahl, the same physician with whom I did the aids pilot study”–yes, Sears’s diet is just the thing for aids, too–”told me the story of one of his patients, a fifty-year-old woman with MS.”

Paul put her on a Zone-favorable diet, and after a few months on the program she came in for a checkup. Paul asked the basic question: “How are you feeling?” Her answer was “Great!” Noticing that she was still using a cane for stability, Paul asked her, “If you’re feeling so great, why are you still using the cane?” Her only response was that since developing MS she always had. Paul took the cane away and told her to walk to the end of the hallway and back. After a few tentative steps, she made the round trip quickly. When Paul asked her if she wanted her cane back, she just smiled and told him to keep it for someone who really needed it.

Put down your carbohydrates and walk!

It is hard, while reading this kind of thing, to escape the conclusion that what is said in a diet book somehow matters less than how it’s said. Sears, after all, isn’t the only diet specialist who seems to be making things up. They all seem to be making things up. But if you read a large number of popular diet books in succession, what is striking is that they all seem to be making things up in precisely the same way. It is as if the diet-book genre had an unspoken set of narrative rules and conventions, and all that matters is how skillfully those rules and conventions are adhered to. Sears, for example, begins fearful and despondent, his father dead of a heart attack at fifty-three, a “sword of Damocles” over his head. Judy Moscovitz, author of “The Rice Diet Report” (three months on the Times best-seller list), tells us, “I was always the fattest kid in the class, and I knew all the pain that only a fat kid can know…. I was always the last one reluctantly chosen for the teams.” Martin Katahn, in his best-seller “The Rotation Diet,” writes, “I was one of those fat kids who had no memory of ever being thin. Instead, I have memories such as not being able to run fast enough to keep up with my playmates, being chosen last for all games that required physical movement.”

Out of that darkness comes light: the Eureka Moment, when the author explains how he stumbled on the radical truth that inpired his diet. Sears found himself in the library of the Boston University School of Medicine, reading everything he could on the subject: “I had no preconceptions, no base of knowledge to work from, so I read everything. I eventually came across an obscure report…” Rachael Heller, who was a co-author of the best-selling “The Carbohydrate Addict’s Diet” (and, incidentally, so fat growing up that she was “always the last one picked for the team”), was at home in bed when her doctor called, postponing her appointment and thereby setting in motion an extraordinary chain of events that involved veal parmigiana, a Greek salad, and two French crullers: “I will always be grateful for that particular arrangement of circumstances…. Sometimes we are fortunate enough to recognize and take advantage of them, sometimes not. This time I did. I believe it saved my life.” Harvey Diamond, the co-author of the three-million-copy-selling “Fit for Life,” was at a music festival two thousand miles from home, when he happened to overhear two people in front of him discussing the theories of a friend in Santa Barbara: “‘Excuse me,’ I interrupted, ‘who is this fellow you are discussing?’ In less than twenty-four hours I was on my way to Santa Barbara. Little did I know that I was on the brink of one of the most remarkable discoveries of my life.”

The Eureka Moment is followed, typically within a few pages, by the Patent Claim–the point at which the author shows why his Eureka Moment, which explains how weight can be lost without sacrifice, is different from the Eureka Moment of all those other diet books explaining how weight can be lost without sacrifice. This is harder than it appears. Dieters are actually attracted to the idea of discipline, because they attribute their condition to a failure of discipline. It’s just that they know themselves well enough to realize that if a diet requires discipline they won’t be able to follow it. At the same time, of course, even as the dieter realizes that what he is looking for–discipline without the discipline–has never been possible, he still clings to the hope that someday it might be. The Patent Claim must negotiate both paradoxes. Here is Sears, in his deft six-paragraph Patent Claim: “These are not unique claims. The proponents of every new diet that comes along say essentially the same thing. But if you’re reading this book, you probably know that these diets don’t really work.”Why don’t they work? Because they “violate the basic biochemical laws required to enter the Zone.”Other diets don’t have discipline. The Zone does. Yet, he adds, “The beauty of the dietary system presented in this book is that . . . it doesn’t call for a great deal of the kind of unrealistic self- sacrifice that causes many people to fall off the diet wagon. . . . In fact, I can even show you how to stay within these dietary guidelines while eating at fast-food restaurants.” It is the very discipline of the Zone system that allows its adherent to lose weight without discipline.

Or consider this from Adele Puhn’s recent runaway best- seller, “The 5-Day Miracle Diet.” America’s No. 1 diet myth, she writes, is that “you have to deprive yourself to lose weight”:

Even though countless diet programs have said you can have your cake and eat it, too, in your heart of hearts, you have that “nibbling” doubt: For a diet to really work, you have to sacrifice. I know. I bought into this myth for a long time myself. And the fact is that on every other diet, deprivation is involved. Motivation can only take you so far. Eventually you’re going to grab for that extra piece of cake, that box of cookies, that cheeseburger and fries. But not the 5-Day Miracle Diet.

Let us pause and savor the five-hundred-and-forty-degree rhetorical triple gainer taken in those few sentences: (1) the idea that diet involves sacrifice is a myth; (2) all diets, to be sure, say that on their diets dieting without sacrifice is not a myth; (3) but you believe that dieting without sacrifice is a myth; (4) and I, too, believed that dieting without sacrifice is a myth; (5) because in fact on all diets dieting without sacrifice is a myth; (6) except on my diet, where dieting without sacrifice is not a myth.

The expository sequence that these books follow–last one picked, moment of enlightenment, assertion of the one true way–finally amounts to nothing less than a conversion narrative. In conception and execution, diet books are self- consciously theological. (Whom did Harvey Diamond meet after his impulsive, desperate mission to Santa Barbara? A man he will only identify, pseudonymously and mysteriously, as Mr. Jensen, an ethereal figure with “clear eyes, radiant skin, serene demeanor and well-proportioned body.”) It is the appropriation of this religious narrative that permits the suspension of disbelief.

There is a more general explanation for all this in the psychological literature–a phenomenon that might be called the Photocopier Effect, after the experiments of the Harvard social scientist Ellen Langer. Langer examined the apparently common-sense idea that if you are trying to persuade someone to do something for you, you are always better off if you provide a reason. She went up to a group of people waiting in line to use a library copying machine and said, “Excuse me, I have five pages. May I use the Xerox machine?” Sixty per cent said yes. Then she repeated the experiment on another group, except that she changed her request to “Excuse me, I have five pages. May I use the Xerox machine, because I’m in a rush?” Ninety-four per cent said yes. This much sounds like common sense: if you say, “because I’m in a rush”–if you explain your need–people are willing to step aside. But here’s where the study gets interesting. Langer then did the experiment a third time, in this case replacing the specific reason with a statement of the obvious: “Excuse me, I have five pages. May I use the Xerox machine, because I have to make some copies?” The percentage who let her do so this time was almost exactly the same as the one in the previous round–ninety-three per cent. The key to getting people to say yes, in other words, wasn’t the explanation “because I’m in a rush” but merely the use of the word “because.” What mattered wasn’t the substance of the explanation but merely the rhetorical form–the conjunctional footprint–of an explanation.

Isn’t this how diet books work? Consider the following paragraph, taken at random from “The Zone”:

In paracrine hormonal responses, the hormone travels only a very short distance from a secreting cell to a target cell. Because of the short distance between the secreting cell and the target cell, paracrine responses don’t need the long-distance capabilities of the bloodstream. Instead, they use the body’s version of a regional system: the paracrine system. Finally, there are the autocrine hormone systems, analogous to the cord that links the handset of the phone to the phone itself. Here the secreting cells release a hormone that comes immediately back to affect the secreting cell itself.

Don’t worry if you can’t follow what Sears is talking about here–following isn’t really the point. It is enough that he is using the word “because.”

3.

If there is any book that defines the diet genre, however, it is “Dr. Atkins’ New Diet Revolution.” Here is the conversion narrative at its finest. Dr. Atkins, a humble corporate physician, is fat. (“I had three chins.”) He begins searching for answers. (“One evening I read about the work that Dr. Garfield Duncan had done in nutrition at the University of Pennsylvania. Fasting patients, he reported, lose all sense of hunger after forty-eight hours without food. That stunned me. . . . That defied logic.”) He tests his unorthodox views on himself. As if by magic, he loses weight. He tests his unorthodox views on a group of executives at A.T. & T. As if by magic, they lose weight. Incredibly, he has come up with a diet that “produces steady weight loss” while setting “no limit on the amount of food you can eat.” In 1972, inspired by his vision, he puts pen to paper. The result is “Dr. Atkins’ Diet Revolution,” one of the fifty best-selling books of all time. In the early nineties, he publishes “Dr. Atkins’ New Diet Revolution,” which sells more than three million copies and is on the Times best-seller list for almost all of 1997. More than two decades of scientific research into health and nutrition have elapsed in the interim, but Atkins’ message has remained the same. Carbohydrates are bad. Everything else is good. Eat the hamburger, hold the bun. Eat the steak, hold the French fries. Here is the list of ingredients for one of his breakfast “weight loss” recommendations: scrambled eggs for six. Keep in mind that Atkins is probably the most influential diet doctor in the world.

12 link sausages (be sure they contain no sugar)
1 3-ounce package cream cheese
1 tablespoon butter
3/4 cup cream
1/4 cup water
1 teaspoon seasoned salt
2 teaspoons parsley
8 eggs, beaten

Atkins’ Patent Claim centers on the magical weight-loss properties of something called “ketosis.” When you eat carbohydrates, your body converts them into glycogen and stores them for ready use. If you are deprived of carbohydrates, however, your body has to turn to its own stores of fat and muscle for energy. Among the intermediate metabolic products of this fat breakdown are ketones, and when you produce lots of ketones, you’re in ketosis. Since an accumulation of these chemicals swiftly becomes toxic, your body works very hard to get rid of them, either through the kidneys, as urine, or through the lungs, by exhaling, so people in ketosis commonly spend a lot of time in the bathroom and have breath that smells like rotten apples. Ketosis can also raise the risk of bone fracture and cardiac arrhythmia and can result in light-headedness, nausea, and the loss of nutrients like potassium and sodium. There is no doubt that you can lose weight while you’re in ketosis. Between all that protein and those trips to the bathroom, you’ll quickly become dehydrated and drop several pounds just through water loss. The nausea will probably curb your appetite. And if you do what Atkins says, and suddenly cut out virtually all carbohydrates, it will take a little while for your body to compensate for all those lost calories by demanding extra protein and fat. The weight loss isn’t permanent, though. After a few weeks your body adjusts, and the weight–and your appetite–comes back.

For Atkins, however, ketosis is as “delightful as sex and sunshine,” which is why he wants dieters to cut out carbohydrates almost entirely. (To avoid bad breath he recommends carrying chlorophyll tablets and purse-size aerosol breath fresheners at all times; to avoid other complications, he recommends regular blood tests.) Somehow, he has convinced himself that his kind of ketosis is different from the bad kind of ketosis, and that his ketosis can actually lead to permanent weight loss. Why he thinks this, however, is a little unclear. In “Dr. Atkins’ Diet Revolution” he thought that the key was in the many trips to the bathroom:”Hundreds of calories are sneaked out of your body every day in the form of ketones and a host of other incompletely broken down molecules of fat. You are disposing of these calories not by work or violent exercise–but just by breathing and allowing your kidneys to function. All this is achieved merely by cutting out your carbohydrates.” Unfortunately, the year after that original edition of Atkins’ book came out, the American Medical Association published a devastating critique of this theory, pointing out, among other things, that ketone losses in the urine and the breath rarely exceed a hundred calories a day–a quantity, the A.M.A. pointed out, “that could not possibly account for the dramatic results claimed for such diets.” In “Dr. Atkins’ New Diet Revolution,” not surprisingly, he’s become rather vague on the subject, mysteriously invoking something he calls Fat Mobilizing Substance. Last year, when I interviewed him, he offered a new hypothesis: that ketosis takes more energy than conventional food metabolism does, and that it is “a much less efficient pathway to burn up your calories via stored fat than it is via glucose.” But he didn’t want to be pinned down. “Nobody has really been able to work out that mechanism as well as I would have liked,”he conceded.

Atkins is a big, white-haired man in his late sixties, well over six feet, with a barrel chest and a gruff, hard-edged voice. On the day we met, he was wearing a high-lapelled, four-button black suit. Given a holster and a six-shooter, he could have passed for the sheriff in a spaghetti western. He is an intimidating figure, his manner brusque and impatient. He gives the impression that he doesn’t like having to explain his theories, that he finds the details tedious and unnecessary. Given the Photocopier Effect, of course, he is quite right. The appearance of an explanation is more important than the explanation itself. But Atkins seems to take this principle farther than anyone else.

For example, in an attempt to convince his readers that eating pork chops, steaks, duck, and rack of lamb in abundance is good for them, Atkins points out that primitive Eskimo cultures had virtually no heart disease, despite a high-fat diet of fish and seal meat. But one obvious explanation for the Eskimo paradox is that cold-water fish and seal meat are rich in n-3 fatty acids–the “good” kind of fat. Red meat, on the other hand, is rich in saturated fat–the “bad” kind of fat. That dietary fats come in different forms, some of which are particularly bad for you and some of which are not, is the kind of basic fact that seventh graders are taught in Introduction to Nutrition. Atkins has a whole chapter on dietary fat in “New Diet Revolution” and doesn’t make the distinction once. All diet-book authors profit from the Photocopier Effect. Atkins lives it.

I watched Atkins recently as he conducted his daily one- hour radio show on New York’s WEVD. We were in a Manhattan town house in the East Fifties, where he has his headquarters, in a sleek, modernist office filled with leather furniture and soapstone sculpture. He sat behind his desk–John Wayne in headphones–as his producer perched in front of him. It was a bravura performance. He spoke quickly and easily, glancing at his notes only briefly, and then deftly gave counsel to listeners around the region.

The first call came from George, on his car phone. George told Atkins his ratio of triglycerides to cholesterol. It wasn’t good. George was a very unhealthy man. “You’re in big trouble,” Atkins said. “You have to change your diet. What do you generally eat? What’s your breakfast?”

“I’ve stopped taking junk foods,” George says. “I don’t eat eggs. I don’t eat bacon.”

“Then that’s– See there.” Atkins’ voice rose in exasperation. “What do you have for breakfast?”

“I have skim milk, cereal, with banana.”

“That’s three carbs!” Atkins couldn’t believe that in this day and age people were still consuming fruit and skim milk. “That’s how you are getting into trouble!… What you need to do, George, seriously, is get ahold of’New Diet Revolution’ and just read what it says.”

Atkins took another call. This time, it was from Robert, forty-one years old, three hundred pounds, and possessed of a formidable Brooklyn accent. He was desperate to lose weight–up on a ledge and wanting Atkins to talk him down. “I really don’t know anything about dieting,” he said. “I’m getting a little discouraged.”

“It’s really very easy,” Atkins told him, switching artfully to the Socratic method. “Do you like meat?”

“Yes.”

“Could you eat a steak?”

“Yes.”

“All by itself, without any French fries?”

“Yes.”

“And let’s say we threw in a salad, but you couldn’t have any bread or anything else.”

“Yeah, I could do that.”

“Well, if you could go through life like that…. Do you like eggs in the morning? Or a cheese omelette?”

“Yes,”Robert said, his voice almost giddy with relief. He called expecting a life sentence of rice cakes. Now he was being sent forth to eat cheeseburgers. “Yes, I do!”

“If you just eat that way,” Atkins told him, “you’ll have eighty pounds off in six months.”

When I first arrived at Atkins’ headquarters, two members of his staff took me on a quick tour of the facility, a vast medical center, where Atkins administers concoctions of his own creation to people suffering from a variety of disorders. Starting from the fifth floor, we went down to the third, and then from the third to the second, taking the elevator each time. It’s a small point, but it did strike me as odd that I should be in the headquarters of the world’s most popular weight-loss expert and be taking the elevator one floor at a time. After watching Atkins’ show, I was escorted out by his public-relations assistant. We were on the second floor. He pressed the elevator button, down. “Why don’t we take the stairs?” I asked. It was just a suggestion. He looked at me and then at the series of closed doors along the corridor. Tentatively, he opened the second. “I think this is it,” he said, and we headed down, first one flight and then another. At the base of the steps was a door. The P.R. man, a slender fellow in a beautiful Italian suit, peered through it: for the moment, he was utterly lost. We were in the basement. It seemed as if nobody had gone down those stairs in a long time.

4.

Why are the Pima so fat? The answer that diet books would give is that the Pima don’t eat as well as they used to. But that’s what is ultimately wrong with diet books. They talk as if food were the only cause of obesity and its only solution, and we know, from just looking at the Pima, that things are not that simple. The diet of the Pima is bad, but no worse than anyone else’s diet.

Exercise is also clearly part of the explanation for why obesity has become epidemic in recent years. Half as many Americans walk to work today as did twenty years ago. Over the same period, the number of calories burned by the average American every day has dropped by about two hundred and fifty. But this doesn’t explain why obesity has hit the Pima so hard, either, since they don’t seem to be any less active than the rest of us.

The answer, of course, is that there is something beyond diet and exercise that influences obesity–that can make the consequences of a bad diet or of a lack of exercise much worse than they otherwise would be–and this is genetic inheritance. Claude Bouchard, a professor of social and preventive medicine at Laval University, in Quebec City, and one of the world’s leading obesity specialists, estimates that we human beings probably carry several dozen genes that are directly related to our weight. “Some affect appetite, some affect satiety. Some affect metabolic rate, some affect the partitioning of excess energy in fat or lean tissue,” he told me. “There are also reasons to believe that there are genes affecting physical-activity level.” Bouchard did a study not long ago in which he took a group of men of similar height, weight, and life style and overfed them by a thousand calories a day, six days a week, for a hundred days. The average weight gain in the group was eighteen pounds. But the range was from nine to twenty-six pounds. Clearly, the men who gained just nine pounds were the ones whose genes had given them the fastest possible metabolism–the ones who burn the most calories in daily living and are the least efficient at storing fat. These are people who have the easiest time staying thin. The men at the other end of the scale are closer to the Pima in physiology. Their obesity genes thriftily stored away as much of the thousand extra calories a day as possible.

One of the key roles for genes appears to be in determining what obesity researchers refer to as setpoints. In the classic experiment in the field, researchers took a group of rats and made a series of lesions in the base of each rat’s brain. As a result, the rats began overeating and ended up much more obese than normal rats. The first conclusion is plain: there is a kind of thermostat in the brain that governs appetite and weight, and if you change the setting on that thermostat appetite and weight will change accordingly. With that finding in mind, the researchers took a second step. They took those same brain-damaged rats and put them on a diet, severely limiting the amount of food they could eat. What happened? The rats didn’t lose weight. In fact, after some initial fluctuations, they ended up at exactly the same weight as before. Only, this time, being unable to attain their new thermostat setting by eating, they reached it by becoming less active–by burning less energy.

Two years ago, a group at Rockefeller University in New York published a landmark study essentially duplicating in human beings what had been done years ago in rats. They found that if you lose weight your body responds by starting to conserve energy: your metabolism slows down; your muscles seem to work more efficiently, burning fewer calories to do the same work. “Let’s say you have two people, side by side, and these people have exactly the same body composition,” Jules Hirsch, a member of the Rockefeller team, says. “They both weigh a hundred and thirty pounds. But there is one difference–the first person maintains his weight effortlessly, while the second person, who used to weigh two hundred pounds, is trying to maintain a lower weight. The second will need fifteen per cent fewer calories per day to do his work. He needs less oxygen and will burn less energy.” The body of the second person is backpedalling furiously in response to all that lost weight. It is doing everything it can to gain it back. In response to weight gain, by contrast, the Rockefeller team found that the body speeds up metabolism and burns more calories during exercise. It tries to lose that extra weight. Human beings, like rats, seem to have a predetermined setpoint, a weight that their body will go to great lengths to maintain.

One key player in this regulatory system may be a chemical called leptin–or, as it is sometimes known, Ob protein–whose discovery four years ago, by Jeff Friedman, of the Howard Hughes Medical Institute at Rockefeller University, prompted a flurry of headlines. In lab animals, leptin tells the brain to cut back on appetite, to speed up metabolism, and to burn stored fat. The theory is that the same mechanism may work in human beings. If you start to overeat, your fat cells will produce more leptin, so your body will do everything it can to get back to the setpoint. That’s why after gaining a few pounds over the holiday season most of us soon return to our normal weight. But if you eat too little or exercise too much, the theory goes, the opposite happens: leptin levels fall. “This is probably the reason that virtually every weight-loss program known to man fails,” José F. Caro, vice-president of endocrine research and clinical investigation at Eli Lilly & Company, told me. “You go to Weight Watchers. You start losing weight. You feel good. But then your fat cells stop producing leptin. Remember, leptin is the hormone that decreases appetite and increases energy expenditure, so just as you are trying to lose weight you lose the hormone that helps you lose weight.”

Obviously, our body’s fat thermostat doesn’t keep us at one weight all our adult lives. “There isn’t a single setpoint for a human being or an animal,” Thomas Wadden, the director of the Weight and Eating Disorders Clinic at the University of Pennsylvania, told me. “The body will regulate a stable weight but at very different levels, depending on food intake–quality of the diet, high fat versus low fat, high sweet versus low sweet–and depending on the amount of physical activity.” It also seems to be a great deal easier to move the setpoint up than to move it down–which, if you think about the Pima, makes perfect sense. In their long history in the desert, those Pima who survived were the ones who were very good at gaining weight during times of plenty–very good, in other words, at overriding the leptin system at the high end. But there would have been no advantage for the ones who were good at losing weight in hard times. The same is probably true for the rest of us, albeit in a less dramatic form. In our evolutionary history, there was advantage in being able to store away whatever calorific windfalls came our way. To understand this interplay between genes and environment, imagine two women, both five feet five. The first might have a setpoint range of a hundred and ten to a hundred and fifty pounds; the second a range of a hundred and twenty-five to a hundred and eighty. The difference in the ranges of the two women is determined by their genes. Where they are in that range is determined by their life styles.

Not long after leptin was discovered, researchers began testing obese people for the hormone, to see whether a fat person was fat because his body didn’t produce enough leptin. They found the opposite: fat people had lots of leptin. Some of the researchers thought this meant that the leptin theory was wrong–that leptin didn’t do what it was supposed to do. But some other scientists now think that as people get fatter and fatter, their bodies simply get less and less sensitive to leptin. The body still pumps out messages to the brain calling for the metabolism to speed up and the appetite to shrink, but the brain just doesn’t respond to those messages with as much sensitivity as it did. This is probably why it is so much easier to gain weight than it is to lose it. The fatter you get, the less effective your own natural weight-control system becomes.

This doesn’t mean that diets can’t work. In those instances in which dieters have the discipline and the will power to restrict their calories permanently, to get regular and vigorous exercise, and to fight the attempt by their own bodies to maintain their current weight, pounds can be lost. (There is also some evidence that if you can keep weight off for an extensive period–three years, say–a lower setpoint can be established.) Most people, though, don’t have that kind of discipline, and even if they do have it the amount of weight that most dieters can expect to lose on a permanent basis may be limited by their setpoint range. The N.I.H. has a national six-year diabetes-prevention study going on right now, in which it is using a program of intensive, one-on-one counselling, dietary modification, and two and a half hours of exercise weekly to see if it can get overweight volunteers to lose seven per cent of their body weight. If that sounds like a modest goal, it should. “A lot of studies look at ten-per-cent weight loss,” said Mary Hoskin, who is coördinating the section of the N.I.H. study involving the Pima. “But if you look at long-term weight loss nobody can maintain ten per cent. That’s why we did seven.”

On the other hand, now that we’re coming to understand the biology of weight gain, it is possible to conceive of diet drugs that would actually work. If your body sabotages your diet by lowering leptin levels as you lose weight, why not give extra leptin to people on diets? That’s what a number of drug companies, including Amgen and Eli Lilly, are working on now. They are trying to develop a leptin or leptin-analogue pill that dieters could take to fool their bodies into thinking they’re getting fatter when they’re actually getting thinner. “It is very easy to lose weight,” José Caro told me. “The difficult thing is to maintain your weight loss. The thinking is that people fail because their leptin goes down. Here is where replacement therapy with leptin or an Ob-protein analogue might prevent the relapse. It is a subtle and important concept. What it tells you is that leptin is not going to be a magic bullet that allows you to eat whatever you want. You have to initiate the weight loss. Then leptin comes in.”

Another idea, which the Hoffmann-La Roche company is exploring, is to focus on the problems obese people have with leptin. Just as Type II diabetics can become resistant to insulin, many overweight people may become resistant to leptin. So why not try to resensitize them? The idea is to find the leptin receptor in the brain and tinker with it to make it work as well in a fat person as it does in a thin person. (Drug companies have actually been pursuing the same strategy with the insulin receptors of diabetics.) Arthur Campfield, who heads the leptin project for Roche, likens the process by which leptin passes the signal about fat to the brain to a firemen’s bucket brigade, where water is passed from hand to hand. “If you have all tall people, you can pass the bucket and it’s very efficient,”he said. “But if two of the people in the chain are small children, then you’re going to spill a lot of water and slow everything down. We want to take a tablet or a capsule that goes into your brain and puts a muscular person in the chain and overcomes that weakness. The elegant solution is to find the place in the chain where we are losing water.”

The steps that take place in the brain when it receives the leptin message are known as the Ob pathway, and any number of these steps may lend themselves to pharmaceutical intervention. Using the Ob pathway to fight obesity represents a quantum leap beyond the kinds of diet drugs that have been available so far. Fen-phen, the popular medication removed from the market last year because of serious side effects, was, by comparison, a relatively crude product, which worked indirectly to suppress appetite. Hoffmann-La Roche is working now on a drug called Xenical, a compound that blocks the absorption of dietary fat by the intestine. You can eat fat; you just don’t keep as much of it in your system. The drug is safe and has shown real, if modest, success in helping chronically obese patients lose weight. It will probably be the next big diet drug. But no one is pretending that it has anywhere near the potential of, say, a drug that would resensitize your leptin receptors.

Campfield talks about the next wave of drug therapy as the third leg of a three-legged stool–as the additional element that could finally make diet and exercise an easy and reliable way to lose weight. Wadden speaks of the new drugs as restoring sanity:”What I think will happen is that people on these medications will report that they are less responsive to their environment. They’ll say that they are not as turned on by Wendy’s or McDonald’s. Food in America has become a recreational activity. It is divorced from nutritional need and hunger. We eat to kill time, to stimulate ourselves, to alter our mood. What these drugs may mean is that we’re going to become less susceptible to these messages.” In the past thirty years, the natural relationship between our bodies and our environment–a relation that was developed over thousands of years–has fallen out ofbalance. For people who cannot restore that natural balance themselves–who lack the discipline, the wherewithal, or, like the Pima, the genes–drugs could be a way of restoring it for them.

5.

Seven years ago, Peter Bennett, the epidemiologist who first stumbled on the Gila River Pima twenty-eight years earlier, led an N.I.H. expedition to Mexico’s Sierra Madre Mountains. Their destination was a a tiny Indian community on the border of Sonora and Chihuahua, seven thousand feet above the desert. “I had known about their existence for at least fifteen years before that,” Bennett says. “The problem was that I could never find anyone who knew much about them. In 1991, it just happened that we linked up with an investigator down in Mexico.” The journey was a difficult one, but the Mexican government had just built a road linking Sonora and Chihuahua, so the team didn’t have to make the final fifty- or sixty-mile trek on horseback. “They were clearly a group who have got along together for a very long time,” Bennett recalls. “My reaction as a stranger going in was: Gee, I think these people are really very friendly, very coöperative. They seem to be interested in what we want to do, and they are willing to stick their arms out and let us take blood samples.” He laughed. “Which is always a good sign.”

The little town in the Sierra Madre is home to the Mexican Pima, the southern remnants of a tribe that once stretched from present-day Arizona down to central Mexico. Like the Pima of the Gila River reservation, they are farmers, living in small clusters of wood-and-adobe rancherías among the pine trees, cultivating beans, corn, and potatoes in the valleys. On that first trip, the N.I.H. team examined no more than a few dozen Pima. Since then, the team has been back five or six times, staying for as many as ten days at a time. Two hundred and fifty of the mountain Pima have now been studied. They have been measured and weighed, their blood sugar has been checked, and their kidneys and eyes have been examined for signs of damage. Genetic samples have been taken and their metabolism has been monitored. The Mexican Pima, it turns out, eat a diet consisting almost entirely ofbeans, potatoes, and corn tortillas, with chicken perhaps once a month. They take in twenty-two hundred calories a day, which is slightly more than the Pima of Arizona do. But on the average each of them puts in twenty-three hours a week of moderate to hard physical labor, whereas the average Arizona Pima puts in two hours. The Mexican Pima’s rates of diabetes are normal. They are slightly shorter than their American counterparts. In weight, there is no comparison: “I would say they are thin,” Bennett says. “Thin. Certainly by American standards.”

There are, of course, a hundred reasons not to draw any great lessons from this. Subsistence farming is no way to make a living in America today, nor are twenty-three hours ofhard physical labor feasible in a society where most people sit at a desk from nine to five. And even if the Arizona Pima wanted to return to the land, they couldn’t. It has been more than a hundred years since the Gila River, which used to provide the tribe with fresh fish and with water for growing beans and squash, was diverted upstream for commercial farming. Yet there is value in the example of the Mexican Pima. People who work with the Pima of Arizona say that the biggest problem they have in trying to fight diabetes and obesity is fatalism–a sense among the tribe that nothing can be done, that the way things are is the way things have to be. It is possible to see in the attitudes of Americans toward weight loss the same creeping resignation. As the world grows fatter, and as one best-selling diet scheme after another inevitably fails, the idea that being slender is an attainable–or even an advisable–condition is slowly receding. Last month, when The New England Journal of Medicine published a study suggesting that the mortality costs of obesity had been overstated, the news was greeted with resounding relief, as if we were all somehow off the hook, as if the issue with obesity were only mortality and not the thousand ways in which being fat undermines our quality of life: the heightened risk of heart disease, hypertension, diabetes, cancer, arthritis, gallbladder disease, trauma, gout, blindness, birth defects, and other aches, pains, and physical indignities too numerous to mention. What we are in danger of losing in the epidemic of obesity is not merely our health but our memory of health. Those Indian towns high in the Sierra Madre should remind the people of Sacaton–and all the rest of us as well–that it is still possible, even for a Pima, to be fit.

© 2013 Malcolm Gladwell.